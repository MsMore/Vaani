{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "finetuning2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMBiPDdvdRjBN5jpdkB0hJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MsMore/Vaani/blob/main/finetuning2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBYinME2YJ2b",
        "outputId": "bef40fb7-9c0d-4bb6-a23a-83c6c9b4a449"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anZ0ff10YKp5"
      },
      "source": [
        "import numpy as np\n",
        "from keras import utils, callbacks\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "#from keras.preprocessing.image import load_img, img_to_array\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqCGtkJAYKsS"
      },
      "source": [
        "train_directory = \"/content/drive/MyDrive/Img/Train\"\n",
        "test_directory = \"/content/drive/MyDrive/Img/Validation\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iIYNtLmYKu7",
        "outputId": "454f64e1-4e12-4da0-946f-1276c3d22ca6"
      },
      "source": [
        "from keras.applications import VGG16\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(128,128, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyWPDTbMYKxa",
        "outputId": "4699adb8-3dd6-4919-f292-49f65c384b30"
      },
      "source": [
        "# Freeze four convolution blocks\n",
        "for layer in vgg_model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "# Make sure you have frozen the correct layers\n",
        "for i, layer in enumerate(vgg_model.layers):\n",
        "    print(i, layer.name, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_1 False\n",
            "1 block1_conv1 False\n",
            "2 block1_conv2 False\n",
            "3 block1_pool False\n",
            "4 block2_conv1 False\n",
            "5 block2_conv2 False\n",
            "6 block2_pool False\n",
            "7 block3_conv1 False\n",
            "8 block3_conv2 False\n",
            "9 block3_conv3 False\n",
            "10 block3_pool False\n",
            "11 block4_conv1 False\n",
            "12 block4_conv2 False\n",
            "13 block4_conv3 False\n",
            "14 block4_pool False\n",
            "15 block5_conv1 True\n",
            "16 block5_conv2 True\n",
            "17 block5_conv3 True\n",
            "18 block5_pool True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdROFJKcYK1R"
      },
      "source": [
        "x = vgg_model.output\n",
        "x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(29, activation='softmax')(x) # Softmax for multiclass\n",
        "transfer_model = Model(inputs=vgg_model.input, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr33mIaPYK4t"
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_accuracy', factor=0.6, patience=8, verbose=1, mode='max', min_lr=5e-5)\n",
        "checkpoint = ModelCheckpoint('vgg16_finetune.h15', monitor= 'val_accuracy', mode= 'max', save_best_only = True, verbose= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr2GCnjwYhOA",
        "outputId": "fb451195-6876-4989-970d-9dca04d0b18a"
      },
      "source": [
        "gen = ImageDataGenerator(rescale=1./255, validation_split=0.1) \n",
        "train = gen.flow_from_directory(train_directory, target_size=(128, 128), subset=\"training\")\n",
        "val = gen.flow_from_directory(test_directory, target_size=(128, 128), subset=\"validation\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9135 images belonging to 29 classes.\n",
            "Found 290 images belonging to 29 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQq6j1ISYhRh",
        "outputId": "9e502831-ca19-43d2-8c18-54510aeab584"
      },
      "source": [
        "from tensorflow.keras import layers, models, Model, optimizers\n",
        "learning_rate= 5e-5\n",
        "transfer_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=learning_rate), metrics=[\"accuracy\"])\n",
        "history = transfer_model.fit(train, batch_size = 1, epochs=25, validation_data=val, callbacks=[lr_reduce,checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "286/286 [==============================] - 41s 83ms/step - loss: 1.4180 - accuracy: 0.6330 - val_loss: 5.5736e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 1.00000, saving model to vgg16_finetune.h15\n",
            "INFO:tensorflow:Assets written to: vgg16_finetune.h15/assets\n",
            "Epoch 2/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 0.0141 - accuracy: 0.9973 - val_loss: 1.6007e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 1.00000\n",
            "Epoch 3/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 0.0051 - accuracy: 0.9990 - val_loss: 6.6653e-04 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 1.00000\n",
            "Epoch 4/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 0.0069 - accuracy: 0.9987 - val_loss: 1.0961e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 1.00000\n",
            "Epoch 5/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 1.5728e-05 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 1.00000\n",
            "Epoch 6/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 3.9495e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 1.00000\n",
            "Epoch 7/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 7.3686e-04 - accuracy: 1.0000 - val_loss: 1.6052e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 1.00000\n",
            "Epoch 8/25\n",
            "286/286 [==============================] - 20s 72ms/step - loss: 0.0012 - accuracy: 0.9994 - val_loss: 1.9526e-07 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 1.00000\n",
            "Epoch 9/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 1.5777e-04 - accuracy: 1.0000 - val_loss: 7.4403e-08 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 1.00000\n",
            "Epoch 10/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 3.6128e-05 - accuracy: 1.0000 - val_loss: 2.3020e-08 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 1.00000\n",
            "Epoch 11/25\n",
            "286/286 [==============================] - 20s 72ms/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 2.5021e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 1.00000\n",
            "Epoch 12/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 3.7501e-04 - accuracy: 1.0000 - val_loss: 6.0016e-08 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 1.00000\n",
            "Epoch 13/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 1.0724e-04 - accuracy: 1.0000 - val_loss: 4.2751e-08 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 1.00000\n",
            "Epoch 14/25\n",
            "286/286 [==============================] - 21s 73ms/step - loss: 5.3121e-05 - accuracy: 1.0000 - val_loss: 1.6443e-09 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 1.00000\n",
            "Epoch 15/25\n",
            "286/286 [==============================] - 21s 73ms/step - loss: 2.2648e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 1.00000\n",
            "Epoch 16/25\n",
            "286/286 [==============================] - 20s 70ms/step - loss: 2.5079e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 1.00000\n",
            "Epoch 17/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 1.6741e-05 - accuracy: 1.0000 - val_loss: 8.2213e-10 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 1.00000\n",
            "Epoch 18/25\n",
            "286/286 [==============================] - 21s 72ms/step - loss: 9.1265e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 1.00000\n",
            "Epoch 19/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 6.1864e-06 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 1.00000\n",
            "Epoch 20/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 0.0128 - accuracy: 0.9963 - val_loss: 4.4843e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 1.00000\n",
            "Epoch 21/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 2.2156e-04 - accuracy: 1.0000 - val_loss: 1.5621e-08 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 1.00000\n",
            "Epoch 22/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 1.4655e-04 - accuracy: 1.0000 - val_loss: 3.9873e-08 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 1.00000\n",
            "Epoch 23/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 1.2866e-07 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 1.00000\n",
            "Epoch 24/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 7.3062e-06 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 1.00000\n",
            "Epoch 25/25\n",
            "286/286 [==============================] - 20s 71ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 3.8845e-07 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 1.00000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhPtbsx1Y7-6",
        "outputId": "a31abfcb-fb0d-4e48-b78b-5d7ba27a9e08"
      },
      "source": [
        "for layer in vgg_model.layers[:15]:\n",
        "  layer.trainable = False\n",
        "x = vgg_model.output\n",
        "x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(29, activation='softmax')(x) # Softmax for multiclass\n",
        "transfer_model = Model(inputs=vgg_model.input, outputs=x)\n",
        "for i, layer in enumerate(transfer_model.layers):\n",
        "  print(i, layer.name, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 input_1 False\n",
            "1 block1_conv1 False\n",
            "2 block1_conv2 False\n",
            "3 block1_pool False\n",
            "4 block2_conv1 False\n",
            "5 block2_conv2 False\n",
            "6 block2_pool False\n",
            "7 block3_conv1 False\n",
            "8 block3_conv2 False\n",
            "9 block3_conv3 False\n",
            "10 block3_pool False\n",
            "11 block4_conv1 False\n",
            "12 block4_conv2 False\n",
            "13 block4_conv3 False\n",
            "14 block4_pool False\n",
            "15 block5_conv1 True\n",
            "16 block5_conv2 True\n",
            "17 block5_conv3 True\n",
            "18 block5_pool True\n",
            "19 flatten_2 True\n",
            "20 dense_6 True\n",
            "21 dropout_2 True\n",
            "22 dense_7 True\n",
            "23 dense_8 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33SRc47sY8HW"
      },
      "source": [
        "#Augment images\n",
        "#rain_datagen = ImageDataGenerator(zoom_range=0.2, rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2)\n",
        "#Fit augmentation to training images\n",
        "#train_generator = train_datagen.flow(train,batch_size=1)\n",
        "#Compile model\n",
        "transfer_model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "#Fit model\n",
        "history = transfer_model.fit_generator(train, validation_data=val, epochs=50, shuffle=True, callbacks=[lr_reduce],verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "FOn_gvmnOunA",
        "outputId": "56a7002e-15c8-48aa-ef98-7fee0f9236a5"
      },
      "source": [
        "transfer_model.save(\"vggRA.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7f26232afc18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransfer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vggRA.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'transfer_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fUcVLSaQV_i",
        "outputId": "0a0086fa-a97b-4a7b-f784-9d9ab8ed9010"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(transfer_model)\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpvpogwcth/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMampoB8QWBy",
        "outputId": "ba4137ef-2f0b-46d9-ff8a-27e8ed29b494"
      },
      "source": [
        "import pathlib\n",
        "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "76202716"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEyF6vRsQhFD"
      },
      "source": [
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vEPjn63QhIN",
        "outputId": "7cccf8fa-edd3-47b2-d03e-1e2ebfbfbb4b"
      },
      "source": [
        "tflite_fp16_model = converter.convert()\n",
        "tflite_model_fp16_file = tflite_models_dir/\"mnist_model_quant_f16.tflite\"\n",
        "tflite_model_fp16_file.write_bytes(tflite_fp16_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpqbpme8gj/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpqbpme8gj/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38110688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pDmOCuBY7F-"
      },
      "source": [
        "import keras\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/vggRA.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4QoT35wZW_W"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "def prepare_image(file):\n",
        "    img_path = '/content/drive/MyDrive/Img/vggTest/'\n",
        "    img = image.load_img(img_path + file, target_size=(128, 128))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
        "    return tf.keras.applications.mobilenet.preprocess_input(img_array_expanded_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9xDQN95jZXBr"
      },
      "source": [
        "for i in range(1,59):  \n",
        "  preprocessed_image = prepare_image('Test ({}).jpg'.format(i))\n",
        "  predictions = model.predict(preprocessed_image)\n",
        "  predictions=np.argmax(predictions[0])\n",
        "  val=predictions\n",
        "  str1=\"ABCD-EFGHIJKLMN=OPQRS TUVWXYZ\"\n",
        "  print(str1[val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA-zF4TEgiIs"
      },
      "source": [
        "import keras\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/vggRiddhi.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbDuPzA8bwWy"
      },
      "source": [
        "import cv2\n",
        "import numpy\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    cv2.resize(cap,(128,128,3))\n",
        "    ret, frame = cap.read()\n",
        "    image = frame\n",
        "    #cv2.imread(frame)\n",
        "    #cv2.resize(frame,(128,128))\n",
        "    #image_np = np.array(frame)\n",
        "    \n",
        "    img_array = image.img_to_array(frame)\n",
        "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    predictions = model.predict(preprocessed_image)\n",
        "    predictions=np.argmax(predictions[0])\n",
        "    val=predictions\n",
        "    str1=\"ABCD-EFGHIJKLMN=OPQRS TUVWXYZ\"\n",
        "    print(str1[val])\n",
        "\n",
        "    cv2.imshow('object detection',  cv2.resize(image, (800, 600)))\n",
        "    \n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        cap.release()\n",
        "        break\n",
        "cap.release()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}