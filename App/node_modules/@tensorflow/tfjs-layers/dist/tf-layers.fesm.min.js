/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
import{backend as t,util as e,serialization as n,tidy as s,sqrt as i,sum as r,mul as a,clipByValue as o,div as l,add as u,relu as h,max as c,tensor1d as p,min as d,slice as f,slice4d as g,slice3d as m,slice2d as y,slice1d as b,gather as w,tile as k,randomNormal as x,elu as v,abs as S,dropout as I,concat as N,fused as A,concat4d as C,concat3d as z,concat2d as D,concat1d as $,zeros as T,ones as E,scalar as F,randomUniform as L,truncatedNormal as _,eye as R,linalg as M,variable as O,dispose as B,nextFrame as P,keep as W,log as U,sub as K,mean as j,softmax as V,neg as q,fill as G,maximum as H,floor as J,oneHot as Z,softplus as X,onesLike as Y,greater as Q,equal as tt,argMax as et,logicalAnd as nt,where as st,train as it,memory as rt,cast as at,Tensor as ot,Optimizer as lt,io as ut,selu as ht,minimum as ct,sigmoid as pt,tanh as dt,logSoftmax as ft,leakyRelu as gt,prelu as mt,transpose as yt,conv2dTranspose as bt,conv3dTranspose as wt,conv1d as kt,conv3d as xt,separableConv2d as vt,depthwiseConv2d as St,expandDims as It,reverse as Nt,unstack as At,stack as Ct,split as zt,conv2d as Dt,any as $t,notEqual as Tt,zerosLike as Et,all as Ft,greaterEqual as Lt,moments as _t,batchNorm2d as Rt,batchNorm3d as Mt,batchNorm4d as Ot,pad as Bt,maxPool as Pt,avgPool as Wt,maxPool3d as Ut,avgPool3d as Kt,squeeze as jt}from"@tensorflow/tfjs-core";function Vt(t){throw new Error(`'${t}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`)}function qt(t,e){if(!t)throw new Error("string"==typeof e?e:e())}function Gt(t,e=[],n=!1){if(null==e&&(e=[]),Array.isArray(t)||Qt(t)&&!n)for(let s=0;s<t.length;++s)Gt(t[s],e,n);else e.push(t);return e}function Ht(t){if(0===t.length)return 1;let e=t[0];for(let n=1;n<t.length;n++)e*=t[n];return e}function Jt(t,e){if(t===e)return!0;if(null==t||null==e)return!1;if(t.length!==e.length)return!1;for(let n=0;n<t.length;n++)if(t[n]!==e[n])return!1;return!0}function Zt(t){return t%1==0}function Xt(t,e){return e<=t.length?t:t+" ".repeat(e-t.length)}function Yt(t,e){const n=e.length;return qt((t=null==t?e.map(((t,e)=>e)):[].concat(t)).every((t=>t>=-n&&t<n)),(()=>`All values in axis param must be in range [-${n}, ${n}) but got axis ${t}`)),qt(t.every((t=>Zt(t))),(()=>`All values in axis param must be integers but got axis ${t}`)),t.map((t=>t<0?n+t:t))}function Qt(t){return t instanceof Float32Array||t instanceof Int32Array||t instanceof Uint8Array}function te(t){if("float32"===t||"int32"===t)return 4;if("complex64"===t)return 8;if("bool"===t)return 1;throw new Error(`Unknown dtype ${t}`)}function ee(t){return"string"==typeof t||t instanceof String}function ne(t){return Array.isArray(t)?ne(t[0]):t instanceof Float32Array?"float32":t instanceof Int32Array||t instanceof Uint8Array?"int32":"number"==typeof t?"float32":ee(t)?"string":function(t){return"boolean"==typeof t}(t)?"bool":"float32"}function se(t){return!!(t&&t.constructor&&t.call&&t.apply)}function ie(t){const e=t.length;if(e<2)return[];const n=new Array(e-1);n[e-2]=t[e-1];for(let s=e-3;s>=0;--s)n[s]=n[s+1]*t[s+1];return n}function re(t,e,n,s=!1){const i=new Array;if(1===e.length){const r=e[0]*(s?2:1);for(let e=0;e<r;e++)i[e]=n[t+e]}else{const r=e[0],a=e.slice(1),o=a.reduce(((t,e)=>t*e))*(s?2:1);for(let e=0;e<r;e++)i[e]=re(t+e*o,a,n,s)}return i}function ae(t,e,n=!1){if(0===t.length)return e[0];const s=t.reduce(((t,e)=>t*e))*(n?2:1);if(0===s)return[];if(s!==e.length)throw new Error(`[${t}] does not match the input size ${e.length}${n?" for a complex tensor":""}.`);return re(0,t,e,n)}function oe(t,e){const n=le(t,e);for(let t=0;t<n.length;t++)n[t]=1;return n}function le(t,e){if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e)return new Uint8Array(t);throw new Error(`Unknown data type ${e}`)}function ue(t){return t&&t.then&&"function"==typeof t.then}class he{constructor(t){this.global=t,this.flags={},this.flagRegistry={},this.urlFlags={},this.getQueryParams=ce,this.populateURLFlags()}setPlatform(t,e){null!=this.platform&&console.warn(`Platform ${this.platformName} has already been set. Overwriting the platform with ${e}.`),this.platformName=t,this.platform=e}registerFlag(t,e,n){if(this.flagRegistry[t]={evaluationFn:e,setHook:n},null!=this.urlFlags[t]){const e=this.urlFlags[t];console.warn(`Setting feature override from URL ${t}: ${e}.`),this.set(t,e)}}async getAsync(t){return t in this.flags||(this.flags[t]=await this.evaluateFlag(t)),this.flags[t]}get(t){if(t in this.flags)return this.flags[t];const e=this.evaluateFlag(t);if(ue(e))throw new Error(`Flag ${t} cannot be synchronously evaluated. Please use getAsync() instead.`);return this.flags[t]=e,this.flags[t]}getNumber(t){return this.get(t)}getBool(t){return this.get(t)}getFlags(){return this.flags}get features(){return this.flags}set(t,e){if(null==this.flagRegistry[t])throw new Error(`Cannot set flag ${t} as it has not been registered.`);this.flags[t]=e,null!=this.flagRegistry[t].setHook&&this.flagRegistry[t].setHook(e)}evaluateFlag(t){if(null==this.flagRegistry[t])throw new Error(`Cannot evaluate flag '${t}': no evaluation function found.`);return this.flagRegistry[t].evaluationFn()}setFlags(t){this.flags=Object.assign({},t)}reset(){this.flags={},this.urlFlags={},this.populateURLFlags()}populateURLFlags(){if(void 0===this.global||void 0===this.global.location||void 0===this.global.location.search)return;const t=this.getQueryParams(this.global.location.search);if("tfjsflags"in t){t.tfjsflags.split(",").forEach((t=>{const[e,n]=t.split(":");this.urlFlags[e]=function(t,e){if("true"===(e=e.toLowerCase())||"false"===e)return"true"===e;if(""+ +e===e)return+e;throw new Error(`Could not parse value flag value ${e} for flag ${t}.`)}(e,n)}))}}}function ce(t){const e={};return t.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g,((t,...n)=>(function(t,e,n){t[decodeURIComponent(e)]=decodeURIComponent(n||"")}(e,n[0],n[1]),n.join("=")))),e}function pe(){return fe}let de,fe=null;function ge(){if(null==de){let t;if("undefined"!=typeof window)t=window;else if("undefined"!=typeof global)t=global;else if("undefined"!=typeof process)t=process;else{if("undefined"==typeof self)throw new Error("Could not find a global object");t=self}de=t}return de}function me(t,e){const n=function(){const t=ge();return null==t._tfGlobals&&(t._tfGlobals=new Map),t._tfGlobals}();if(n.has(t))return n.get(t);{const s=e();return n.set(t,s),n.get(t)}}const ye=me("kernelRegistry",(()=>new Map)),be=me("gradRegistry",(()=>new Map));function we(t,e){const n=function(t,e){return`${e}_${t}`}(t,e);return ye.get(n)}function ke(t){return be.get(t)}function xe(t){const e=ye.entries(),n=[];for(;;){const{done:s,value:i}=e.next();if(s)break;const[r,a]=i,[o]=r.split("_");o===t&&n.push(a)}return n}function ve(t){const{kernelName:e}=t;be.has(e)&&pe().getBool("DEBUG")&&console.warn(`Overriding the gradient for '${e}'`),be.set(e,t)}function Se(t,e){if("string"===e)throw new Error("Cannot convert a string[] to a TypedArray");if(Array.isArray(t)&&(t=Gt(t)),pe().getBool("DEBUG")&&function(t,e){for(let n=0;n<t.length;n++){const s=t[n];if(isNaN(s)||!isFinite(s))throw Error(`A tensor of type ${e} being uploaded contains ${s}.`)}}(t,e),function(t,e){return t instanceof Float32Array&&"float32"===e||t instanceof Int32Array&&"int32"===e||t instanceof Uint8Array&&"bool"===e}(t,e))return t;if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e){const e=new Uint8Array(t.length);for(let n=0;n<e.length;++n)0!==Math.round(t[n])&&(e[n]=1);return e}throw new Error(`Unknown data type ${e}`)}function Ie(){return pe().platform.now()}function Ne(t,e="utf-8"){return e=e||"utf-8",pe().platform.decode(t,e)}class Ae{constructor(t,e){this.backendTimer=t,this.logger=e,null==e&&(this.logger=new ze)}profileKernel(t,e,n){let s;const i=()=>{s=n()};let r;const a=Ie();if(this.backendTimer.timerAvailable())r=this.backendTimer.time(i);else{i();for(const t of s)t.dataSync();r=Promise.resolve({kernelMs:Ie()-a})}if(pe().getBool("CHECK_COMPUTATION_FOR_ERRORS"))for(let e=0;e<s.length;e++){const n=s[e];n.data().then((e=>{Ce(e,n.dtype,t)}))}return{kernelName:t,outputs:s,inputs:e,timeMs:r.then((t=>t.kernelMs)),extraInfo:r.then((t=>null!=t.getExtraProfileInfo?t.getExtraProfileInfo():""))}}logKernelProfile(t){const{kernelName:e,outputs:n,timeMs:s,inputs:i,extraInfo:r}=t;n.forEach((t=>{Promise.all([t.data(),s,r]).then((n=>{this.logger.logKernelProfile(e,t,n[0],n[1],i,n[2])}))}))}}function Ce(t,e,n){if("float32"!==e)return!1;for(let e=0;e<t.length;e++){const s=t[e];if(isNaN(s)||!isFinite(s))return console.warn(`Found ${s} in the result of '${n}'`),!0}return!1}class ze{logKernelProfile(t,e,n,s,i,r){const a="number"==typeof s?Xt(`${s}ms`,9):s.error,o=Xt(t,25),l=e.rank,u=e.size,h=Xt(e.shape.toString(),14);let c="";for(const t in i){const n=i[t];if(null!=n){const s=n.shape||e.shape,i=s.length;c+=`${t}: ${i}D ${i>0?s:""} `}}console.log(`%c${o}\t%c${a}\t%c${l}D ${h}\t%c${u}\t%c${c}\t%c${r}`,"font-weight:bold","color:red","color:blue","color: orange","color: green","color: steelblue")}}function De(t,e,n,s){const i=ie(e),r=function(t,e,n,s){const i=Ht(e),r=s[s.length-1],a=new Array(r).fill(0),o=e.length,l="complex64"===n?Fe(t):t;if(o>1)for(let t=0;t<i/r;t++){const e=t*r;for(let t=0;t<r;t++)a[t]=Math.max(a[t],$e(l[e+t],0,n).length)}return a}(t,e,n,i),a=e.length,o=Ee(t,e,n,i,r),l=["Tensor"];return s&&(l.push(`  dtype: ${n}`),l.push(`  rank: ${a}`),l.push(`  shape: [${e}]`),l.push("  values:")),l.push(o.map((t=>"    "+t)).join("\n")),l.join("\n")}function $e(t,e,n){let s;return s=Array.isArray(t)?`${parseFloat(t[0].toFixed(7))} + ${parseFloat(t[1].toFixed(7))}j`:ee(t)?`'${t}'`:"bool"===n?Te(t):parseFloat(t.toFixed(7)).toString(),Xt(s,e)}function Te(t){return 0===t?"false":"true"}function Ee(t,e,n,s,i,r=!0){const a="complex64"===n?2:1,o=e[0],l=e.length;if(0===l){if("complex64"===n){return[$e(Fe(t)[0],0,n)]}return"bool"===n?[Te(t[0])]:[t[0].toString()]}if(1===l){if(o>20){const e=3*a;let s=Array.from(t.slice(0,e)),r=Array.from(t.slice((o-3)*a,o*a));return"complex64"===n&&(s=Fe(s),r=Fe(r)),["["+s.map(((t,e)=>$e(t,i[e],n))).join(", ")+", ..., "+r.map(((t,e)=>$e(t,i[o-3+e],n))).join(", ")+"]"]}return["["+("complex64"===n?Fe(t):Array.from(t)).map(((t,e)=>$e(t,i[e],n))).join(", ")+"]"]}const u=e.slice(1),h=s.slice(1),c=s[0]*a,p=[];if(o>20){for(let e=0;e<3;e++){const s=e*c,r=s+c;p.push(...Ee(t.slice(s,r),u,n,h,i,!1))}p.push("...");for(let e=o-3;e<o;e++){const s=e*c,r=s+c;p.push(...Ee(t.slice(s,r),u,n,h,i,e===o-1))}}else for(let e=0;e<o;e++){const s=e*c,r=s+c;p.push(...Ee(t.slice(s,r),u,n,h,i,e===o-1))}const d=2===l?",":"";p[0]="["+p[0]+d;for(let t=1;t<p.length-1;t++)p[t]=" "+p[t]+d;let f=",\n";for(let t=2;t<l;t++)f+="\n";return p[p.length-1]=" "+p[p.length-1]+"]"+(r?"":f),p}function Fe(t){const e=[];for(let n=0;n<t.length;n+=2)e.push([t[n],t[n+1]]);return e}let Le=null;class _e{constructor(t,e,n,s){this.kept=!1,this.isDisposedInternal=!1,this.shape=t.slice(),this.dtype=e||"float32",this.size=Ht(t),this.strides=ie(t),this.dataId=n,this.id=s,this.rankType=this.rank<5?this.rank.toString():"higher"}get rank(){return this.shape.length}async buffer(){const t=await this.data();return null.buffer(this.shape,this.dtype,t)}bufferSync(){return null.buffer(this.shape,this.dtype,this.dataSync())}async array(){const t=await this.data();return ae(this.shape,t,"complex64"===this.dtype)}arraySync(){return ae(this.shape,this.dataSync(),"complex64"===this.dtype)}async data(){this.throwIfDisposed();const t=Le().read(this.dataId);if("string"===this.dtype){const e=await t;try{return e.map((t=>Ne(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}}return t}dataSync(){this.throwIfDisposed();const t=Le().readSync(this.dataId);if("string"===this.dtype)try{return t.map((t=>Ne(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}return t}async bytes(){this.throwIfDisposed();const t=await Le().read(this.dataId);return"string"===this.dtype?t:new Uint8Array(t.buffer)}dispose(){this.isDisposed||(Le().disposeTensor(this),this.isDisposedInternal=!0)}get isDisposed(){return this.isDisposedInternal}throwIfDisposed(){if(this.isDisposed)throw new Error("Tensor is disposed.")}print(t=!1){return null.print(this,t)}clone(){return this.throwIfDisposed(),null.clone(this)}toString(t=!1){return De(this.dataSync(),this.shape,this.dtype,t)}cast(t){return this.throwIfDisposed(),null.cast(this,t)}variable(t=!0,e,n){return this.throwIfDisposed(),Le().makeVariable(this,t,e,n)}}function Re(){return me("Tensor",(()=>_e))}Object.defineProperty(_e,Symbol.hasInstance,{value:t=>!!t&&null!=t.data&&null!=t.dataSync&&null!=t.throwIfDisposed}),Re();class Me extends _e{constructor(t,e,n,s){super(t.shape,t.dtype,t.dataId,s),this.trainable=e,this.name=n}assign(t){if(t.dtype!==this.dtype)throw new Error(`dtype of the new value (${t.dtype}) and previous value (${this.dtype}) must match`);if(!Jt(t.shape,this.shape))throw new Error(`shape of the new value (${t.shape}) and previous value (${this.shape}) must match`);Le().disposeTensor(this),this.dataId=t.dataId,Le().incRef(this,null)}dispose(){Le().disposeVariable(this),this.isDisposedInternal=!0}}var Oe,Be,Pe,We,Ue;Object.defineProperty(Me,Symbol.hasInstance,{value:t=>t instanceof _e&&null!=t.assign&&t.assign instanceof Function}),function(t){t.R0="R0",t.R1="R1",t.R2="R2",t.R3="R3",t.R4="R4",t.R5="R5",t.R6="R6"}(Oe||(Oe={})),function(t){t.float32="float32",t.int32="int32",t.bool="int32",t.complex64="complex64"}(Be||(Be={})),function(t){t.float32="float32",t.int32="int32",t.bool="bool",t.complex64="complex64"}(Pe||(Pe={})),function(t){t.float32="float32",t.int32="float32",t.bool="float32",t.complex64="complex64"}(We||(We={})),function(t){t.float32="complex64",t.int32="complex64",t.bool="complex64",t.complex64="complex64"}(Ue||(Ue={}));const Ke={float32:We,int32:Be,bool:Pe,complex64:Ue};function je(t,e){if(t.dtype===e.dtype)return[t,e];const n=function(t,e){if("string"===t||"string"===e){if("string"===t&&"string"===e)return"string";throw new Error(`Can not upcast ${t} with ${e}`)}return Ke[t][e]}(t.dtype,e.dtype);return[t.cast(n),e.cast(n)]}function Ve(t){const e=[];return qe(t,e,new Set),e}function qe(t,e,n){if(null==t)return;if(t instanceof _e)return void e.push(t);if(s=t,!Array.isArray(s)&&"object"!=typeof s)return;var s;const i=t;for(const t in i){const s=i[t];n.has(s)||(n.add(s),qe(s,e,n))}}function Ge(t){return null!=t.kernelName}class He{constructor(){this.registeredVariables={},this.nextTapeNodeId=0,this.numBytes=0,this.numTensors=0,this.numStringTensors=0,this.numDataBuffers=0,this.gradientDepth=0,this.kernelDepth=0,this.scopeStack=[],this.numDataMovesStack=[],this.nextScopeId=0,this.tensorInfo=new WeakMap,this.profiling=!1,this.activeProfile={newBytes:0,newTensors:0,peakBytes:0,kernels:[],result:null,get kernelNames(){return Array.from(new Set(this.kernels.map((t=>t.name))))}}}dispose(){for(const t in this.registeredVariables)this.registeredVariables[t].dispose()}}class Je{constructor(t){this.ENV=t,this.registry={},this.registryFactory={},this.pendingBackendInitId=0,this.state=new He}async ready(){if(null!=this.pendingBackendInit)return this.pendingBackendInit.then((()=>{}));if(null!=this.backendInstance)return;const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e];if(await this.initializeBackend(n).success)return void await this.setBackend(n)}throw new Error("Could not initialize any backends, all backend initializations failed.")}get backend(){if(null!=this.pendingBackendInit)throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);if(null==this.backendInstance){const{name:t,asyncInit:e}=this.initializeBackendsAndReturnBest();if(e)throw new Error(`The highest priority backend '${t}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);this.setBackend(t)}return this.backendInstance}backendNames(){return Object.keys(this.registryFactory)}findBackend(t){if(!(t in this.registry)){if(!(t in this.registryFactory))return null;{const{asyncInit:e}=this.initializeBackend(t);if(e)return null}}return this.registry[t]}findBackendFactory(t){return t in this.registryFactory?this.registryFactory[t].factory:null}registerBackend(t,e,n=1){return t in this.registryFactory?(console.warn(`${t} backend was already registered. Reusing existing backend factory.`),!1):(this.registryFactory[t]={factory:e,priority:n},!0)}async setBackend(t){if(null==this.registryFactory[t])throw new Error(`Backend name '${t}' not found in registry`);if(this.backendName=t,null==this.registry[t]){this.backendInstance=null;const{success:e,asyncInit:n}=this.initializeBackend(t);if(!(n?await e:e))return!1}return this.backendInstance=this.registry[t],this.setupRegisteredKernels(),this.profiler=new Ae(this.backendInstance),!0}setupRegisteredKernels(){xe(this.backendName).forEach((t=>{null!=t.setupFunc&&t.setupFunc(this.backendInstance)}))}disposeRegisteredKernels(t){xe(t).forEach((e=>{null!=e.disposeFunc&&e.disposeFunc(this.registry[t])}))}initializeBackend(t){const e=this.registryFactory[t];if(null==e)throw new Error(`Cannot initialize backend ${t}, no registration found.`);try{const n=e.factory();if(!n||n instanceof class{refCount(t){return Vt("refCount")}incRef(t){return Vt("incRef")}timerAvailable(){return!0}time(t){return Vt("time")}read(t){return Vt("read")}readSync(t){return Vt("readSync")}numDataIds(){return Vt("numDataIds")}disposeData(t,e){return Vt("disposeData")}write(t,e,n){return Vt("write")}move(t,e,n,s,i){return Vt("move")}memory(){return Vt("memory")}floatPrecision(){return Vt("floatPrecision")}epsilon(){return 32===this.floatPrecision()?1e-7:1e-4}dispose(){return Vt("dispose")}}||"function"!=typeof n.then)return this.registry[t]=n,{success:!0,asyncInit:!1};{const e=++this.pendingBackendInitId,s=n.then((n=>!(e<this.pendingBackendInitId)&&(this.registry[t]=n,this.pendingBackendInit=null,!0))).catch((n=>(e<this.pendingBackendInitId||(this.pendingBackendInit=null,console.warn(`Initialization of backend ${t} failed`),console.warn(n.stack||n.message)),!1)));return this.pendingBackendInit=s,{success:s,asyncInit:!0}}}catch(e){return console.warn(`Initialization of backend ${t} failed`),console.warn(e.stack||e.message),{success:!1,asyncInit:!1}}}removeBackend(t){if(!(t in this.registryFactory))throw new Error(`${t} backend not found in registry`);this.backendName===t&&null!=this.pendingBackendInit&&this.pendingBackendInitId++,t in this.registry&&(this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t]),delete this.registryFactory[t],this.backendName===t&&(this.pendingBackendInit=null,this.backendName=null,this.backendInstance=null)}getSortedBackends(){if(0===Object.keys(this.registryFactory).length)throw new Error("No backend found in registry.");return Object.keys(this.registryFactory).sort(((t,e)=>this.registryFactory[e].priority-this.registryFactory[t].priority))}initializeBackendsAndReturnBest(){const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e],{success:s,asyncInit:i}=this.initializeBackend(n);if(i||s)return{name:n,asyncInit:i}}throw new Error("Could not initialize any backends, all backend initializations failed.")}moveData(t,e){const n=this.state.tensorInfo.get(e),s=n.backend,i=this.readSync(e),r=s.refCount(e);s.disposeData(e,!0),n.backend=t,t.move(e,i,n.shape,n.dtype,r),this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack[this.state.numDataMovesStack.length-1]++}tidy(t,e){let n,s=null;if(null==e){if("function"!=typeof t)throw new Error("Please provide a function to tidy()");e=t}else{if("string"!=typeof t&&!(t instanceof String))throw new Error("When calling with two arguments, the first argument to tidy() must be a string");if("function"!=typeof e)throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");s=t}return this.scopedRun((()=>this.startScope(s)),(()=>this.endScope(n)),(()=>(n=e(),n instanceof Promise&&console.error("Cannot return a Promise inside of tidy."),n)))}scopedRun(t,e,n){t();try{const t=n();return e(),t}catch(t){throw e(),t}}nextTensorId(){return Je.nextTensorId++}nextVariableId(){return Je.nextVariableId++}clone(t){const e=Ze.runKernel("Identity",{x:t}),n={x:t};return this.addTapeNode(this.state.activeScope.name,n,[e],(t=>({x:()=>{const e={x:t},n={dtype:"float32"};return Ze.runKernel("Cast",e,n)}})),[],{}),e}runKernel(t,e,n){if(!(null!=we(t,this.backendName)))throw new Error(`Kernel '${t}' not registered for backend '${this.backendName}'`);return this.runKernelFunc({kernelName:t,inputs:e,attrs:n})}shouldCheckForMemLeaks(){return this.ENV.getBool("IS_TEST")}checkKernelForMemLeak(t,e,n){const s=this.backend.numDataIds();let i=0;n.forEach((t=>{i+="complex64"===t.dtype?3:1}));const r=this.state.numDataMovesStack[this.state.numDataMovesStack.length-1],a=s-e-i-r;if(a>0)throw new Error(`Backend '${this.backendName}' has an internal memory leak (${a} data ids) after running '${t}'`)}runKernelFunc(t){let e,n=[];const s=this.isTapeOn(),i=this.state.numBytes,r=this.state.numTensors;let a,o;this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack.push(0),null==this.backendName&&this.backend;const l=Ge(t)?t.kernelName:null!=this.state.activeScope?this.state.activeScope.name:"";if(Ge(t)){const{kernelName:e,inputs:i,attrs:r}=t;null==this.backendName&&this.backend;const l=we(e,this.backendName);qt(null!=l,(()=>`Cannot find registered kernel '${e}' for backend '${this.backendName}'`)),a=()=>{const t=this.backend.numDataIds();o=l.kernelFunc({inputs:i,attrs:r,backend:this.backend});const a=Array.isArray(o)?o:[o];this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(e,t,a);const u=a.map((t=>{if(null!=t.rank)return t;const{dataId:e,shape:n,dtype:s}=t;return this.makeTensorFromDataId(e,n,s)}));if(s){const t=this.getTensorsForGradient(e,i,u);n=this.saveTensorsForBackwardMode(t)}return u}}else{const{forwardFunc:e}=t,i=t=>{s&&(n=t.map((t=>this.keep(this.clone(t)))))};a=()=>{const t=this.backend.numDataIds();o=this.tidy((()=>e(this.backend,i)));const n=Array.isArray(o)?o:[o];return this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(l,t,n),n}}const{inputs:u,attrs:h}=t,c=Ge(t)?null:t.backwardsFunc;let p;return this.scopedRun((()=>this.state.kernelDepth++),(()=>this.state.kernelDepth--),(()=>{this.ENV.getBool("DEBUG")||this.state.profiling?(p=this.profiler.profileKernel(l,u,(()=>a())),this.ENV.getBool("DEBUG")&&this.profiler.logKernelProfile(p),e=p.outputs):e=a()})),s&&this.addTapeNode(l,u,e,c,n,h),this.state.profiling&&this.state.activeProfile.kernels.push({name:l,bytesAdded:this.state.numBytes-i,totalBytesSnapshot:this.state.numBytes,tensorsAdded:this.state.numTensors-r,totalTensorsSnapshot:this.state.numTensors,inputShapes:Object.keys(u).map((t=>null!=u[t]?u[t].shape:null)),outputShapes:e.map((t=>t.shape)),kernelTimeMs:p.timeMs,extraInfo:p.extraInfo}),Array.isArray(o)?e:e[0]}saveTensorsForBackwardMode(t){return t.map((t=>this.keep(this.clone(t))))}getTensorsForGradient(t,e,n){const s=ke(t);if(null!=s){const t=s.inputsToSave||[],i=s.outputsToSave||[];let r;s.saveAllInputs?(qt(Array.isArray(e),(()=>"saveAllInputs is true, expected inputs to be an array.")),r=Object.keys(e).map((t=>e[t]))):r=t.map((t=>e[t]));const a=n.filter(((t,e)=>i[e]));return r.concat(a)}return[]}makeTensor(t,e,n,s){if(null==t)throw new Error("Values passed to engine.makeTensor() are null");n=n||"float32",s=s||this.backend;let i=t;"string"===n&&ee(t[0])&&(i=t.map((t=>function(t,e="utf-8"){return e=e||"utf-8",pe().platform.encode(t,e)}(t))));const r=s.write(i,e,n),a=new _e(e,n,r,this.nextTensorId());if(this.trackTensor(a,s),"string"===n){const t=this.state.tensorInfo.get(r),e=function(t){if(null==t)return 0;let e=0;return t.forEach((t=>e+=t.length)),e}(i);this.state.numBytes+=e-t.bytes,t.bytes=e}return a}makeTensorFromDataId(t,e,n,s){const i=new _e(e,n=n||"float32",t,this.nextTensorId());return this.trackTensor(i,s),i}makeVariable(t,e=!0,n,s){n=n||this.nextVariableId().toString(),null!=s&&s!==t.dtype&&(t=t.cast(s));const i=new Me(t,e,n,this.nextTensorId());if(null!=this.state.registeredVariables[i.name])throw new Error(`Variable with name ${i.name} was already registered`);return this.state.registeredVariables[i.name]=i,this.incRef(i,this.backend),i}trackTensor(t,e){this.state.numTensors++,"string"===t.dtype&&this.state.numStringTensors++;let n=0;"complex64"!==t.dtype&&"string"!==t.dtype&&(n=t.size*te(t.dtype)),this.state.numBytes+=n,this.state.tensorInfo.has(t.dataId)||(this.state.numDataBuffers++,this.state.tensorInfo.set(t.dataId,{backend:e||this.backend,dtype:t.dtype,shape:t.shape,bytes:n})),t instanceof Me||this.track(t)}incRef(t,e){this.trackTensor(t,e),this.backend.incRef(t.dataId)}removeDataId(t,e){this.state.tensorInfo.has(t)&&this.state.tensorInfo.get(t).backend===e&&(this.state.tensorInfo.delete(t),this.state.numDataBuffers--)}disposeTensor(t){if(!this.state.tensorInfo.has(t.dataId))return;const e=this.state.tensorInfo.get(t.dataId);if(this.state.numTensors--,"string"===t.dtype&&(this.state.numStringTensors--,this.state.numBytes-=e.bytes),"complex64"!==t.dtype&&"string"!==t.dtype){const e=t.size*te(t.dtype);this.state.numBytes-=e}e.backend.disposeData(t.dataId)&&this.removeDataId(t.dataId,e.backend)}disposeVariables(){for(const t in this.state.registeredVariables){const e=this.state.registeredVariables[t];this.disposeVariable(e)}}disposeVariable(t){this.disposeTensor(t),null!=this.state.registeredVariables[t.name]&&delete this.state.registeredVariables[t.name]}memory(){const t=this.backend.memory();return t.numTensors=this.state.numTensors,t.numDataBuffers=this.state.numDataBuffers,t.numBytes=this.state.numBytes,this.state.numStringTensors>0&&(t.unreliable=!0,null==t.reasons&&(t.reasons=[]),t.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)")),t}async profile(t){this.state.profiling=!0;const e=this.state.numBytes,n=this.state.numTensors;this.state.activeProfile.kernels=[],this.state.activeProfile.result=await t(),this.state.profiling=!1,this.state.activeProfile.peakBytes=Math.max(...this.state.activeProfile.kernels.map((t=>t.totalBytesSnapshot))),this.state.activeProfile.newBytes=this.state.numBytes-e,this.state.activeProfile.newTensors=this.state.numTensors-n;for(const t of this.state.activeProfile.kernels)t.kernelTimeMs=await t.kernelTimeMs,t.extraInfo=await t.extraInfo;return this.state.activeProfile}isTapeOn(){return this.state.gradientDepth>0&&0===this.state.kernelDepth}addTapeNode(t,e,n,s,i,r){const a={id:this.state.nextTapeNodeId++,kernelName:t,inputs:e,outputs:n,saved:i},o=ke(t);null!=o&&(s=o.gradFunc),null!=s&&(a.gradient=t=>(t=t.map(((t,e)=>{if(null==t){const t=n[e],s=le(t.size,t.dtype);return this.makeTensor(s,t.shape,t.dtype)}return t})),s(t.length>1?t:t[0],i,r))),this.state.activeTape.push(a)}keep(t){return t.kept=!0,t}startTape(){0===this.state.gradientDepth&&(this.state.activeTape=[]),this.state.gradientDepth++}endTape(){this.state.gradientDepth--}startScope(t){const e={track:[],name:"unnamed scope",id:this.state.nextScopeId++};t&&(e.name=t),this.state.scopeStack.push(e),this.state.activeScope=e}endScope(t){const e=Ve(t),n=new Set(e.map((t=>t.id)));for(let t=0;t<this.state.activeScope.track.length;t++){const e=this.state.activeScope.track[t];e.kept||n.has(e.id)||e.dispose()}const s=this.state.scopeStack.pop();this.state.activeScope=0===this.state.scopeStack.length?null:this.state.scopeStack[this.state.scopeStack.length-1],e.forEach((t=>{t.kept||t.scopeId!==s.id||this.track(t)}))}gradients(t,e,n,s=!1){if(qt(e.length>0,(()=>"gradients() received an empty list of xs.")),null!=n&&"float32"!==n.dtype)throw new Error(`dy must have 'float32' dtype, but has '${n.dtype}'`);const i=this.scopedRun((()=>this.startTape()),(()=>this.endTape()),(()=>this.tidy("forward",t)));qt(i instanceof _e,(()=>"The result y returned by f() must be a tensor."));const r=function(t,e,n){const s={},i={};for(let t=0;t<e.length;t++)s[e[t].id]=!0;for(let n=0;n<t.length;n++){const r=t[n],a=r.inputs;for(const t in a){const n=a[t];let o=!1;for(let t=0;t<e.length;t++)if(s[n.id]){r.outputs.forEach((t=>s[t.id]=!0)),o=!0,i[r.id]=!0;break}if(o)break}}const r={};r[n.id]=!0;const a={};for(let e=t.length-1;e>=0;e--){const n=t[e],s=n.inputs;for(let t=0;t<n.outputs.length;t++)if(r[n.outputs[t].id]){for(const t in s)r[s[t].id]=!0,a[n.id]=!0;break}}const o=[];for(let e=0;e<t.length;e++){const n=t[e];if(i[n.id]&&a[n.id]){const t={};for(const e in n.inputs){const i=n.inputs[e];s[i.id]&&(t[e]=i)}const e=Object.assign({},n);e.inputs=t,e.outputs=n.outputs,o.push(e)}}return o}(this.state.activeTape,e,i);if(!s&&0===r.length&&e.length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");return this.tidy("backward",(()=>{const t={};t[i.id]=null==n?function(t){const e=oe(Ht(t),"float32");return Ze.makeTensor(e,t,"float32")}(i.shape):n,function(t,e,n,s){for(let i=e.length-1;i>=0;i--){const r=e[i],a=[];if(r.outputs.forEach((e=>{const n=t[e.id];null!=n?a.push(n):a.push(null)})),null==r.gradient)throw new Error(`Cannot compute gradient: gradient function not found for ${r.kernelName}.`);const o=r.gradient(a);for(const e in r.inputs){if(!(e in o))throw new Error(`Cannot backprop through input ${e}. Available gradients found: ${Object.keys(o)}.`);const i=n((()=>o[e]()));if("float32"!==i.dtype)throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input ${e} must have 'float32' dtype, but has '${i.dtype}'`);const a=r.inputs[e];if(!Jt(i.shape,a.shape))throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input '${e}' has shape '${i.shape}', which does not match the shape of the input '${a.shape}'`);if(null==t[a.id])t[a.id]=i;else{const e=t[a.id];t[a.id]=s(e,i),e.dispose()}}}}(t,r,(t=>this.tidy(t)),Xe);const s=e.map((e=>t[e.id]));return 0===this.state.gradientDepth&&(this.state.activeTape.forEach((t=>{for(const e of t.saved)e.dispose()})),this.state.activeTape=null),{value:i,grads:s}}))}customGrad(t){return qt(se(t),(()=>"The f passed in customGrad(f) must be a function.")),(...e)=>{let n;qt(e.every((t=>t instanceof _e)),(()=>"The args passed in customGrad(f)(x1, x2,...) must all be tensors"));const s={};e.forEach(((t,e)=>{s[e]=t}));return this.runKernelFunc({forwardFunc:(s,i)=>(n=t(...e,i),qt(n.value instanceof _e,(()=>"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor")),qt(se(n.gradFunc),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.")),n.value),backwardsFunc:(t,s)=>{const i=n.gradFunc(t,s),r=Array.isArray(i)?i:[i];qt(r.length===e.length,(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).")),qt(r.every((t=>t instanceof _e)),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors."));const a={};return r.forEach(((t,e)=>{a[e]=()=>t})),a},inputs:s})}}readSync(t){return this.state.tensorInfo.get(t).backend.readSync(t)}read(t){return this.state.tensorInfo.get(t).backend.read(t)}async time(t){const e=Ie(),n=await this.backend.time(t);return n.wallMs=Ie()-e,n}track(t){return null!=this.state.activeScope&&(t.scopeId=this.state.activeScope.id,this.state.activeScope.track.push(t)),t}get registeredVariables(){return this.state.registeredVariables}reset(){this.pendingBackendInitId++,this.state.dispose(),this.ENV.reset(),this.state=new He;for(const t in this.registry)this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t];this.backendName=null,this.backendInstance=null,this.pendingBackendInit=null}}Je.nextTensorId=0,Je.nextVariableId=0;const Ze=function(){const t=ge();if(null==t._tfengine){const e=new he(t);t._tfengine=new Je(e)}var e;return e=t._tfengine.ENV,fe=e,Le=()=>t._tfengine,t._tfengine}();function Xe(t,e){const n={a:t,b:e};return Ze.runKernel("Add",n)}function Ye(t,e){let n=t;if(Qt(t))return"string"===e?[]:[t.length];if(!Array.isArray(t))return[];const s=[];for(;Array.isArray(n)||Qt(n)&&"string"!==e;)s.push(n.length),n=n[0];return Array.isArray(t)&&pe().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")&&Qe(t,s,[]),s}function Qe(t,e,n){if(n=n||[],!Array.isArray(t)&&!Qt(t))return void qt(0===e.length,(()=>`Element arr[${n.join("][")}] is a primitive, but should be an array/TypedArray of ${e[0]} elements`));qt(e.length>0,(()=>`Element arr[${n.join("][")}] should be a primitive, but is an array of ${t.length} elements`)),qt(t.length===e[0],(()=>`Element arr[${n.join("][")}] should have ${e[0]} elements, but has ${t.length} elements`));const s=e.slice(1);for(let e=0;e<t.length;++e)Qe(t[e],s,n.concat(e))}function tn(t,e,n,s){if("string_or_numeric"!==t){if(null==t)throw new Error("Expected dtype cannot be null.");if("numeric"!==t&&t!==e||"numeric"===t&&"string"===e)throw new Error(`Argument '${n}' passed to '${s}' must be ${t} tensor, but got ${e} tensor`)}}function en(t,e,n,s="numeric"){if(t instanceof _e)return tn(s,t.dtype,e,n),t;let i=ne(t);if("string"!==i&&["bool","int32","float32"].indexOf(s)>=0&&(i=s),tn(s,i,e,n),null==t||!Qt(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t){const s=null==t?"null":t.constructor.name;throw new Error(`Argument '${e}' passed to '${n}' must be a Tensor or TensorLike, but got '${s}'`)}const r=Ye(t,i);Qt(t)||Array.isArray(t)||(t=[t]);const a="string"!==i?Se(t,i):Gt(t,[],!0);return Ze.makeTensor(a,r,i)}function nn(t,e,n,s="numeric"){if(!Array.isArray(t))throw new Error(`Argument ${e} passed to ${n} must be a \`Tensor[]\` or \`TensorLike[]\``);return t.map(((t,i)=>en(t,`${e}[${i}]`,n,s)))}function sn(t){const e=Object.keys(t);if(1!==e.length)throw new Error(`Please provide an object with a single key (operation name) mapping to a function. Got an object with ${e.length} keys.`);let n=e[0];const s=t[n];n.endsWith("_")&&(n=n.substring(0,n.length-1)),n+="__op";const i=(...t)=>{Ze.startScope(n);try{const e=s(...t);return ue(e)&&console.error("Cannot return a Promise inside of tidy."),Ze.endScope(e),e}catch(t){throw Ze.endScope(null),t}};return Object.defineProperty(i,"name",{value:n,configurable:!0}),i}const rn=sn({abs_:function(t){const e=en(t,"x","abs");if("complex64"===e.dtype){const t={x:e};return Ze.runKernel("ComplexAbs",t)}{const t={x:e};return Ze.runKernel("Abs",t)}}});const an=sn({acos_:function(t){const e={x:en(t,"x","acos")};return Ze.runKernel("Acos",e)}});const on=sn({acosh_:function(t){const e={x:en(t,"x","acosh")};return Ze.runKernel("Acosh",e)}});const ln=sn({add_:function(t,e){let n=en(t,"a","add"),s=en(e,"b","add");[n,s]=je(n,s);const i={a:n,b:s};return Ze.runKernel("Add",i)}});const un=sn({all_:function(t,e=null,n=!1){const s={x:en(t,"x","all","bool")},i={axis:e,keepDims:n};return Ze.runKernel("All",s,i)}});const hn=sn({any_:function(t,e=null,n=!1){const s={x:en(t,"x","any","bool")},i={axis:e,keepDims:n};return Ze.runKernel("Any",s,i)}});const cn=sn({argMax_:function(t,e=0){const n={x:en(t,"x","argMax")},s={axis:e};return Ze.runKernel("ArgMax",n,s)}});const pn=sn({argMin_:function(t,e=0){const n={x:en(t,"x","argMin")},s={axis:e};return Ze.runKernel("ArgMin",n,s)}});const dn=sn({asin_:function(t){const e={x:en(t,"x","asin")};return Ze.runKernel("Asin",e)}});const fn=sn({asinh_:function(t){const e={x:en(t,"x","asinh")};return Ze.runKernel("Asinh",e)}});const gn=sn({atan_:function(t){const e={x:en(t,"x","atan")};return Ze.runKernel("Atan",e)}});const mn=sn({atan2_:function(t,e){let n=en(t,"a","atan2"),s=en(e,"b","atan2");[n,s]=je(n,s);const i={a:n,b:s};return Ze.runKernel("Atan2",i)}});const yn=sn({atanh_:function(t){const e={x:en(t,"x","atanh")};return Ze.runKernel("Atanh",e)}});const bn=sn({cast_:function(t,e){const n=en(t,"x","cast");if(!function(t){return"bool"===t||"complex64"===t||"float32"===t||"int32"===t||"string"===t}(e))throw new Error(`Failed to cast to unknown dtype ${e}`);if("string"===e&&"string"!==n.dtype||"string"!==e&&"string"===n.dtype)throw new Error("Only strings can be casted to strings");const s={x:n},i={dtype:e};return Ze.runKernel("Cast",s,i)}});function wn(t,e,n,s,i,r,a="channelsLast"){const[o,l]=kn(e);let u;if("channelsLast"===a)u=[o,l,t[3],t[3]];else{if("channelsFirst"!==a)throw new Error(`Unknown dataFormat ${a}`);u=[o,l,t[1],t[1]]}return function(t,e,n,s,i,r,a=!1,o="channelsLast"){let[l,u,h,c]=[-1,-1,-1,-1];if("channelsLast"===o)[l,u,h,c]=t;else{if("channelsFirst"!==o)throw new Error(`Unknown dataFormat ${o}`);[l,c,u,h]=t}const[p,d,,f]=e,[g,m]=kn(n),[y,b]=kn(s),w=xn(p,y),k=xn(d,b),{padInfo:x,outHeight:v,outWidth:S}=function(t,e,n,s,i,r,a,o,l){let u,h,c;if("number"==typeof t){u={top:t,bottom:t,left:t,right:t,type:0===t?"VALID":"NUMBER"};const i=function(t,e,n,s,i){null==s&&(s=function(t,e,n,s=1){const i=xn(e,s);return Math.floor((t[0]*(n-1)-n+i)/2)}(t,e,n));const r=t[0],a=t[1],o=vn((r-e+2*s)/n+1,i),l=vn((a-e+2*s)/n+1,i);return[o,l]}([e,n],r,s,t,o);h=i[0],c=i[1]}else if("same"===t){h=Math.ceil(e/s),c=Math.ceil(n/i);const t=Math.max(0,(h-1)*s+r-e),o=Math.max(0,(c-1)*i+a-n),l=Math.floor(t/2),p=t-l,d=Math.floor(o/2);u={top:l,bottom:p,left:d,right:o-d,type:"SAME"}}else if("valid"===t)u={top:0,bottom:0,left:0,right:0,type:"VALID"},h=Math.ceil((e-r+1)/s),c=Math.ceil((n-a+1)/i);else{if("object"!=typeof t)throw Error(`Unknown padding parameter: ${t}`);{const p="channelsLast"===l?t[1][0]:t[2][0],d="channelsLast"===l?t[1][1]:t[2][1],f="channelsLast"===l?t[2][0]:t[3][0],g="channelsLast"===l?t[2][1]:t[3][1];u={top:p,bottom:d,left:f,right:g,type:0===p&&0===d&&0===f&&0===g?"VALID":"EXPLICIT"},h=vn((e-r+p+d)/s+1,o),c=vn((n-a+f+g)/i+1,o)}}return{padInfo:u,outHeight:h,outWidth:c}}(i,u,h,g,m,w,k,r,o),I=a?f*c:f;let N;"channelsFirst"===o?N=[l,I,v,S]:"channelsLast"===o&&(N=[l,v,S,I]);return{batchSize:l,dataFormat:o,inHeight:u,inWidth:h,inChannels:c,outHeight:v,outWidth:S,outChannels:I,padInfo:x,strideHeight:g,strideWidth:m,filterHeight:p,filterWidth:d,effectiveFilterHeight:w,effectiveFilterWidth:k,dilationHeight:y,dilationWidth:b,inShape:t,outShape:N,filterShape:e}}(t,u,n,s,i,r,!1,a)}function kn(t){return"number"==typeof t?[t,t,t]:2===t.length?[t[0],t[1],1]:t}function xn(t,e){return e<=1?t:t+(t-1)*(e-1)}function vn(t,e){if(!e)return Math.trunc(t);switch(e){case"round":return Math.round(t);case"ceil":return Math.ceil(t);case"floor":return Math.floor(t);default:throw new Error(`Unknown roundingMode ${e}`)}}function Sn(t){const[e,n,s]=kn(t);return 1===e&&1===n&&1===s}function In(t,e){return Sn(t)||Sn(e)}const Nn=sn({reshape_:function(t,e){const n={x:en(t,"x","reshape","string_or_numeric")},s={shape:e};return Ze.runKernel("Reshape",n,s)}});const An=sn({avgPool_:function(t,e,n,s,i){const r=en(t,"x","avgPool","float32");qt(In(n,1),(()=>`Error in avgPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`));let a=r,o=!1;3===r.rank&&(o=!0,a=Nn(r,[1,r.shape[0],r.shape[1],r.shape[2]])),qt(4===a.rank,(()=>`Error in avgPool: x must be rank 4 but got rank ${a.rank}.`)),null!=i&&qt(Zt(s),(()=>`Error in avgPool: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`));const l={x:a},u={filterSize:e,strides:n,pad:s,dimRoundingMode:i};let h=Ze.runKernel("AvgPool",l,u);return h=bn(h,r.dtype),o?Nn(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const Cn=sn({clone_:function(t){const e={x:en(t,"x","clone","string_or_numeric")};return Ze.runKernel("Identity",e)}});const zn=sn({concat_:function(t,e=0){qt(t.length>=1,(()=>"Pass at least one tensor to concat"));const n=nn(t,"tensors","concat","string_or_numeric");if("complex64"===n[0].dtype&&n.forEach((t=>{if("complex64"!==t.dtype)throw new Error(`Cannot concatenate complex64 tensors with a tensor\n          with dtype ${t.dtype}. `)})),1===n.length)return Cn(n[0]);const s=n,i={axis:e};return Ze.runKernel("Concat",s,i)}});const Dn=sn({matMul_:function(t,e,n=!1,s=!1){let i=en(t,"a","matMul"),r=en(e,"b","matMul");[i,r]=je(i,r);const a={a:i,b:r},o={transposeA:n,transposeB:s};return Ze.runKernel("BatchMatMul",a,o)}});const $n=sn({mul_:function(t,e){let n=en(t,"a","mul"),s=en(e,"b","mul");[n,s]=je(n,s);const i={a:n,b:s};return Ze.runKernel("Multiply",i)}});const Tn=sn({sigmoid_:function(t){const e={x:en(t,"x","sigmoid")};return Ze.runKernel("Sigmoid",e)}});const En=sn({slice_:function(t,e,n){const s=en(t,"x","slice","string_or_numeric");if(0===s.rank)throw new Error("Slicing scalar is not possible");const i={x:s},r={begin:e,size:n};return Ze.runKernel("Slice",i,r)}});const Fn=sn({tanh_:function(t){const e={x:en(t,"x","tanh")};return Ze.runKernel("Tanh",e)}});const Ln=sn({batchToSpaceND_:function(t,e,n){const s=en(t,"x","batchToSpaceND"),i=e.reduce(((t,e)=>t*e));qt(s.rank>=1+e.length,(()=>`input rank is ${s.rank} but should be > than blockShape.length ${e.length}`)),qt(n.length===e.length,(()=>`crops.length is ${n.length} but should be equal to blockShape.length  ${e.length}`)),qt(s.shape[0]%i==0,(()=>`input tensor batch is ${s.shape[0]} but is not divisible by the product of the elements of blockShape ${e.join(" * ")} === ${i}`));const r={x:s},a={blockShape:e,crops:n};return Ze.runKernel("BatchToSpaceND",r,a)}});const _n=sn({batchNorm_:function(t,e,n,s,i,r){null==r&&(r=.001);const a=en(t,"x","batchNorm"),o=en(e,"mean","batchNorm"),l=en(n,"variance","batchNorm");let u,h;null!=i&&(u=en(i,"scale","batchNorm")),null!=s&&(h=en(s,"offset","batchNorm")),qt(o.rank===l.rank,(()=>"Batch normalization gradient requires mean and variance to have equal ranks.")),qt(null==h||o.rank===h.rank,(()=>"Batch normalization gradient requires mean and offset to have equal ranks.")),qt(null==u||o.rank===u.rank,(()=>"Batch normalization gradient requires mean and scale to have equal ranks."));const c={x:function(t){let e;return e=0===t.rank||1===t.rank?Nn(t,[1,1,1,t.size]):2===t.rank?Nn(t,[1,1,t.shape[0],t.shape[1]]):3===t.rank?Nn(t,[1,t.shape[0],t.shape[1],t.shape[2]]):t,e}(a),scale:u,offset:h,mean:o,variance:l},p={varianceEpsilon:r},d=Ze.runKernel("FusedBatchNorm",c,p);return Nn(d,a.shape)}});const Rn=sn({broadcastTo_:function(t,e){let n=en(t,"broadcastTo","x");const s=n.shape;if(e.some((t=>!(t>0)||t%1!=0)))throw new Error(`broadcastTo(): Invalid broadcast shape [${e}].`);if(e.length<n.rank)throw new Error(`broadcastTo(): shape.length=${e.length} < input.rank=${n.rank}.`);if(e.length>n.rank){const t=n.shape.slice();for(;t.length<e.length;)t.unshift(1);n=Nn(n,t)}const i=n.shape,r=Array.from(e);for(let t=e.length-1;t>=0;t--)if(i[t]===e[t])r[t]=1;else if(1!==n.shape[t])throw new Error(`broadcastTo(): [${s}] cannot be broadcast to [${e}].`);if(0===r.map(((t,e)=>t>1?e:-1)).filter((t=>t>=0)).length)return Cn(n);const a={x:n},o={reps:r};return Ze.runKernel("Tile",a,o)}});const Mn=sn({ceil_:function(t){const e={x:en(t,"x","ceil")};return Ze.runKernel("Ceil",e)}});const On=sn({clipByValue_:function(t,e,n){const s=en(t,"x","clipByValue");qt(e<=n,(()=>`Error in clip: min (${e}) must be less than or equal to max (${n}).`));const i={x:s},r={clipValueMin:e,clipValueMax:n};return Ze.runKernel("ClipByValue",i,r)}});const Bn=sn({complex_:function(t,e){const n=en(t,"real","complex"),s=en(e,"imag","complex");!function(t,e,n=""){qt(Jt(t,e),(()=>n+` Shapes ${t} and ${e} must match`))}(n.shape,s.shape,`real and imag shapes, ${n.shape} and ${s.shape}, must match in call to tf.complex().`);const i={real:n,imag:s};return Ze.runKernel("Complex",i)}});const Pn=sn({conv2d_:function(t,e,n,s,i="NHWC",r=[1,1],a){const o=en(t,"x","conv2d"),l=en(e,"filter","conv2d");let u=o,h=!1;3===o.rank&&(h=!0,u=Nn(o,[1,o.shape[0],o.shape[1],o.shape[2]])),qt(4===u.rank,(()=>`Error in conv2d: input must be rank 4, but got rank ${u.rank}.`)),qt(4===l.rank,(()=>`Error in conv2d: filter must be rank 4, but got rank ${l.rank}.`)),null!=a&&qt(Zt(s),(()=>`Error in conv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`));const c="NHWC"===i?u.shape[3]:u.shape[1];qt(c===l.shape[2],(()=>`Error in conv2d: depth of input (${c}) must match input depth for filter ${l.shape[2]}.`)),qt(In(n,r),(()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${r}'`));const p={x:u,filter:l},d={strides:n,pad:s,dataFormat:i,dilations:r,dimRoundingMode:a},f=Ze.runKernel("Conv2D",p,d);return h?Nn(f,[f.shape[1],f.shape[2],f.shape[3]]):f}});const Wn=sn({conv1d_:function(t,e,n,s,i="NWC",r=1,a){const o=en(t,"x","conv1d"),l=en(e,"filter","conv1d");let u=o,h=!1;2===o.rank&&(h=!0,u=Nn(o,[1,o.shape[0],o.shape[1]])),qt(3===u.rank,(()=>`Error in conv1d: input must be rank 3, but got rank ${u.rank}.`)),qt(3===l.rank,(()=>`Error in conv1d: filter must be rank 3, but got rank ${l.rank}.`)),null!=a&&qt(Zt(s),(()=>`Error in conv1d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`)),qt(u.shape[2]===l.shape[1],(()=>`Error in conv1d: depth of input (${u.shape[2]}) must match input depth for filter ${l.shape[1]}.`)),qt(In(n,r),(()=>`Error in conv1D: Either stride or dilation must be 1. Got stride ${n} and dilation '${r}'`)),qt("NWC"===i,(()=>`Error in conv1d: got dataFormat of ${i} but only NWC is currently supported.`));const c=Nn(l,[1,l.shape[0],l.shape[1],l.shape[2]]),p=Nn(u,[u.shape[0],1,u.shape[1],u.shape[2]]),d=Pn(p,c,[1,n],s,"NHWC",[1,r],a);return Nn(d,h?[d.shape[2],d.shape[3]]:[d.shape[0],d.shape[2],d.shape[3]])}});const Un=sn({conv2DBackpropInput_:function(t,e,n,s,i,r="NHWC",a){qt(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let o=t,l=e,u=!1;3===e.rank&&(u=!0,l=Nn(e,[1,e.shape[0],e.shape[1],e.shape[2]]),o=[1,t[0],t[1],t[2]]),qt(4===o.length,(()=>`Error in conv2dDerInput: inShape must be length 4, but got length ${o.length}.`)),qt(4===l.rank,(()=>`Error in conv2dDerInput: dy must be rank 4, but got rank ${l.rank}`)),qt(4===n.rank,(()=>`Error in conv2dDerInput: filter must be rank 4, but got rank ${n.rank}`));const h="NHWC"===r?o[3]:o[1],c="NHWC"===r?l.shape[3]:l.shape[1];qt(h===n.shape[2],(()=>`Error in conv2dDerInput: depth of input (${h}) must match input depth for filter ${n.shape[2]}.`)),qt(c===n.shape[3],(()=>`Error in conv2dDerInput: depth of output (${c}) must match output depth for filter ${n.shape[3]}.`)),null!=a&&qt(Zt(i),(()=>`Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const p={dy:l,filter:n},d={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,inputShape:o},f=Ze.runKernel("Conv2DBackpropInput",p,d);return u?Nn(f,[f.shape[1],f.shape[2],f.shape[3]]):f}});const Kn=sn({conv2dTranspose_:function(t,e,n,s,i,r){const a=en(t,"x","conv2dTranspose"),o=en(e,"filter","conv2dTranspose");return Un(n,a,o,s,i,"NHWC",r)}});const jn=sn({conv3DBackpropInput_:function(t,e,n,s,i){qt(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let r=t,a=e,o=!1;4===e.rank&&(o=!0,a=Nn(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]]),r=[1,t[0],t[1],t[2],t[3]]);const l=r[4],u=a.shape[4];qt(5===r.length,(()=>`Error in conv3dDerInput: inShape must be length 5, but got length ${r.length}.`)),qt(5===a.rank,(()=>`Error in conv3dDerInput: dy must be rank 5, but got rank ${a.rank}`)),qt(5===n.rank,(()=>`Error in conv3dDerInput: filter must be rank 5, but got rank ${n.rank}`)),qt(l===n.shape[3],(()=>`Error in conv3dDerInput: depth of input (${l}) must match input depth for filter ${n.shape[3]}.`)),qt(u===n.shape[4],(()=>`Error in conv3dDerInput: depth of output (${u}) must match output depth for filter ${n.shape[4]}.`));const h={dy:a,filter:n},c={pad:i,strides:s,inputShape:r},p=Ze.runKernel("Conv3DBackpropInputV2",h,c);return o?Nn(p,[p.shape[1],p.shape[2],p.shape[3],p.shape[4]]):p}});const Vn=sn({cos_:function(t){const e={x:en(t,"x","cos")};return Ze.runKernel("Cos",e)}});const qn=sn({cosh_:function(t){const e={x:en(t,"x","cosh")};return Ze.runKernel("Cosh",e)}});const Gn=sn({cumsum_:function(t,e=0,n=!1,s=!1){const i={x:en(t,"x","cumsum")},r={axis:e,exclusive:n,reverse:s};return Ze.runKernel("Cumsum",i,r)}});const Hn=sn({depthToSpace_:function(t,e,n="NHWC"){const s=en(t,"x","depthToSpace"),i="NHWC"===n?s.shape[1]:s.shape[2],r="NHWC"===n?s.shape[2]:s.shape[3],a="NHWC"===n?s.shape[3]:s.shape[1];qt(i*e>=0,(()=>`Negative dimension size caused by overflow when multiplying\n    ${i} and ${e}  for depthToSpace with input shape\n    ${s.shape}`)),qt(r*e>=0,(()=>`Negative dimension size caused by overflow when multiplying\n    ${r} and ${e} for depthToSpace with input shape\n        ${s.shape}`)),qt(a%(e*e)==0,(()=>`Dimension size must be evenly divisible by ${e*e} but is ${a} for depthToSpace with input shape ${s.shape}`));const o={x:s},l={blockSize:e,dataFormat:n};return Ze.runKernel("DepthToSpace",o,l)}});const Jn=sn({depthwiseConv2d_:function(t,e,n,s,i="NHWC",r=[1,1],a){const o=en(t,"x","depthwiseConv2d"),l=en(e,"filter","depthwiseConv2d");let u=o,h=!1;3===o.rank&&(h=!0,u=Nn(o,[1,o.shape[0],o.shape[1],o.shape[2]])),qt(4===u.rank,(()=>`Error in depthwiseConv2d: input must be rank 4, but got rank ${u.rank}.`)),qt(4===l.rank,(()=>`Error in depthwiseConv2d: filter must be rank 4, but got rank ${l.rank}.`)),qt(u.shape[3]===l.shape[2],(()=>`Error in depthwiseConv2d: number of input channels (${u.shape[3]}) must match the inChannels dimension in filter ${l.shape[2]}.`)),null!=a&&qt(Zt(s),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`));const c={x:u,filter:l},p={strides:n,pad:s,dataFormat:i,dilations:r,dimRoundingMode:a},d=Ze.runKernel("DepthwiseConv2dNative",c,p);return h?Nn(d,[d.shape[1],d.shape[2],d.shape[3]]):d}});const Zn=sn({dilation2d_:function(t,e,n,s,i=[1,1],r="NHWC"){const a=en(t,"x","dilation2d"),o=en(e,"filter","dilation2d");qt(3===a.rank||4===a.rank,(()=>`Error in dilation2d: input must be rank 3 or 4, but got rank ${a.rank}.`)),qt(3===o.rank,(()=>`Error in dilation2d: filter must be rank 3, but got rank ${o.rank}.`)),qt("NHWC"===r,(()=>`Error in dilation2d: Only NHWC is currently supported, but got dataFormat of ${r}`));let l=a,u=!1;3===a.rank&&(l=Nn(a,[1,a.shape[0],a.shape[1],a.shape[2]]),u=!0);const h={x:l,filter:o},c={strides:n,pad:s,dilations:i},p=Ze.runKernel("Dilation2D",h,c);return u?Nn(p,[p.shape[1],p.shape[2],p.shape[3]]):p}});const Xn=sn({floorDiv_:function(t,e){let n=en(t,"a","floorDiv"),s=en(e,"b","floorDiv");[n,s]=je(n,s);const i={a:n,b:s};return Ze.runKernel("FloorDiv",i)}});const Yn=sn({div_:function(t,e){let n=en(t,"a","div"),s=en(e,"b","div");if([n,s]=je(n,s),"int32"===n.dtype&&"int32"===s.dtype)return Xn(n,s);const i={a:n,b:s};return Ze.runKernel("RealDiv",i,{})}});function Qn(t,e){const n=[];for(let s=0;s<e.length;s++){const i=t[t.length-s-1],r=e.length-s-1,a=e[r];(null==i||1===i&&a>1)&&n.unshift(r)}return n}function ts(t,e){const n=[],s=Math.max(t.length,e.length);for(let i=0;i<s;i++){let s=t[t.length-i-1];null==s&&(s=1);let r=e[e.length-i-1];if(null==r&&(r=1),1===s)n.unshift(r);else if(1===r)n.unshift(s);else{if(s!==r){throw Error(`Operands could not be broadcast together with shapes ${t} and ${e}.`)}n.unshift(s)}}return n}const es=sn({equal_:function(t,e){let n=en(t,"a","equal"),s=en(e,"b","equal");[n,s]=je(n,s),ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("Equal",i)}});const ns=sn({where_:function(t,e,n){const s=en(e,"a","where"),i=en(n,"b","where"),r=en(t,"condition","where","bool"),a=ts(ts(r.shape,s.shape),i.shape),o={condition:Rn(r,a),t:Rn(s,a),e:Rn(i,a)};return Ze.runKernel("Select",o)}});const ss=sn({zerosLike_:function(t){const e={x:en(t,"x","zerosLike")};return Ze.runKernel("ZerosLike",e)}});const is=sn({divNoNan_:function(t,e){let n=en(t,"a","div"),s=en(e,"b","div");[n,s]=je(n,s);const i=Yn(n,s),r=ss(i),a=es(s,r);return ns(a,r,i)}});const rs=sn({dot_:function(t,e){const n=en(t,"t1","dot"),s=en(e,"t2","dot");qt(!(1!==n.rank&&2!==n.rank||1!==s.rank&&2!==s.rank),(()=>`Error in dot: inputs must all be rank 1 or 2, but got ranks ${n.rank} and ${s.rank}.`));const i=1===n.rank?n.size:n.shape[1],r=1===s.rank?s.size:s.shape[0];if(qt(i===r,(()=>`Error in dot: inner dimensions of inputs must match, but got ${i} and ${r}.`)),1===n.rank&&1===s.rank){const t=Nn(n,[1,-1]),e=Nn(s,[-1,1]),i=Dn(t,e);return Nn(i,[])}if(1===n.rank&&2===s.rank){const t=Nn(n,[1,-1]),e=Nn(s,[s.shape[0],s.shape[1]]),i=Dn(t,e);return Nn(i,[i.size])}if(2===n.rank&&1===s.rank){const t=Nn(s,[-1,1]),e=Dn(n,t);return Nn(e,[e.size])}{const t=Nn(s,[s.shape[0],s.shape[1]]);return Dn(n,t)}}});const as=sn({elu_:function(t){const e={x:en(t,"x","elu")};return Ze.runKernel("Elu",e)}});const os=sn({erf_:function(t){let e=en(t,"x","erf");qt("int32"===e.dtype||"float32"===e.dtype,(()=>"Input dtype must be `int32` or `float32`.")),"int32"===e.dtype&&(e=bn(e,"float32"));const n={x:e};return Ze.runKernel("Erf",n)}});const ls=sn({exp_:function(t){const e={x:en(t,"x","exp")};return Ze.runKernel("Exp",e)}});const us=sn({expandDims_:function(t,e=0){const n=en(t,"x","expandDims","string_or_numeric");qt(e<=n.rank,(()=>"Axis must be <= rank of the tensor"));const s={input:n},i={dim:e};return Ze.runKernel("ExpandDims",s,i)}});const hs=sn({expm1_:function(t){const e={x:en(t,"x","expm1")};return Ze.runKernel("Expm1",e)}});const cs=sn({tile_:function(t,e){const n=en(t,"x","tile","string_or_numeric");qt(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of reps ${e}.`));const s={x:n},i={reps:e};return Ze.runKernel("Tile",s,i)}});const ps=sn({floor_:function(t){const e={x:en(t,"x","floor")};return Ze.runKernel("Floor",e)}});const ds=sn({gather_:function(t,e,n=0,s=0){const i={x:en(t,"x","gather"),indices:en(e,"indices","gather","int32")},r={axis:n,batchDims:s};return Ze.runKernel("GatherV2",i,r)}});const fs=sn({greater_:function(t,e){let n=en(t,"a","greater"),s=en(e,"b","greater");[n,s]=je(n,s),ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("Greater",i)}});const gs=sn({greaterEqual_:function(t,e){let n=en(t,"a","greaterEqual"),s=en(e,"b","greaterEqual");[n,s]=je(n,s),ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("GreaterEqual",i)}});const ms=sn({imag_:function(t){const e={input:en(t,"input","imag")};return Ze.runKernel("Imag",e)}});const ys=sn({isFinite_:function(t){const e={x:en(t,"x","isFinite")};return Ze.runKernel("IsFinite",e)}});const bs=sn({isInf_:function(t){const e={x:en(t,"x","isInf")};return Ze.runKernel("IsInf",e)}});const ws=sn({isNaN_:function(t){const e={x:en(t,"x","isNaN")};return Ze.runKernel("IsNan",e)}});const ks=sn({leakyRelu_:function(t,e=.2){const n={x:en(t,"x","leakyRelu")},s={alpha:e};return Ze.runKernel("LeakyRelu",n,s)}});const xs=sn({less_:function(t,e){let n=en(t,"a","less"),s=en(e,"b","less");[n,s]=je(n,s),ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("Less",i)}});const vs=sn({lessEqual_:function(t,e){let n=en(t,"a","lessEqual"),s=en(e,"b","lessEqual");[n,s]=je(n,s),ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("LessEqual",i)}});const Ss=sn({localResponseNormalization_:function(t,e=5,n=1,s=1,i=.5){const r=en(t,"x","localResponseNormalization");qt(4===r.rank||3===r.rank,(()=>`Error in localResponseNormalization: x must be rank 3 or 4 but got\n               rank ${r.rank}.`)),qt(Zt(e),(()=>`Error in localResponseNormalization: depthRadius must be an integer but got depthRadius ${e}.`));let a=r,o=!1;3===r.rank&&(o=!0,a=Nn(r,[1,r.shape[0],r.shape[1],r.shape[2]]));const l={x:a},u={depthRadius:e,bias:n,alpha:s,beta:i},h=Ze.runKernel("LRN",l,u);return o?Nn(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const Is=sn({log_:function(t){const e={x:en(t,"x","log")};return Ze.runKernel("Log",e)}});const Ns=sn({log1p_:function(t){const e={x:en(t,"x","log1p")};return Ze.runKernel("Log1p",e)}});function As(t){return Ze.customGrad(t)}const Cs=sn({neg_:function(t){const e={x:en(t,"x","neg")};return Ze.runKernel("Neg",e)}});const zs=sn({softplus_:function(t){const e={x:en(t,"x","softplus")};return Ze.runKernel("Softplus",e)}});const Ds=sn({logSigmoid_:function(t){const e=en(t,"x","logSigmoid");return As((t=>({value:Cs(zs(Cs(t))),gradFunc:e=>$n(e,Tn(Cs(t)))})))(e)}});const $s=sn({max_:function(t,e=null,n=!1){const s={x:en(t,"x","max")},i={reductionIndices:e,keepDims:n};return Ze.runKernel("Max",s,i)}});const Ts=sn({sub_:function(t,e){let n=en(t,"a","sub"),s=en(e,"b","sub");[n,s]=je(n,s);const i={a:n,b:s};return Ze.runKernel("Sub",i)}});const Es=sn({sum_:function(t,e=null,n=!1){let s=en(t,"x","sum");"bool"===s.dtype&&(s=bn(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Ze.runKernel("Sum",i,r)}});const Fs=sn({logSoftmax_:function(t,e=-1){const n=en(t,"logits","logSoftmax");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Log Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and axis was ${e}`);return As(((t,n)=>{const s=$s(t,e,!0),i=Ts(t,s),r=Ts(bn(i,"float32"),Is(Es(ls(i),e,!0)));n([r]);return{value:r,gradFunc:(t,n)=>{const[s]=n,i=ls(s);return Ts(t,$n(Es(t,e,!0),i))}}}))(n)}});function Ls(t,e){return function(t,e,n){const s=t.length+e.length,i=[];let r=0,a=0;for(let o=0;o<s;o++)-1===n.indexOf(o)?i.push(t[r++]):i.push(e[a++]);return i}(t,e.map((t=>1)),e)}function _s(t){return t.map(((t,e)=>[e,t])).sort(((t,e)=>t[1]-e[1])).map((t=>t[0]))}const Rs=sn({logSumExp_:function(t,e=null,n=!1){const s=en(t,"x","logSumExp"),i=Yt(e,s.shape),r=$s(s,i,!0),a=Ts(s,r),o=ls(a),l=Es(o,i),u=Is(l),h=ln(Nn(r,u.shape),u);if(n){const t=Ls(h.shape,i);return Nn(h,t)}return h}});const Ms=sn({logicalAnd_:function(t,e){const n=en(t,"a","logicalAnd","bool"),s=en(e,"b","logicalAnd","bool");ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("LogicalAnd",i)}});const Os=sn({logicalNot_:function(t){const e={x:en(t,"x","logicalNot","bool")};return Ze.runKernel("LogicalNot",e)}});const Bs=sn({logicalOr_:function(t,e){const n=en(t,"a","logicalOr","bool"),s=en(e,"b","logicalOr","bool");ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("LogicalOr",i)}});const Ps=sn({logicalXor_:function(t,e){const n=en(t,"a","logicalXor","bool"),s=en(e,"b","logicalXor","bool");return ts(n.shape,s.shape),Ms(Bs(t,e),Os(Ms(t,e)))}});const Ws=sn({maxPool_:function(t,e,n,s,i){const r=en(t,"x","maxPool");let a=r,o=!1;3===r.rank&&(o=!0,a=Nn(r,[1,r.shape[0],r.shape[1],r.shape[2]])),qt(4===a.rank,(()=>`Error in maxPool: input must be rank 4 but got rank ${a.rank}.`)),qt(In(n,1),(()=>`Error in maxPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`)),null!=i&&qt(Zt(s),(()=>`Error in maxPool: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`));const l={x:a},u={filterSize:e,strides:n,pad:s,dimRoundingMode:i},h=Ze.runKernel("MaxPool",l,u);return o?Nn(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const Us=sn({maximum_:function(t,e){let n=en(t,"a","maximum"),s=en(e,"b","maximum");[n,s]=je(n,s),"bool"===n.dtype&&(n=bn(n,"int32"),s=bn(s,"int32")),ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("Maximum",i)}});const Ks=sn({mean_:function(t,e=null,n=!1){const s={x:en(t,"x","mean")},i={axis:e,keepDims:n};return Ze.runKernel("Mean",s,i)}});function js(t,e="float32"){if("complex64"===e){const e=js(t,"float32"),n=js(t,"float32");return Bn(e,n)}const n=le(Ht(t),e);return Ze.makeTensor(n,t,e)}function Vs(t,e="float32"){if("complex64"===e){const e=Vs(t,"float32"),n=js(t,"float32");return Bn(e,n)}const n=oe(Ht(t),e);return Ze.makeTensor(n,t,e)}const qs=sn({min_:function(t,e=null,n=!1){const s={x:en(t,"x","min")},i={axis:e,keepDims:n};return Ze.runKernel("Min",s,i)}});const Gs=sn({minimum_:function(t,e){let n=en(t,"a","minimum"),s=en(e,"b","minimum");[n,s]=je(n,s),"bool"===n.dtype&&(n=bn(n,"int32"),s=bn(s,"int32")),ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("Minimum",i)}});const Hs=sn({mirrorPad_:function(t,e,n){qt("reflect"===n||"symmetric"===n,(()=>`Invalid mode. Mode must be either reflect or symmetric. Got ${n}.`));const s=en(t,"x","mirrorPad");if(0===s.rank)throw new Error("mirrorPad(scalar) is not defined. Pass non-scalar to mirrorPad");qt(e.length===s.rank,(()=>`Padding doesn't match input. Must be ${s.rank}. Got ${e.length}.`));const i="reflect"===n?1:0;for(let t=0;t<s.rank;t++)qt(2===e[t].length,(()=>"Invalid number of paddings. Must be length of 2 each.")),qt(e[t][0]>=0&&e[t][0]<=s.shape[t]-i&&e[t][1]>=0&&e[t][1]<=s.shape[t]-i,(()=>`Padding in dimension ${t} cannot be greater than or equal to ${s.shape[t]-i} or less than 0 for input of shape ${s.shape}`));const r={paddings:e,mode:n},a={x:s};return Ze.runKernel("MirrorPad",a,r)}});const Js=sn({mod_:function(t,e){let n=en(t,"a","mod"),s=en(e,"b","mod");[n,s]=je(n,s);const i={a:n,b:s};return Ze.runKernel("Mod",i)}});const Zs=sn({square_:function(t){const e=en(t,"x","square");return Ze.runKernel("Square",{x:e},{})}});const Xs=sn({notEqual_:function(t,e){let n=en(t,"a","notEqual"),s=en(e,"b","notEqual");[n,s]=je(n,s),ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("NotEqual",i)}});const Ys=sn({oneHot_:function(t,e,n=1,s=0){if(e<2)throw new Error(`Error in oneHot: depth must be >=2, but it is ${e}`);const i={indices:en(t,"indices","oneHot","int32")},r={depth:e,onValue:n,offValue:s};return Ze.runKernel("OneHot",i,r)}});const Qs=sn({onesLike_:function(t){const e={x:en(t,"x","onesLike")};return Ze.runKernel("OnesLike",e)}});const ti=sn({pad_:function(t,e,n=0){const s=en(t,"x","pad");if(0===s.rank)throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");const i={paddings:e,constantValue:n},r={x:s};return Ze.runKernel("PadV2",r,i)}});const ei=sn({spaceToBatchND_:function(t,e,n){const s=en(t,"x","spaceToBatchND");qt(s.rank>=1+e.length,(()=>`input rank ${s.rank} should be > than [blockShape] ${e.length}`)),qt(n.length===e.length,(()=>`paddings.shape[0] ${n.length} must be equal to [blockShape] ${e.length}`)),qt(s.shape.reduce(((t,s,i)=>i>0&&i<=e.length?t&&(s+n[i-1][0]+n[i-1][1])%e[i-1]==0:t),!0),(()=>`input spatial dimensions ${s.shape.slice(1)} with paddings ${n.toString()} must be divisible by blockShapes ${e.toString()}`));const i={x:s},r={blockShape:e,paddings:n};return Ze.runKernel("SpaceToBatchND",i,r)}});const ni=sn({pool_:function(t,e,n,s,i,r){null==i&&(i=[1,1]),null==r&&(r=1),0===s&&(s="valid");const a=en(t,"x","maxPool");let o=a,l=!1;3===a.rank&&(l=!0,o=Nn(a,[1,a.shape[0],a.shape[1],a.shape[2]])),qt(In(r,i),(()=>`Error in pool: Either strides or dilations must be 1. Got strides ${r} and dilations '${i}'`));const u=wn(o.shape,e,r,i,s),h=[u.dilationHeight,u.dilationWidth];let c;c="same"===s?function(t,e){const n=t.map(((t,n)=>t+(t-1)*(e[n]-1))).map((t=>t-1)),s=n.map((t=>Math.floor(t/2))),i=n.map(((t,e)=>t-s[e]));return n.map(((t,e)=>[s[e],i[e]]))}([u.filterHeight,u.filterWidth],h):[[0,0],[0,0]];const p=1===h[0]&&1===h[1],[d,f]=function(t,e,n){const s=n.map((t=>t[0])),i=n.map((t=>t[1])),r=t.concat(s,i),a=e.map(((t,e)=>(t-r[e]%t)%t)),o=i.map(((t,e)=>t+a[e])),l=e.map(((t,e)=>[s[e],o[e]])),u=e.map(((t,e)=>[0,a[e]]));return[l,u]}([u.inHeight,u.inWidth],h,c),g=p?s:"valid",m=p?o:ei(o,h,d),y=("avg"===n?()=>An(m,e,r,g):()=>Ws(m,e,r,g))(),b=p?y:Ln(y,h,f);return l?Nn(b,[b.shape[1],b.shape[2],b.shape[3]]):b}});const si=sn({pow_:function(t,e){let n=en(t,"base","pow"),s=en(e,"exp","pow");[n,s]=je(n,s);const i={a:n,b:s};return Ze.runKernel("Pow",i)}});const ii=sn({prelu_:function(t,e){const n={x:en(t,"x","prelu"),alpha:en(e,"alpha","prelu")};return Ze.runKernel("Prelu",n)}});const ri=sn({prod_:function(t,e=null,n=!1){let s=en(t,"x","prod");"bool"===s.dtype&&(s=bn(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Ze.runKernel("Prod",i,r)}});const ai=sn({real_:function(t){const e={input:en(t,"input","real")};return Ze.runKernel("Real",e)}});const oi=sn({reciprocal_:function(t){const e={x:en(t,"x","reciprocal")};return Ze.runKernel("Reciprocal",e)}});const li=sn({relu_:function(t){const e={x:en(t,"x","relu")};return Ze.runKernel("Relu",e)}});const ui=sn({relu6_:function(t){const e={x:en(t,"x","relu6")};return Ze.runKernel("Relu6",e)}});const hi=sn({reverse_:function(t,e){const n={x:en(t,"x","reverse")},s={dims:e};return Ze.runKernel("Reverse",n,s)}});const ci=sn({round_:function(t){const e={x:en(t,"x","round")};return Ze.runKernel("Round",e)}});const pi=sn({rsqrt_:function(t){const e={x:en(t,"x","rsqrt")};return Ze.runKernel("Rsqrt",e)}});function di(t,e,n,s){if(null==s&&(s=ne(t)),"complex64"===s)throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");if(!Qt(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t)throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");if(null!=e){!function(t){t.forEach((e=>{qt(Number.isInteger(e)&&e>=0,(()=>`Tensor must have a shape comprised of positive integers but got shape [${t}].`))}))}(e);const t=Ht(e),s=Ht(n);qt(t===s,(()=>`Based on the provided shape, [${e}], the tensor should have ${t} values but has ${s}`));for(let t=0;t<n.length;++t){const s=n[t],i=t!==n.length-1||s!==Ht(e.slice(t));qt(n[t]===e[t]||!i,(()=>`Error creating a new Tensor. Inferred shape (${n}) does not match the provided shape (${e}). `))}}return Qt(t)||Array.isArray(t)||(t=[t]),e=e||n,t="string"!==s?Se(t,s):Gt(t,[],!0),Ze.makeTensor(t,e,s)}function fi(t,e){if((Qt(t)&&"string"!==e||Array.isArray(t))&&"complex64"!==e)throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");if("string"===e&&Qt(t)&&!(t instanceof Uint8Array))throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");return di(t,[],[],e)}const gi=sn({selu_:function(t){const e={x:en(t,"x","selu")};return Ze.runKernel("Selu",e)}});const mi=sn({separableConv2d_:function(t,e,n,s,i,r=[1,1],a="NHWC"){const o=en(t,"x","separableConv2d"),l=en(e,"depthwiseFilter","separableConv2d"),u=en(n,"pointwiseFilter","separableConv2d");let h=o,c=!1;if(3===o.rank&&(c=!0,h=Nn(o,[1,o.shape[0],o.shape[1],o.shape[2]])),"NCHW"===a)throw new Error("separableConv2d currently does not support dataFormat NCHW; only NHWC is supported");qt(4===h.rank,(()=>`Error in separableConv2d: input must be rank 4, but got rank ${h.rank}.`)),qt(4===l.rank,(()=>`Error in separableConv2d: depthwise filter must be rank 4, but got rank ${l.rank}.`)),qt(4===u.rank,(()=>`Error in separableConv2d: pointwise filter must be rank 4, but got rank ${l.rank}.`)),qt(1===u.shape[0],(()=>`Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got ${u.shape[0]}.`)),qt(1===u.shape[1],(()=>`Error in separableConv2d: the second dimension of pointwise filter must be 1, but got ${u.shape[1]}.`));const p=l.shape[2],d=l.shape[3];qt(u.shape[2]===p*d,(()=>`Error in separableConv2d: the third dimension of pointwise filter must be ${p*d}, but got ${u.shape[2]}.`));const f=Jn(h,l,s,i,a,r),g=Pn(f,u,1,"valid",a);return c?Nn(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});const yi=sn({sign_:function(t){const e={x:en(t,"x","sign")};return Ze.runKernel("Sign",e)}});const bi=sn({sin_:function(t){const e={x:en(t,"x","sin")};return Ze.runKernel("Sin",e)}});const wi=sn({sinh_:function(t){const e={x:en(t,"x","sinh")};return Ze.runKernel("Sinh",e)}});const ki=sn({softmax_:function(t,e=-1){const n=en(t,"logits","softmax","float32");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and dim was ${e}`);const s={logits:n},i={dim:e};return Ze.runKernel("Softmax",s,i)}});const xi=sn({fft_:function(t){qt("complex64"===t.dtype,(()=>`The dtype for tf.spectral.fft() must be complex64 but got ${t.dtype}.`));const e={input:t};return Ze.runKernel("FFT",e)}});const vi=sn({ifft_:function(t){qt("complex64"===t.dtype,(()=>`The dtype for tf.spectral.ifft() must be complex64 but got ${t.dtype}.`));const e={input:t};return Ze.runKernel("IFFT",e)}});const Si=sn({irfft_:function(t){const e=t.shape[t.shape.length-1],n=t.size/e;let s;if(e<=2){const i=Nn(t,[n,e]);s=vi(i)}else{const i=[n,2*(e-1)],r=Nn(ai(t),[n,e]),a=Nn(ms(t),[n,e]),o=hi(En(r,[0,1],[n,e-2]),1),l=$n(hi(En(a,[0,1],[n,e-2]),1),fi(-1)),u=zn([r,o],1),h=zn([a,l],1),c=Nn(Bn(u,h),[i[0],i[1]]);s=vi(c)}if(s=ai(s),3===t.rank&&0!==t.shape[0]){const e=s,n=t.shape[0];s=Nn(s,[n,s.shape[0]/n,s.shape[1]]),e.dispose()}return s}});const Ii=sn({split_:function(t,e,n=0){const s={x:en(t,"x","split")},i={numOrSizeSplits:e,axis:n};return Ze.runKernel("SplitV",s,i)}});const Ni=sn({rfft_:function(t,e){qt("float32"===t.dtype,(()=>`The dtype for rfft() must be real value but got ${t.dtype}`));let n=t.shape[t.shape.length-1];const s=t.size/n;let i;if(null!=e&&e<n){const s=t.shape.map((t=>0)),r=t.shape.map((t=>t));r[t.shape.length-1]=e,i=En(t,s,r),n=e}else if(null!=e&&e>n){const s=t.shape.map((t=>t));s[t.shape.length-1]=e-n,i=zn([t,js(s)],t.shape.length-1),n=e}else i=t;const r=ss(i),a=Nn(Bn(i,r),[s,n]),o=xi(a),l=Math.floor(n/2)+1,u=ai(o),h=ms(o),c=Ii(u,[l,n-l],u.shape.length-1),p=Ii(h,[l,n-l],h.shape.length-1),d=i.shape.slice();return d[i.shape.length-1]=l,Nn(Bn(c[0],p[0]),d)}});const Ai=sn({sqrt_:function(t){const e={x:en(t,"x","sqrt")};return Ze.runKernel("Sqrt",e)}});const Ci=sn({squaredDifference_:function(t,e){let n=en(t,"a","squaredDifference"),s=en(e,"b","squaredDifference");[n,s]=je(n,s),ts(n.shape,s.shape);const i={a:n,b:s};return Ze.runKernel("SquaredDifference",i,{})}});const zi=sn({squeeze_:function(t,e){const n=en(t,"x","squeeze");return Nn(n,function(t,e){const n=[],s=[],i=null!=e&&Array.isArray(e)&&0===e.length,r=null==e||i?null:Yt(e,t).sort();let a=0;for(let e=0;e<t.length;++e){if(null!=r){if(r[a]===e&&1!==t[e])throw new Error(`Can't squeeze axis ${e} since its dim '${t[e]}' is not 1`);(null==r[a]||r[a]>e)&&1===t[e]&&(n.push(t[e]),s.push(e)),r[a]<=e&&a++}1!==t[e]&&(n.push(t[e]),s.push(e))}return{newShape:n,keptDims:s}}(n.shape,e).newShape)}});const Di=sn({stack_:function(t,e=0){const n=nn(t,"tensors","stack","string_or_numeric");qt(n.length>=1,(()=>"Pass at least one tensor to tf.stack")),n.length>0&&qt(e<=n[0].rank,(()=>"Axis must be <= rank of the tensor"));const s=n,i={axis:e};return Ze.runKernel("Pack",s,i)}});const $i=sn({step_:function(t,e=0){const n={x:en(t,"x","step")},s={alpha:e};return Ze.runKernel("Step",n,s)}});const Ti=sn({stridedSlice_:function(t,e,n,s,i=0,r=0,a=0,o=0,l=0){const u={x:en(t,"x","stridedSlice")},h={begin:e,end:n,strides:s,beginMask:i,endMask:r,ellipsisMask:a,newAxisMask:o,shrinkAxisMask:l};return Ze.runKernel("StridedSlice",u,h)}});const Ei=sn({tan_:function(t){const e={x:en(t,"x","tan")};return Ze.runKernel("Tan",e)}});const Fi=sn({topk_:function(t,e=1,n=!0){const s=en(t,"x","topk");if(0===s.rank)throw new Error("topk() expects the input to be of rank 1 or higher");const i=s.shape[s.shape.length-1];if(e>i)throw new Error(`'k' passed to topk() must be <= the last dimension (${i}) but got ${e}`);const r={x:s},a={k:e,sorted:n},[o,l]=Ze.runKernel("TopK",r,a);return{values:o,indices:l}}});const Li=sn({unique_:function(t,e=0){const n=en(t,"x","unique","string_or_numeric");qt(n.rank>0,(()=>"The input tensor must be at least 1D"));const s={x:n},i={axis:e},[r,a]=Ze.runKernel("Unique",s,i);return{values:r,indices:a}}});const _i=sn({unsortedSegmentSum_:function(t,e,n){const s=en(t,"x","unsortedSegmentSum"),i=en(e,"segmentIds","unsortedSegmentSum","int32");qt(Zt(n),(()=>"numSegments must be of dtype int"));const r={x:s,segmentIds:i},a={numSegments:n};return Ze.runKernel("UnsortedSegmentSum",r,a)}});const Ri=sn({unstack_:function(t,e=0){const n=en(t,"x","unstack","string_or_numeric");qt(e>=-n.shape.length&&e<n.shape.length,(()=>`Axis = ${e} is not in [-${n.shape.length}, ${n.shape.length})`));const s={value:n},i={axis:e};return Ze.runKernel("Unpack",s,i)}});const Mi=sn({transpose_:function(t,e){const n=en(t,"x","transpose");if(null==e&&(e=n.shape.map(((t,e)=>e)).reverse()),qt(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of perm ${e}.`)),e.forEach((t=>{qt(t>=0&&t<n.rank,(()=>"All entries in 'perm' must be between 0 and "+(n.rank-1)+` but got ${e}`))})),n.rank<=1)return n.clone();const s={x:n},i={perm:e};return Ze.runKernel("Transpose",s,i)}});function Oi(t,e,n=null){if(0===t.rank)return rn(t);if(1!==t.rank&&null===n)return Oi(Nn(t,[-1]),e,n);if(1===t.rank||"number"==typeof n||Array.isArray(n)&&1===n.length){if(1===e)return Es(rn(t),n);if(e===1/0)return $s(rn(t),n);if(e===-1/0)return qs(rn(t),n);if("euclidean"===e||2===e)return Ai(Es(si(rn(t),fi(2,"int32")),n));throw new Error(`Error in norm: invalid ord value: ${e}`)}if(Array.isArray(n)&&2===n.length){if(1===e)return $s(Es(rn(t),n[0]),n[1]-1);if(e===1/0)return $s(Es(rn(t),n[1]),n[0]);if(e===-1/0)return qs(Es(rn(t),n[1]),n[0]);if("fro"===e||"euclidean"===e)return Ai(Es(Zs(t),n));throw new Error(`Error in norm: invalid ord value: ${e}`)}throw new Error(`Error in norm: invalid axis: ${n}`)}const Bi=sn({norm_:function(t,e="euclidean",n=null,s=!1){const i=Oi(t=en(t,"x","norm"),e,n);let r=i.shape;if(s){const e=Yt(n,t.shape);r=Ls(i.shape,e)}return Nn(i,r)}});const Pi=sn({conv2DBackpropFilter_:function(t,e,n,s,i,r="NHWC",a){let o=t;3===t.rank&&(o=Nn(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=Nn(e,[1,e.shape[0],e.shape[1],e.shape[2]])),qt(4===o.rank,(()=>`Error in conv2dDerFilter: input must be rank 4, but got shape ${o.shape}.`)),qt(4===l.rank,(()=>`Error in conv2dDerFilter: dy must be rank 4, but got shape ${l.shape}.`)),qt(4===n.length,(()=>`Error in conv2dDerFilter: filterShape must be length 4, but got ${n}.`));const u="NHWC"===r?o.shape[3]:o.shape[1],h="NHWC"===r?l.shape[3]:l.shape[1];qt(u===n[2],(()=>`Error in conv2dDerFilter: depth of input ${u}) must match input depth in filter (${n[2]}.`)),qt(h===n[3],(()=>`Error in conv2dDerFilter: depth of dy (${h}) must match output depth for filter (${n[3]}).`)),null!=a&&qt(Zt(i),(()=>`Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const c={x:o,dy:l},p={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,filterShape:n};return Ze.runKernel("Conv2DBackpropFilter",c,p)}});const Wi=sn({depthwiseConv2dNativeBackpropFilter_:function(t,e,n,s,i,r=[1,1],a){let o=t;3===t.rank&&(o=Nn(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=Nn(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={x:o,dy:l},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,filterShape:n};return Ze.runKernel("DepthwiseConv2dNativeBackpropFilter",u,h)}});const Ui=sn({depthwiseConv2dNativeBackpropInput_:function(t,e,n,s,i,r=[1,1],a){let o=e,l=!1;3===e.rank&&(l=!0,o=Nn(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={dy:o,filter:n},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,inputShape:t},c=Ze.runKernel("DepthwiseConv2dNativeBackpropInput",u,h);return l?Nn(c,[c.shape[1],c.shape[2],c.shape[3]]):c}});const Ki=sn({resizeBilinear_:function(t,e,n=!1,s=!1){const i=en(t,"images","resizeBilinear");qt(3===i.rank||4===i.rank,(()=>`Error in resizeBilinear: x must be rank 3 or 4, but got rank ${i.rank}.`)),qt(2===e.length,(()=>`Error in resizeBilinear: new shape must 2D, but got shape ${e}.`)),qt(!1===s||!1===n,(()=>"Error in resizeBilinear: If halfPixelCenters is true, alignCorners must be false."));let r=i,a=!1;3===i.rank&&(a=!0,r=Nn(i,[1,i.shape[0],i.shape[1],i.shape[2]]));const o={images:r},l={alignCorners:n,halfPixelCenters:s,size:e},u=Ze.runKernel("ResizeBilinear",o,l);return a?Nn(u,[u.shape[1],u.shape[2],u.shape[3]]):u}});const ji=sn({resizeNearestNeighbor_:function(t,e,n=!1,s=!1){const i=en(t,"images","resizeNearestNeighbor");qt(3===i.rank||4===i.rank,(()=>`Error in resizeNearestNeighbor: x must be rank 3 or 4, but got rank ${i.rank}.`)),qt(2===e.length,(()=>`Error in resizeNearestNeighbor: new shape must 2D, but got shape ${e}.`)),qt("float32"===i.dtype||"int32"===i.dtype,(()=>"`images` must have `int32` or `float32` as dtype")),qt(!1===s||!1===n,(()=>"Error in resizeNearestNeighbor: If halfPixelCenters is true, alignCorners must be false."));let r=i,a=!1;3===i.rank&&(a=!0,r=Nn(i,[1,i.shape[0],i.shape[1],i.shape[2]]));const o={images:r},l={alignCorners:n,halfPixelCenters:s,size:e},u=Ze.runKernel("ResizeNearestNeighbor",o,l);return a?Nn(u,[u.shape[1],u.shape[2],u.shape[3]]):u}});Re().prototype.abs=function(){return this.throwIfDisposed(),rn(this)},Re().prototype.acos=function(){return this.throwIfDisposed(),an(this)},Re().prototype.acosh=function(){return this.throwIfDisposed(),on(this)},Re().prototype.add=function(t){return this.throwIfDisposed(),ln(this,t)},Re().prototype.all=function(t,e){return this.throwIfDisposed(),un(this,t,e)},Re().prototype.any=function(t,e){return this.throwIfDisposed(),hn(this,t,e)},Re().prototype.argMax=function(t){return this.throwIfDisposed(),cn(this,t)},Re().prototype.argMin=function(t){return this.throwIfDisposed(),pn(this,t)},Re().prototype.asScalar=function(){return this.throwIfDisposed(),qt(1===this.size,(()=>"The array must have only 1 element.")),Nn(this,[])},Re().prototype.asType=function(t){return this.throwIfDisposed(),bn(this,t)},Re().prototype.as1D=function(){return this.throwIfDisposed(),Nn(this,[this.size])},Re().prototype.as2D=function(t,e){return this.throwIfDisposed(),Nn(this,[t,e])},Re().prototype.as3D=function(t,e,n){return this.throwIfDisposed(),Nn(this,[t,e,n])},Re().prototype.as4D=function(t,e,n,s){return this.throwIfDisposed(),Nn(this,[t,e,n,s])},Re().prototype.as5D=function(t,e,n,s,i){return this.throwIfDisposed(),Nn(this,[t,e,n,s,i])},Re().prototype.asin=function(){return this.throwIfDisposed(),dn(this)},Re().prototype.asinh=function(){return this.throwIfDisposed(),fn(this)},Re().prototype.atan=function(){return this.throwIfDisposed(),gn(this)},Re().prototype.atan2=function(t){return this.throwIfDisposed(),mn(this,t)},Re().prototype.atanh=function(){return this.throwIfDisposed(),yn(this)},Re().prototype.avgPool=function(t,e,n,s){return this.throwIfDisposed(),An(this,t,e,n,s)},Re().prototype.batchToSpaceND=function(t,e){return this.throwIfDisposed(),Ln(this,t,e)},Re().prototype.batchNorm=function(t,e,n,s,i){return this.throwIfDisposed(),_n(this,t,e,n,s,i)},Re().prototype.broadcastTo=function(t){return this.throwIfDisposed(),Rn(this,t)},Re().prototype.cast=function(t){return this.throwIfDisposed(),bn(this,t)},Re().prototype.ceil=function(){return this.throwIfDisposed(),Mn(this)},Re().prototype.clipByValue=function(t,e){return this.throwIfDisposed(),On(this,t,e)},Re().prototype.concat=function(t,e){return this.throwIfDisposed(),t instanceof _e&&(t=[t]),zn([this,...t],e)},Re().prototype.conv1d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Wn(this,t,e,n,s,i,r)},Re().prototype.conv2dTranspose=function(t,e,n,s,i){return this.throwIfDisposed(),Kn(this,t,e,n,s,i)},Re().prototype.conv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Pn(this,t,e,n,s,i,r)},Re().prototype.cos=function(){return this.throwIfDisposed(),Vn(this)},Re().prototype.cosh=function(){return this.throwIfDisposed(),qn(this)},Re().prototype.cumsum=function(t,e,n){return this.throwIfDisposed(),Gn(this,t,e,n)},Re().prototype.depthToSpace=function(t,e){return this.throwIfDisposed(),Hn(this,t,e)},Re().prototype.depthwiseConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Jn(this,t,e,n,s,i,r)},Re().prototype.dilation2d=function(t,e,n,s,i){return this.throwIfDisposed(),Zn(this,t,e,n,s,i)},Re().prototype.divNoNan=function(t){return this.throwIfDisposed(),is(this,t)},Re().prototype.div=function(t){return this.throwIfDisposed(),Yn(this,t)},Re().prototype.dot=function(t){return this.throwIfDisposed(),rs(this,t)},Re().prototype.elu=function(){return this.throwIfDisposed(),as(this)},Re().prototype.equal=function(t){return this.throwIfDisposed(),es(this,t)},Re().prototype.erf=function(){return this.throwIfDisposed(),os(this)},Re().prototype.exp=function(){return this.throwIfDisposed(),ls(this)},Re().prototype.expandDims=function(t){return this.throwIfDisposed(),us(this,t)},Re().prototype.expm1=function(){return this.throwIfDisposed(),hs(this)},Re().prototype.fft=function(){return this.throwIfDisposed(),xi(this)},Re().prototype.flatten=function(){return this.throwIfDisposed(),Nn(this,[this.size])},Re().prototype.floor=function(){return this.throwIfDisposed(),ps(this)},Re().prototype.floorDiv=function(t){return this.throwIfDisposed(),Xn(this,t)},Re().prototype.gather=function(t,e){return this.throwIfDisposed(),ds(this,t,e)},Re().prototype.greaterEqual=function(t){return this.throwIfDisposed(),gs(this,t)},Re().prototype.greater=function(t){return this.throwIfDisposed(),fs(this,t)},Re().prototype.ifft=function(){return this.throwIfDisposed(),vi(this)},Re().prototype.irfft=function(){return this.throwIfDisposed(),Si(this)},Re().prototype.isFinite=function(){return this.throwIfDisposed(),ys(this)},Re().prototype.isInf=function(){return this.throwIfDisposed(),bs(this)},Re().prototype.isNaN=function(){return this.throwIfDisposed(),ws(this)},Re().prototype.leakyRelu=function(t){return this.throwIfDisposed(),ks(this,t)},Re().prototype.lessEqual=function(t){return this.throwIfDisposed(),vs(this,t)},Re().prototype.less=function(t){return this.throwIfDisposed(),xs(this,t)},Re().prototype.localResponseNormalization=function(t,e,n,s){return this.throwIfDisposed(),Ss(this,t,e,n,s)},Re().prototype.logSigmoid=function(){return this.throwIfDisposed(),Ds(this)},Re().prototype.logSoftmax=function(t){return this.throwIfDisposed(),Fs(this,t)},Re().prototype.logSumExp=function(t,e){return this.throwIfDisposed(),Rs(this,t,e)},Re().prototype.log=function(){return this.throwIfDisposed(),Is(this)},Re().prototype.log1p=function(){return this.throwIfDisposed(),Ns(this)},Re().prototype.logicalAnd=function(t){return this.throwIfDisposed(),Ms(this,t)},Re().prototype.logicalNot=function(){return this.throwIfDisposed(),Os(this)},Re().prototype.logicalOr=function(t){return this.throwIfDisposed(),Bs(this,t)},Re().prototype.logicalXor=function(t){return this.throwIfDisposed(),Ps(this,t)},Re().prototype.matMul=function(t,e,n){return this.throwIfDisposed(),Dn(this,t,e,n)},Re().prototype.maxPool=function(t,e,n,s){return this.throwIfDisposed(),Ws(this,t,e,n,s)},Re().prototype.max=function(t,e){return this.throwIfDisposed(),$s(this,t,e)},Re().prototype.maximum=function(t){return this.throwIfDisposed(),Us(this,t)},Re().prototype.mean=function(t,e){return this.throwIfDisposed(),Ks(this,t,e)},Re().prototype.min=function(t,e){return this.throwIfDisposed(),qs(this,t,e)},Re().prototype.minimum=function(t){return this.throwIfDisposed(),Gs(this,t)},Re().prototype.mirrorPad=function(t,e){return this.throwIfDisposed(),Hs(this,t,e)},Re().prototype.mod=function(t){return this.throwIfDisposed(),Js(this,t)},Re().prototype.mul=function(t){return this.throwIfDisposed(),$n(this,t)},Re().prototype.neg=function(){return this.throwIfDisposed(),Cs(this)},Re().prototype.norm=function(t,e,n){return this.throwIfDisposed(),Bi(this,t,e,n)},Re().prototype.notEqual=function(t){return this.throwIfDisposed(),Xs(this,t)},Re().prototype.oneHot=function(t,e=1,n=0){return this.throwIfDisposed(),Ys(this,t,e,n)},Re().prototype.onesLike=function(){return this.throwIfDisposed(),Qs(this)},Re().prototype.pad=function(t,e){return this.throwIfDisposed(),ti(this,t,e)},Re().prototype.pool=function(t,e,n,s,i){return this.throwIfDisposed(),ni(this,t,e,n,s,i)},Re().prototype.pow=function(t){return this.throwIfDisposed(),si(this,t)},Re().prototype.prelu=function(t){return this.throwIfDisposed(),ii(this,t)},Re().prototype.prod=function(t,e){return this.throwIfDisposed(),ri(this,t,e)},Re().prototype.reciprocal=function(){return this.throwIfDisposed(),oi(this)},Re().prototype.relu=function(){return this.throwIfDisposed(),li(this)},Re().prototype.relu6=function(){return this.throwIfDisposed(),ui(this)},Re().prototype.reshapeAs=function(t){return this.throwIfDisposed(),Nn(this,t.shape)},Re().prototype.reshape=function(t){return this.throwIfDisposed(),Nn(this,t)},Re().prototype.resizeBilinear=function(t,e,n){return this.throwIfDisposed(),Ki(this,t,e,n)},Re().prototype.resizeNearestNeighbor=function(t,e,n){return this.throwIfDisposed(),ji(this,t,e,n)},Re().prototype.reverse=function(t){return this.throwIfDisposed(),hi(this,t)},Re().prototype.rfft=function(){return this.throwIfDisposed(),Ni(this)},Re().prototype.round=function(){return this.throwIfDisposed(),ci(this)},Re().prototype.rsqrt=function(){return this.throwIfDisposed(),pi(this)},Re().prototype.selu=function(){return this.throwIfDisposed(),gi(this)},Re().prototype.separableConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),mi(this,t,e,n,s,i,r)},Re().prototype.sigmoid=function(){return this.throwIfDisposed(),Tn(this)},Re().prototype.sign=function(){return this.throwIfDisposed(),yi(this)},Re().prototype.sin=function(){return this.throwIfDisposed(),bi(this)},Re().prototype.sinh=function(){return this.throwIfDisposed(),wi(this)},Re().prototype.slice=function(t,e){return this.throwIfDisposed(),En(this,t,e)},Re().prototype.softmax=function(t){return this.throwIfDisposed(),ki(this,t)},Re().prototype.softplus=function(){return this.throwIfDisposed(),zs(this)},Re().prototype.spaceToBatchND=function(t,e){return this.throwIfDisposed(),ei(this,t,e)},Re().prototype.split=function(t,e){return this.throwIfDisposed(),Ii(this,t,e)},Re().prototype.sqrt=function(){return this.throwIfDisposed(),Ai(this)},Re().prototype.square=function(){return this.throwIfDisposed(),Zs(this)},Re().prototype.squaredDifference=function(t){return this.throwIfDisposed(),Ci(this,t)},Re().prototype.squeeze=function(t){return this.throwIfDisposed(),zi(this,t)},Re().prototype.stack=function(t,e){this.throwIfDisposed();const n=t instanceof _e?[this,t]:[this,...t];return Di(n,e)},Re().prototype.step=function(t){return this.throwIfDisposed(),$i(this,t)},Re().prototype.stridedSlice=function(t,e,n,s,i,r,a,o){return this.throwIfDisposed(),Ti(this,t,e,n,s,i,r,a,o)},Re().prototype.sub=function(t){return this.throwIfDisposed(),Ts(this,t)},Re().prototype.sum=function(t,e){return this.throwIfDisposed(),Es(this,t,e)},Re().prototype.tan=function(){return this.throwIfDisposed(),Ei(this)},Re().prototype.tanh=function(){return this.throwIfDisposed(),Fn(this)},Re().prototype.tile=function(t){return this.throwIfDisposed(),cs(this,t)},Re().prototype.toBool=function(){return this.throwIfDisposed(),bn(this,"bool")},Re().prototype.toFloat=function(){return this.throwIfDisposed(),bn(this,"float32")},Re().prototype.toInt=function(){return this.throwIfDisposed(),bn(this,"int32")},Re().prototype.topk=function(t,e){return this.throwIfDisposed(),Fi(this,t,e)},Re().prototype.transpose=function(t){return this.throwIfDisposed(),Mi(this,t)},Re().prototype.unique=function(t){return this.throwIfDisposed(),Li(this,t)},Re().prototype.unsortedSegmentSum=function(t,e){return this.throwIfDisposed(),_i(this,t,e)},Re().prototype.unstack=function(t){return this.throwIfDisposed(),Ri(this,t)},Re().prototype.where=function(t,e){return this.throwIfDisposed(),ns(t,this,e)},Re().prototype.zerosLike=function(){return this.throwIfDisposed(),ss(this)};const Vi={kernelName:"Abs",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(t,$i(bn(n,"float32"),-1))}}},qi={kernelName:"Acos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Zs(bn(n,"float32")),s=Ai(Ts(fi(1),e));return Cs(Yn(t,s))}}}},Gi={kernelName:"Acosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Ai(Ts(Zs(bn(n,"float32")),1));return Yn(t,e)}}}},Hi={kernelName:"Add",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=ts(n.shape,s.shape);return{a:()=>{let e=t;const s=Qn(n.shape,i);return s.length>0&&(e=Es(e,s)),Nn(e,n.shape)},b:()=>{let e=t;const n=Qn(s.shape,i);return n.length>0&&(e=Es(e,n)),Nn(e,s.shape)}}}},Ji={kernelName:"AddN",saveAllInputs:!0,gradFunc:(t,e)=>{const n={};return e.forEach(((e,s)=>{n[s]=()=>t.clone()})),n}},Zi={kernelName:"ArgMax",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ss(n)}}},Xi={kernelName:"ArgMin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ss(n)}}},Yi={kernelName:"Asin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,Ai(Ts(fi(1),Zs(bn(n,"float32")))))}}},Qi={kernelName:"Asinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Ai(ln(fi(1),Zs(bn(n,"float32"))));return Yn(t,e)}}}},tr={kernelName:"Atan2",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=ts(n.shape,s.shape);return{a:()=>{const e=ln(Zs(n),Zs(s));let r=$n(t,Yn(s,e));const a=Qn(n.shape,i);return a.length>0&&(r=Es(r,a)),Nn(r,n.shape)},b:()=>{const e=ln(Zs(n),Zs(s));let r=Cs($n(t,Yn(n,e)));const a=Qn(s.shape,i);return a.length>0&&(r=Es(r,a)),Nn(r,s.shape)}}}},er={kernelName:"Atan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,ln(Zs(bn(n,"float32")),1))}}},nr={kernelName:"Atanh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,Ts(fi(1),Zs(bn(n,"float32"))))}}};const sr=sn({avgPool3dGrad_:function(t,e,n,s,i,r){const a=en(t,"dy","avgPool3dGrad"),o=en(e,"input","avgPool3dGrad");let l=a,u=o,h=!1;4===o.rank&&(h=!0,l=Nn(a,[1,a.shape[0],a.shape[1],a.shape[2],a.shape[3]]),u=Nn(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]])),qt(5===l.rank,(()=>`Error in avgPool3dGrad: dy must be rank 5 but got rank ${l.rank}.`)),qt(5===u.rank,(()=>`Error in avgPool3dGrad: input must be rank 5 but got rank ${u.rank}.`)),null!=r&&qt(Zt(i),(()=>`Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`));const c={dy:l,input:u},p={filterSize:n,strides:s,pad:i,dimRoundingMode:r},d=Ze.runKernel("AvgPool3DGrad",c,p);return h?Nn(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}}),ir={kernelName:"AvgPool3D",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a,dimRoundingMode:o}=n;return{x:()=>sr(t,s,i,r,a,o)}}};const rr=sn({avgPoolGrad_:function(t,e,n,s,i){const r=en(t,"dy","avgPoolGrad"),a=en(e,"input","avgPoolGrad");qt(a.rank===r.rank,(()=>`Rank of input (${a.rank}) does not match rank of dy (${r.rank})`));let o=a,l=r,u=!1;3===a.rank&&(u=!0,o=Nn(a,[1,a.shape[0],a.shape[1],a.shape[2]]),l=Nn(r,[1,r.shape[0],r.shape[1],r.shape[2]])),qt(4===l.rank,(()=>`Error in avgPoolGrad: dy must be rank 4 but got rank ${l.rank}.`)),qt(4===o.rank,(()=>`Error in avgPoolGrad: input must be rank 4 but got rank ${o.rank}.`));const h={dy:l,input:o},c={filterSize:n,strides:s,pad:i},p=Ze.runKernel("AvgPoolGrad",h,c);return u?Nn(p,[p.shape[1],p.shape[2],p.shape[3]]):p}}),ar={kernelName:"AvgPool",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a}=n;return{x:()=>rr(t,s,i,r,a)}}},or={kernelName:"BatchMatMul",inputsToSave:["a","b"],gradFunc:(t,e,n)=>{const[s,i]=e,{transposeA:r,transposeB:a}=n;return r||a?!r&&a?{a:()=>Dn(t,i,!1,!1),b:()=>Dn(t,s,!0,!1)}:r&&!a?{a:()=>Dn(i,t,!1,!0),b:()=>Dn(s,t,!1,!1)}:{a:()=>Dn(i,t,!0,!0),b:()=>Dn(t,s,!0,!0)}:{a:()=>Dn(t,i,!1,!0),b:()=>Dn(s,t,!0,!1)}}},lr={kernelName:"BatchToSpaceND",gradFunc:(t,e,n)=>{const{blockShape:s,crops:i}=n;return{x:()=>ei(t,s,i)}}},ur={kernelName:"BroadcastTo",gradFunc:(t,e,n)=>{const s=n,i=s.inputShape,r=s.shape,a=Array.from(r);for(let t=i.length-1;t>=0;t--)if(i[t]===r[t])a[t]=1;else if(1!==i[t])throw new Error(`broadcastTo(): [${i}] cannot be broadcast to [${r}].`);const o=[];for(let t=0;t<a.length;t++)a[t]>1&&o.push(t);return{x:()=>Es(t,o,!0)}}},hr={kernelName:"Cast",gradFunc:t=>({x:()=>t.clone()})},cr={kernelName:"Ceil",gradFunc:t=>({x:()=>ss(t)})},pr={kernelName:"ClipByValue",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{clipValueMin:i,clipValueMax:r}=n;return{x:()=>ns(Ms(gs(s,i),vs(s,r)),t,ss(t))}}},dr={kernelName:"ComplexAbs",inputsToSave:["x"],gradFunc:Vi.gradFunc},fr={kernelName:"Concat",saveAllInputs:!0,gradFunc:(t,e,n)=>{const s=e.map((t=>t.shape)),{axis:i}=n,r=Yt(i,e[0].shape)[0],a=s.map((t=>t[r]));return Ii(t,a,r).map((t=>()=>t))}},gr={kernelName:"Conv2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{dilations:r,strides:a,pad:o,dataFormat:l}=n;return qt(Sn(r),(()=>`Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${r}'`)),{x:()=>Un(s.shape,t,i,a,o,l),filter:()=>Pi(s,t,i.shape,a,o,l)}}},mr={kernelName:"Conv2DBackpropInput",inputsToSave:["dy","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{strides:r,pad:a,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>Pn(t,i,r,a,o,1,l),filter:()=>Pi(t,s,i.shape,r,a,o,l)}}};const yr=sn({conv3DBackpropFilter_:function(t,e,n,s,i){let r=t;4===t.rank&&(r=Nn(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]));let a=e;4===a.rank&&(a=Nn(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]])),qt(5===r.rank,(()=>`Error in conv3dDerFilter: input must be rank 5, but got shape ${r.shape}.`)),qt(5===a.rank,(()=>`Error in conv3dDerFilter: dy must be rank 5, but got shape ${a.shape}.`)),qt(5===n.length,(()=>`Error in conv3dDerFilter: filterShape must be length 5, but got ${n}.`)),qt(r.shape[4]===n[3],(()=>`Error in conv3dDerFilter: depth of input ${r.shape[4]}) must match input depth in filter (${n[3]}.`)),qt(a.shape[4]===n[4],(()=>`Error in conv3dDerFilter: depth of dy (${a.shape[4]}) must match output depth for filter (${n[4]}).`));const o={x:r,dy:a},l={strides:s,pad:i,filterShape:n};return Ze.runKernel("Conv3DBackpropFilterV2",o,l)}}),br={kernelName:"Conv3D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r}=n;qt(Sn(s),(()=>`Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${s}'`));const[a,o]=e;return{x:()=>jn(a.shape,t,o,i,r),filter:()=>yr(a,t,o.shape,i,r)}}},wr={kernelName:"Cos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(Cs(bi(bn(n,"float32"))),t)}}},kr={kernelName:"Cosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(wi(bn(n,"float32")),t)}}},xr={kernelName:"Cumsum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i,exclusive:r,reverse:a}=n;return{x:()=>{const e=function(t,e){if(function(t,e){for(let n=0;n<t.length;++n)if(t[t.length-n-1]!==e-1-n)return!1;return!0}(t,e))return null;const n=[];for(let s=0;s<e;++s)-1===t.indexOf(s)&&n.push(s);return t.forEach((t=>n.push(t))),n}([i],s.rank);let n=Gn(t,i,r,!a);return null!=e&&(n=Mi(n,e)),n}}}},vr={kernelName:"DepthwiseConv2dNative",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r,dimRoundingMode:a}=n,o=null==s?[1,1]:s;qt(Sn(o),(()=>`Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${o}'`));const[l,u]=e;return qt(4===l.rank,(()=>`Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${l.rank}.`)),qt(4===u.rank,(()=>`Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${u.rank}.`)),qt(l.shape[3]===u.shape[2],(()=>`Error in gradient of depthwiseConv2d: number of input channels (${l.shape[3]}) must match the inChannels dimension in filter ${u.shape[2]}.`)),qt(In(i,o),(()=>`Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${i} and dilations '${o}'.`)),null!=a&&qt(Zt(r),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`)),{x:()=>Ui(l.shape,t,u,i,r,s,a),filter:()=>Wi(l,t,u.shape,i,r,s,a)}}},Sr={kernelName:"Dilation2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,r={x:s,filter:i,dy:t},a={x:s,filter:i,dy:t};return{x:()=>Ze.runKernel("Dilation2DBackpropInput",r,n),filter:()=>Ze.runKernel("Dilation2DBackpropFilter",a,n)}}},Ir={kernelName:"Elu",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e,s={dy:t,y:n};return{x:()=>Ze.runKernel("EluGrad",s)}}},Nr={kernelName:"Erf",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=$n(ls(Cs(Zs(n))),2/Math.sqrt(Math.PI));return{x:()=>$n(t,s)}}},Ar={kernelName:"Exp",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(t,n)}}},Cr={kernelName:"ExpandDims",inputsToSave:["input"],gradFunc:(t,e)=>{const[n]=e;return{input:()=>Nn(t,n.shape)}}},zr={kernelName:"Expm1",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(t,ls(n))}}},Dr={kernelName:"Floor",gradFunc:t=>({x:()=>ss(t)})},$r={kernelName:"FloorDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=ts(n.shape,s.shape);return{a:()=>{const e=Yn(t,bn(s,"float32")),r=Qn(n.shape,i);return r.length>0?Nn(Es(e,r),n.shape):e},b:()=>{let e=$n(t,bn(n,"float32"));const r=Qn(s.shape,i);r.length>0&&(e=Nn(Es(e,r),s.shape));const a=Zs(s);return Cs(Yn(e,bn(a,"float32")))}}}},Tr={kernelName:"FusedBatchNorm",inputsToSave:["x","mean","variance","scale"],gradFunc:(t,e,n)=>{const{varianceEpsilon:s}=n,[i,r,a,o]=e,l=null==o?fi(1):o,u=Qn(r.shape,i.shape),h=[];if(1===r.rank){for(let t=0;t<i.shape.length-1;++t)h.push(i.shape[t]);h.push(1)}const c=Ts(i,r),p=$n(t,l),d=pi(ln(a,fi(s))),f=$n($n($n(d,d),d),fi(-.5));return{x:()=>1===r.rank?Nn($n($n(t,cs(Nn(d,[1,1,1,r.shape[0]]),h)),l),i.shape):Nn($n($n(t,d),l),i.shape),mean:()=>{let t=$n($n(d,fi(-1)),p);return 1===r.rank&&(t=Es(t,u)),Nn(t,r.shape)},variance:()=>{let t=$n($n(f,c),p);return 1===r.rank&&(t=Es(t,u)),Nn(t,r.shape)},scale:()=>{const e=$n(c,d);let n=$n(t,e);return 1===r.rank&&(n=Es(n,u)),Nn(n,r.shape)},offset:()=>{let e=t;return 1===r.rank&&(e=Es(e,u)),Nn(e,r.shape)}}}},Er={kernelName:"GatherV2",inputsToSave:["x","indices"],gradFunc:(t,e,n)=>{const[s,i]=e,{axis:r}=n,a=Yt(r,s.shape)[0];return{x:()=>{const e=s.shape,n=i.size,o=e.slice(0,a),l=o.length,u=e.slice(r,e.length).slice(1),h=u.length,c=Fr(0,l),p=Fr(l+1,l+1+h),d=Lr([o,[n],u]),f=Nn(t,d),g=Nn(i,[n]),m=Lr([[l],c,p]),y=Mi(f,m);let b=_i(y,g,s.shape[a]);const w=_s(m);return b=Mi(b,w),b},indices:()=>i}}};function Fr(t,e){const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Lr(t){const e=[];for(let n=0;n<t.length;++n)for(let s=0;s<t[n].length;++s)e.push(t[n][s]);return e}const _r={kernelName:"GreaterEqual",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>ss(n),b:()=>ss(s)}}},Rr={kernelName:"Identity",gradFunc:t=>({x:()=>bn(t,"float32")})},Mr={kernelName:"IsFinite",gradFunc:t=>({x:()=>ss(t)})},Or={kernelName:"IsInf",gradFunc:t=>({x:()=>ss(t)})},Br={kernelName:"IsNan",gradFunc:t=>({x:()=>ss(t)})},Pr={kernelName:"LeakyRelu",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{alpha:i}=n,r=fs(s,0);return{x:()=>ns(r,t,$n(t,i))}}},Wr={kernelName:"Log1p",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,ln(n,1))}}},Ur={kernelName:"Log",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,bn(n,"float32"))}}},Kr={kernelName:"LogSoftmax",inputsToSave:[],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;return{logits:()=>{const e=ls(s);return Ts(t,$n(Es(t,i,!0),e))}}}};const jr=sn({localResponseNormalizationBackprop_:function(t,e,n,s=5,i=1,r=1,a=.5){const o={x:t,y:e,dy:n},l={depthRadius:s,bias:i,alpha:r,beta:a};return Ze.runKernel("LRNGrad",o,l)}}),Vr={kernelName:"LRN",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{depthRadius:r,bias:a,alpha:o,beta:l}=n;return{x:()=>jr(s,i,t,r,a,o,l)}}};function qr(t,e,n,s){return e.rank<n.rank&&(e=Nn(e,Ls(e.shape,s))),t.rank<n.rank&&(t=Nn(t,Ls(t.shape,s))),{x:()=>$n(t,bn(es(n,e),t.dtype))}}const Gr={kernelName:"Max",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{reductionIndices:i}=s,r=e[0],a=qr(t,e[1],r,Yt(i,r.shape));return{x:()=>a.x()}}},Hr={kernelName:"Maximum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>$n(t,bn(gs(n,s),"float32")),b:()=>$n(t,bn(xs(n,s),"float32"))}}};const Jr=sn({maxPool3dGrad_:function(t,e,n,s,i,r,a){const o=en(t,"dy","maxPool3dGrad"),l=en(e,"input","maxPool3dGrad"),u=en(n,"output","maxPool3dGrad");let h=o,c=l,p=u,d=!1;4===l.rank&&(d=!0,h=Nn(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]]),c=Nn(l,[1,l.shape[0],l.shape[1],l.shape[2],l.shape[3]]),p=Nn(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]])),qt(5===h.rank,(()=>`Error in maxPool3dGrad: dy must be rank 5 but got rank ${h.rank}.`)),qt(5===c.rank,(()=>`Error in maxPool3dGrad: input must be rank 5 but got rank ${c.rank}.`)),qt(5===p.rank,(()=>`Error in maxPool3dGrad: output must be rank 5 but got rank ${p.rank}.`)),null!=a&&qt(Zt(r),(()=>`Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const f={dy:h,input:c,output:p},g={filterSize:s,strides:i,pad:r,dimRoundingMode:a},m=Ze.runKernel("MaxPool3DGrad",f,g);return d?Nn(m,[m.shape[1],m.shape[2],m.shape[3],m.shape[4]]):m}}),Zr={kernelName:"MaxPool3D",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o,dimRoundingMode:l}=n;return{x:()=>Jr(t,s,i,r,a,o,l)}}};const Xr=sn({maxPoolGrad_:function(t,e,n,s,i,r,a){const o=en(t,"dy","maxPoolGrad"),l=en(e,"input","maxPoolGrad"),u=en(n,"output","maxPoolGrad");qt(l.rank===o.rank,(()=>`Rank of input (${l.rank}) does not match rank of dy (${o.rank})`)),qt(4===o.rank,(()=>`Error in maxPoolGrad: dy must be rank 4 but got rank ${o.rank}.`)),qt(4===l.rank,(()=>`Error in maxPoolGrad: input must be rank 4 but got rank ${l.rank}.`)),null!=a&&qt(Zt(r),(()=>`Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const h={dy:o,input:l,output:u},c={filterSize:s,strides:i,pad:r,dimRoundingMode:a};return Ze.runKernel("MaxPoolGrad",h,c)}}),Yr={kernelName:"PadV2",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>En(t,r,s.shape)}}};const Qr={kernelName:"SpaceToBatchND",gradFunc:(t,e,n)=>{const{blockShape:s,paddings:i}=n;return{x:()=>Ln(t,s,i)}}},ta={kernelName:"SplitV",gradFunc:(t,e,n)=>{const{axis:s}=n;return{x:()=>zn(t,s)}}};const ea=[Vi,qi,Gi,Hi,Ji,Zi,Xi,Yi,Qi,tr,er,nr,ir,ar,or,lr,ur,hr,cr,pr,dr,fr,mr,gr,br,wr,kr,xr,vr,Sr,{kernelName:"RealDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=ts(n.shape,s.shape);return{a:()=>{const e=Yn(t,bn(s,"float32")),r=Qn(n.shape,i);return r.length>0?Nn(Es(e,r),n.shape):e},b:()=>{let e=$n(t,bn(n,"float32"));const r=Qn(s.shape,i);r.length>0&&(e=Nn(Es(e,r),s.shape));const a=Zs(s);return Cs(Yn(e,bn(a,"float32")))}}}},Ir,Nr,Ar,Cr,zr,$r,Dr,Tr,Er,_r,Rr,Mr,Or,Br,Pr,Wr,Ur,Kr,Vr,Gr,Gr,Hr,Zr,{kernelName:"MaxPool",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o}=n;return{x:()=>Xr(t,s,i,r,a,o)}}},{kernelName:"Mean",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n,r=Yt(i,s.shape),a=Ht(function(t,e){const n=[],s=t.length;for(let i=0;i<s;i++)-1===e.indexOf(i)&&n.push(t[i]);return[n,e.map((e=>t[e]))]}(s.shape,r)[1]);return{x:()=>{const e=s.shape.slice();r.forEach((t=>{e[t]=1}));const n=Nn(t,e);return Yn($n(n,Vs(s.shape,"float32")),a)}}}},{kernelName:"Min",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{axis:i}=s,[r,a]=e,o=qr(t,a,r,Yt(i,r.shape));return{x:()=>o.x()}}},{kernelName:"Minimum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>$n(t,bn(vs(n,s),"float32")),b:()=>$n(t,bn(fs(n,s),"float32"))}}},{kernelName:"MirrorPad",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>En(t,r,s.shape)}}},{kernelName:"Mod",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=ts(n.shape,s.shape);return{a:()=>{const e=Qn(n.shape,i);return e.length>0?Nn(Es(t,e),n.shape):t},b:()=>{const e=$n(t,Cs(ps(Yn(n,s)))),r=Qn(s.shape,i);return r.length>0?Nn(Es(e,r),s.shape):e}}}},{kernelName:"Multiply",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=ts(n.shape,s.shape);return{a:()=>{const e=$n(t,bn(s,"float32")),r=Qn(n.shape,i);return r.length>0?Nn(Es(e,r),n.shape):e},b:()=>{const e=$n(t,bn(n,"float32")),r=Qn(s.shape,i);return r.length>0?Nn(Es(e,r),s.shape):e}}}},{kernelName:"Neg",gradFunc:t=>({x:()=>Cs(t)})},{kernelName:"OneHot",inputsToSave:["indices"],gradFunc:(t,e)=>{const n=e[0];return{indices:()=>js(n.shape,"float32")}}},{kernelName:"OnesLike",gradFunc:t=>({x:()=>ss(t)})},{kernelName:"Pack",saveAllInputs:!0,gradFunc:(t,e,n)=>{const{axis:s}=n;return Ri(t,s).map((t=>()=>t))}},Yr,Yr,{kernelName:"Pow",inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(t,e)=>{const[n,s,i]=e,r=n,a=s,o=ts(r.shape,a.shape);return{a:()=>{const e=bn(a,"float32");let n=$n(t,$n(e,si(r,Ts(e,fi(1)))));const s=Qn(r.shape,o);return s.length>0&&(n=Es(n,s)),Nn(n,r.shape)},b:()=>{const e=fs(r,0),n=ns(e,Is(r),ss(r));let s=$n(t,$n(i,n));const l=Qn(a.shape,o);return l.length>0&&(s=Es(s,l)),Nn(s,a.shape)}}}},{kernelName:"Prelu",inputsToSave:["x","alpha"],gradFunc:(t,e)=>{const[n,s]=e,i=fs(n,0);return{x:()=>ns(i,t,$n(t,s)),alpha:()=>{let e=ns(i,ss(t),$n(t,n));const r=Qn(s.shape,t.shape);return r.length>0&&(e=Es(e,r)),Nn(e,s.shape)}}}},{kernelName:"Reciprocal",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,Cs(Zs(n)))}}},{kernelName:"Relu6",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=$n(vs(n,6),$i(n));return{x:()=>$n(t,bn(s,"float32"))}}},{kernelName:"Relu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(t,bn($i(n),"float32"))}}},{kernelName:"Reshape",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Nn(t,n.shape)}}},{kernelName:"ResizeBilinear",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Ze.runKernel("ResizeBilinearGrad",i,n)}}},{kernelName:"ResizeNearestNeighbor",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Ze.runKernel("ResizeNearestNeighborGrad",i,n)}}},{kernelName:"Reverse",gradFunc:(t,e,n)=>{const{dims:s}=n,i=Yt(s,t.shape);return{x:()=>hi(t,i)}}},{kernelName:"Round",gradFunc:t=>({x:()=>ss(t)})},{kernelName:"Rsqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Cs(Yn(t,$n(si(n,1.5),2)))}}},{kernelName:"Select",inputsToSave:["condition"],gradFunc:(t,e)=>{const[n]=e;return{condition:()=>bn(ss(n),"float32"),t:()=>$n(t,bn(n,t.dtype)),e:()=>$n(t,bn(Os(n),t.dtype))}}},{kernelName:"Selu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=fs(n,fi(0)),s=fi(1.7580993408473768),i=fi(1.0507009873554805),r=$n(t,i),a=$n($n(t,s),ls(bn(n,"float32")));return ns(e,r,a)}}}},{kernelName:"Sigmoid",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(t,$n(n,Ts(fi(1),n)))}}},{kernelName:"Sign",gradFunc:t=>({x:()=>ss(t)})},{kernelName:"Sin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(Vn(bn(n,"float32")),t)}}},{kernelName:"Sinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(qn(bn(n,"float32")),t)}}},{kernelName:"Slice",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{begin:i,size:r}=n,a=s.shape,[o,l]=function(t,e,n){let s;const i=t.shape.length;let r;return s="number"==typeof e?[e,...new Array(i-1).fill(0)]:e.length<i?e.concat(new Array(i-e.length).fill(0)):e.slice(),s.forEach((t=>{qt(-1!==t,(()=>"slice() does not support negative begin indexing."))})),r=null==n?new Array(i).fill(-1):"number"==typeof n?[n,...new Array(i-1).fill(-1)]:n.length<i?n.concat(new Array(i-n.length).fill(-1)):n,r=r.map(((e,n)=>e>=0?e:(qt(-1===e,(()=>`Negative size values should be exactly -1 but got ${e} for the slice() size at index ${n}.`)),t.shape[n]-s[n]))),[s,r]}(s,i,r),u=[];for(let e=0;e<t.rank;e++)u.push([o[e],a[e]-o[e]-l[e]]);return{x:()=>ti(t,u)}}},{kernelName:"Softmax",outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{dim:i}=n,r=$n(t,s);return{logits:()=>Ts(r,$n(Es(r,[i],true),s))}}},{kernelName:"Softplus",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(t,Tn(n))}}},Qr,Qr,ta,ta,{kernelName:"Sqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,$n(Ai(bn(n,"float32")),2))}}},{kernelName:"SquaredDifference",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=fi(2);return{a:()=>$n(t,$n(i,Ts(n,s))),b:()=>$n(t,$n(i,Ts(s,n)))}}},{kernelName:"Square",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(t,$n(bn(n,"float32"),2))}}},{kernelName:"Step",gradFunc:t=>({x:()=>ss(t)})},{kernelName:"Sub",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=ts(n.shape,s.shape);return{a:()=>{let e=t;const s=Qn(n.shape,i);return s.length>0&&(e=Es(e,s)),Nn(e,n.shape)},b:()=>{let e=t;const n=Qn(s.shape,i);return n.length>0&&(e=Es(e,n)),Nn(Cs(e),s.shape)}}}},{kernelName:"Sum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,i=s.shape.slice(),{axis:r}=n;Yt(r,s.shape).forEach((t=>{i[t]=1}));const a=Nn(t,i),o=$n(a,Vs(s.shape,"float32"));return{x:()=>o}}},{kernelName:"Tan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Yn(t,Zs(Vn(n)))}}},{kernelName:"Tanh",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>$n(Ts(fi(1),Zs(n)),t)}}},{kernelName:"Tile",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{reps:i}=n;return{x:()=>{let e=ss(s);if(1===s.rank)for(let n=0;n<i[0];++n)e=ln(e,En(t,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)e=ln(e,En(t,[n*s.shape[0],r*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)e=ln(e,En(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error(`Gradient for tile operation is not implemented for rank-${s.rank} tensors yet.`);for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)for(let o=0;o<i[3];++o)e=ln(e,En(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return e}}}},{kernelName:"Transpose",gradFunc:(t,e,n)=>{const s=n,{perm:i}=s,r=_s(i);return{x:()=>Mi(t,r)}}},{kernelName:"Unpack",gradFunc:(t,e,n)=>{const s=n,{axis:i}=s;return{value:()=>Di(t,i)}}},{kernelName:"UnsortedSegmentSum",inputsToSave:["segmentIds"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>function(t,e){const n=Us(e,ss(e)),s=ds(t,n);let i=gs(e,fi(0,"int32"));const r=s.rank-i.rank;for(let t=0;t<r;++t)i=us(i,t+1);i=Ms(i,Vs(s.shape,"bool"));const a=ss(s);return ns(i,s,a)}(t,n)}}},{kernelName:"ZerosLike",gradFunc:t=>({x:()=>ss(t)})}];for(const t of ea)ve(t);let na;function sa(){return null==na&&(na=t().epsilon()),na}class ia extends Error{constructor(t){super(t),Object.setPrototypeOf(this,ia.prototype)}}class ra extends Error{constructor(t){super(t),Object.setPrototypeOf(this,ra.prototype)}}class aa extends Error{constructor(t){super(t),Object.setPrototypeOf(this,aa.prototype)}}class oa extends Error{constructor(t){super(t),Object.setPrototypeOf(this,oa.prototype)}}class la extends Error{constructor(t){super(t),Object.setPrototypeOf(this,la.prototype)}}function ua(t,e){if(Array.isArray(t)){let n=[];for(let s=0;s<e;s++)n=n.concat(t);return n}{const n=new Array(e);return n.fill(t),n}}function ha(t,e){if(!t)throw new la(e)}function ca(t,e){let n=0;for(const s of t)s===e&&n++;return n}function pa(t){return 1===t.length?t[0]:t}function da(t){return Array.isArray(t)?t:[t]}function fa(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function ga(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,((t,e)=>e.toUpperCase()))}let ma={};function ya(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function ba(t){if(null!=t&&"object"==typeof t)if(Array.isArray(t))t.forEach((t=>ba(t)));else{const e=Object.keys(t);for(const n of e){const e=t[n];null!=e&&"object"==typeof e&&(Array.isArray(e)||"ndarray"!==e.type||"number"!=typeof e.value?ba(e):t[n]=e.value)}}}function wa(t,e={},n={},s="object",i=!1){if("string"==typeof t){const i=t;let r;if(i in n)r=n[i];else if(i in ma)r=ma[i];else if(r=e[i],null==r)throw new aa(`Unknown ${s}: ${t}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return r}{const r=t;if(null==r.className||null==r.config)throw new aa(`${s}: Improper config format: ${JSON.stringify(r)}.\n'className' and 'config' must set.`);const a=r.className;let o,l;if(a in n?[o,l]=n[a]:a in ma?[o,l]=ma.className:a in e&&([o,l]=e[a]),null==o)throw new aa(`Unknown ${s}: ${a}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=l){const t={};for(const e of Object.keys(ma))t[e]=ma[e];for(const e of Object.keys(n))t[e]=n[e];r.config.customObjects=t;const e=Object.assign({},ma);for(const t of Object.keys(n))ma[t]=n[t];ba(r.config);const s=l(o,r.config,n,i);return ma=Object.assign({},e),s}{const t=Object.assign({},ma);for(const t of Object.keys(n))ma[t]=n[t];const e=new o(r.config);return ma=Object.assign({},t),e}}}function ka(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function xa(t){if(null==t)return t;const e=[];for(const n of t)-1===e.indexOf(n)&&e.push(n);return e}function va(t){if(null==t)throw new aa(`Invalid value in obj: ${JSON.stringify(t)}`);for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function Sa(t,e,n){if(null!=n&&t.indexOf(n)<0)throw new aa(`${n} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function Ia(t,e,n=0,s=1/0){return ha(n>=0),ha(s>=n),Array.isArray(t)&&t.length>=n&&t.length<=s&&t.every((t=>typeof t===e))}function Na(t,n){Array.isArray(t)?(e.assert(t.length>0,(()=>`${n} is unexpectedly an empty array.`)),t.forEach(((t,e)=>Na(t,`element ${e+1} of ${n}`)))):e.assert(Number.isInteger(t)&&t>0,(()=>`Expected ${n} to be a positive integer, but got ${Aa(t)}.`))}function Aa(t){return null===t?"null":Array.isArray(t)?"["+t.map((t=>Aa(t))).join(",")+"]":"string"==typeof t?`"${t}"`:`${t}`}function Ca(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}function za(t,e){return s((()=>i(r(a(t,t),e,!0))))}class Da extends n.Serializable{getConfig(){return{}}}class $a extends Da{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s((()=>{const e=za(t,this.axis),n=o(e,0,this.maxValue);return a(t,l(n,u(sa(),e)))}))}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}$a.className="MaxNorm",n.registerClass($a);class Ta extends Da{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s((()=>l(t,u(sa(),za(t,this.axis)))))}getConfig(){return{axis:this.axis}}}Ta.className="UnitNorm",n.registerClass(Ta);class Ea extends Da{apply(t){return h(t)}}Ea.className="NonNeg",n.registerClass(Ea);class Fa extends Da{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s((()=>{const e=za(t,this.axis),n=u(a(this.rate,o(e,this.minValue,this.maxValue)),a(1-this.rate,e));return a(t,l(n,u(sa(),e)))}))}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}Fa.className="MinMaxNorm",n.registerClass(Fa);const La={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function _a(t){return ya(t)}function Ra(t,e={}){return wa(t,n.SerializationMap.getMap().classNameMap,e,"constraint")}function Ma(t){if(null==t)return null;if("string"==typeof t){return Ra({className:t in La?La[t]:t,config:{}})}return t instanceof Da?t:Ra(t)}var Oa=Object.freeze({__proto__:null,maxNorm:function(t){return new $a(t)},unitNorm:function(t){return new Ta(t)},nonNeg:function(){return new Ea},minMaxNorm:function(t){return new Fa(t)}});const Ba=["channelsFirst","channelsLast"],Pa=["nearest","bilinear"],Wa=["valid","same","causal"],Ua=["max","avg"],Ka=["sum","mul","concat","ave"],ja=new Map;function Va(t){Sa(Ba,"DataFormat",t)}function qa(t){Sa(Wa,"PaddingMode",t)}function Ga(t){Sa(Ua,"PoolMode",t)}const Ha=[];function Ja(t,e){Ha.push(t);try{const t=e();return Ha.pop(),t}catch(t){throw Ha.pop(),t}}function Za(t){if(!Qa(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===Ha.length?"":Ha.join("/")+"/")+t}function Xa(t){if(!Qa(t))throw new Error("Not a valid tensor name: '"+t+"'");ja.has(t)||ja.set(t,0);const e=ja.get(t);if(ja.set(t,ja.get(t)+1),e>0){const n=`${t}_${e}`;return ja.set(n,1),n}return t}const Ya=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function Qa(t){return!!t.match(Ya)}function to(t,e,n){null==e&&(e=0),null==n&&(n=t.length);let s=1;for(let i=e;i<n;++i)s*=t[i];return s}function eo(t){return t=Array.isArray(t)?new Float32Array(t):t,p(t)}function no(t){return d(eo(t)).dataSync()[0]}function so(t){return c(eo(t)).dataSync()[0]}function io(t,e){if(e<t)throw new aa(`end (${e}) < begin (${t}) is forbidden.`);const n=[];for(let s=t;s<e;++s)n.push(s);return n}function ro(t,e){return t.asType(e)}function ao(t,e=-1){const n=t.shape.slice();return e<0&&(e=n.length+e+1),n.splice(e,0,1),t.reshape(n)}function oo(t,e,n){return s((()=>{switch(t.rank){case 1:return b(t,e,n);case 2:return y(t,[e,0],[n,t.shape[1]]);case 3:return m(t,[e,0,0],[n,t.shape[1],t.shape[2]]);case 4:return g(t,[e,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3]]);case 5:return f(t,[e,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return f(t,[e,0,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new aa(`sliceAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function lo(t,e,n){return s((()=>{switch(t.rank){case 1:return b(t,e,n);case 2:return y(t,[0,e],[t.shape[0],n]);case 3:return m(t,[0,0,e],[t.shape[0],t.shape[1],n]);case 4:return g(t,[0,0,0,e],[t.shape[0],t.shape[1],t.shape[2],n]);default:throw new aa(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function uo(t,e,n,i){return s((()=>{switch(t.rank){case 1:return b(t,e,n);case 2:switch(i){case 1:return oo(t,e,n);case 2:return lo(t,e,n);default:throw new aa(`The axis is not within the rank of the tensor ${i}`)}case 3:switch(i){case 1:return oo(t,e,n);case 2:return m(t,[0,e,0],[t.shape[0],n,t.shape[2]]);case 3:return lo(t,e,n);default:throw new aa(`The axis is not within the rank of the tensor ${i}`)}case 4:switch(i){case 1:return oo(t,e,n);case 2:return g(t,[0,e,0,0],[t.shape[0],n,t.shape[2],t.shape[3]]);case 3:return g(t,[0,0,e,0],[t.shape[0],t.shape[1],n,t.shape[3]]);case 4:return lo(t,e,n);default:throw new aa(`The axis is not within the rank of the tensor ${i}`)}default:throw new aa(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function ho(t,e=-1){let n;return e<0&&(n=t[0].rank,e=0!==n?n:0),e===t[0].rank&&(e=-1),N(t,e)}function co(t,e){switch(t.rank){case 1:return $([t,e]);case 2:return D([t,e],0);case 3:return z([t,e],0);case 4:return C([t,e],0);default:throw new aa(`concatAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}function po(t,e){if(Array.isArray(e)||(e=[e]),t.rank!==e.length)throw new aa(`The length of input n (${e.length}) does not match the number of dimensions in input x (${t.rank})`);return k(t,e)}function fo(t,e=0,n=1,s,i){return x(t,e,n,s,i)}function go(t,e,n,s){if(t.rank<2||e.rank<2)throw new oa(`dot requires both inputs to be rank >= 2 but got x shape = ${t.shape} and y shape = ${e.shape}`);if(e.rank>=3){if(t.shape.slice(-1)[0]!==e.shape.slice(-2)[0])throw new oa(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${t.shape} and  y shape = ${e.shape}`)}if(2===t.rank&&2===e.rank){const i=!1,r=!1;return A.matMul({a:t,b:e,transposeA:i,transposeB:r,bias:s?bo(t.rank,s,"channelsLast"):null,activation:n})}{const i=t.shape.slice(),r=i.pop();t=t.reshape([-1,r]);const a=e.shape.slice(),o=a.pop(),l=a.pop(),u=[...a,o],h=Array.from({length:e.rank},((t,n)=>0===n?e.rank-2:n<=e.rank-2?n-1:n));e=e.transpose(h).reshape([l,-1]);const c=[...i,...u],p=!1,d=!1;return A.matMul({a:t,b:e,transposeA:p,transposeB:d,bias:s?bo(t.rank,s,"channelsLast"):null,activation:n}).reshape(c)}}function mo(t,e,n){return s((()=>(e=Array.isArray(e)?p(e,"int32"):e.toInt(),w(t,e,n))))}function yo(t){return a(t,t)}function bo(t,e,n){const s=e.shape;if(1!==e.rank&&e.rank!==t)throw new aa(`Unexpected bias dimensions: ${e.rank}; expected it to be 1 or ${t}`);if(5===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1,1]):e.reshape([1,s[3],s[0],s[1],s[2]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,1,s[0]]):e.reshape([1].concat(s))}else if(4===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1]):e.reshape([1,s[2],s[0],s[1]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,s[0]]):e.reshape([1].concat(s))}else if(3===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1]):e.reshape([1,s[1],s[0]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,s[0]]):e.reshape([1].concat(s))}else if(t<3)return e;throw new aa(`Unsupported input rank by biasAdd: ${e.rank}`)}function wo(t,e,n){return s((()=>(null==n&&(n="channelsLast"),Va(n),t.add(bo(t.rank,e,n)))))}function ko(t,e,n,i){return s((()=>I(t,e,n,i)))}function xo(t,e,n=!1){return n?t():e()}const vo=["fanIn","fanOut","fanAvg"],So=["normal","uniform","truncatedNormal"];class Io extends n.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class No extends Io{apply(t,e){return T(t,e)}}No.className="Zeros",n.registerClass(No);class Ao extends Io{apply(t,e){return E(t,e)}}Ao.className="Ones",n.registerClass(Ao);class Co extends Io{constructor(t){if(super(),"object"!=typeof t)throw new aa(`Expected argument of type ConstantConfig but got ${t}`);if(void 0===t.value)throw new aa(`config must have value set but got ${t}`);this.value=t.value}apply(t,e){return s((()=>a(F(this.value),E(t,e))))}getConfig(){return{value:this.value}}}Co.className="Constant",n.registerClass(Co);class zo extends Io{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,e){return L(t,this.minval,this.maxval,e)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}zo.className="RandomUniform",n.registerClass(zo);class Do extends Io{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new oa(`randomNormal does not support dType ${e}.`);return fo(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Do.className="RandomNormal",n.registerClass(Do);class $o extends Io{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new oa(`truncatedNormal does not support dType ${e}.`);return _(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}$o.className="TruncatedNormal",n.registerClass($o);class To extends Io{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,e){return s((()=>{if(2!==t.length||t[0]!==t[1])throw new aa("Identity matrix initializer can only be used for 2D square matrices.");return a(this.gain,R(t[0]))}))}getConfig(){return{gain:this.gain}}}To.className="Identity",n.registerClass(To);class Eo extends Io{constructor(t){if(super(),t.scale<0)throw new aa(`scale must be a positive float. Got: ${t.scale}`);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,Sa(vo,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){Sa(So,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,e){const n=function(t,e="channelsLast"){let n,s;if(Va(e),2===t.length)n=t[0],s=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=to(t,2);n=t[1]*e,s=t[0]*e}else if("channelsLast"===e){const e=to(t,0,t.length-2);n=t[t.length-2]*e,s=t[t.length-1]*e}}else{const e=to(t);n=Math.sqrt(e),s=Math.sqrt(e)}return[n,s]}(t),s=n[0],i=n[1];let r=this.scale;if("fanIn"===this.mode?r/=Math.max(1,s):"fanOut"===this.mode?r/=Math.max(1,i):r/=Math.max(1,(s+i)/2),"normal"===this.distribution){const n=Math.sqrt(r);if("float32"!==(e=e||"float32")&&"int32"!==e)throw new oa(`${this.getClassName()} does not support dType ${e}.`);return _(t,0,n,e,this.seed)}{const n=Math.sqrt(3*r);return L(t,-n,n,e)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}Eo.className="VarianceScaling",n.registerClass(Eo);class Fo extends Eo{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Eo.className}}Fo.className="GlorotUniform",n.registerClass(Fo);class Lo extends Eo{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Eo.className}}Lo.className="GlorotNormal",n.registerClass(Lo);class _o extends Eo{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Eo.className}}_o.className="HeNormal",n.registerClass(_o);class Ro extends Eo{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Eo.className}}Ro.className="HeUniform",n.registerClass(Ro);class Mo extends Eo{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Eo.className}}Mo.className="LeCunNormal",n.registerClass(Mo);class Oo extends Eo{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Eo.className}}Oo.className="LeCunNormal",n.registerClass(Oo);class Bo extends Io{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new oa("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,e){return s((()=>{if(t.length<2)throw new oa("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const e=fo(t[0]>t[1]?[t[1],t[0]]:t,0,1,"float32");let n=M.gramSchmidt(e);return t[0]>t[1]&&(n=n.transpose()),a(this.gain,n)}))}getConfig(){return{gain:this.gain,seed:this.seed}}}Bo.className="Orthogonal",n.registerClass(Bo);const Po={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function Wo(t,e={}){return wa(t,n.SerializationMap.getMap().classNameMap,e,"initializer")}function Uo(t){return ya(t)}function Ko(t){if("string"==typeof t){const e=t in Po?Po[t]:t;if("GlorotNormal"===e)return new Lo;if("GlorotUniform"===e)return new Fo;if("HeNormal"===e)return new _o;if("HeUniform"===e)return new Ro;if("LeCunNormal"===e)return new Mo;if("LeCunUniform"===e)return new Oo;{const t={};return t.className=e,t.config={},Wo(t)}}return t instanceof Io?t:Wo(t)}var jo=Object.freeze({__proto__:null,zeros:function(){return new No},ones:function(){return new Ao},constant:function(t){return new Co(t)},randomUniform:function(t){return new zo(t)},randomNormal:function(t){return new Do(t)},truncatedNormal:function(t){return new $o(t)},identity:function(t){return new To(t)},varianceScaling:function(t){return new Eo(t)},glorotUniform:function(t){return new Fo(t)},glorotNormal:function(t){return new Lo(t)},heNormal:function(t){return new _o(t)},heUniform:function(t){return new Ro(t)},leCunNormal:function(t){return new Mo(t)},leCunUniform:function(t){return new Oo(t)},orthogonal:function(t){return new Bo(t)}});let Vo=0;function qo(){return Vo++}const Go={};function Ho(t=""){return t in Go||(Go[t]=0),Go[t]+=1,t+Go[t].toString()}function Jo(t){return Array.isArray(t)&&Array.isArray(t[0])}function Zo(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function Xo(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new aa(`Expected Tensor length to be 1; got ${t.length}`);e=t[0]}else e=t;return e}function Yo(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return(t=t)[0];throw new aa(`Expected exactly 1 Shape; got ${t.length}`)}return t}function Qo(t){let e=0;for(const n of t)0===n.shape.length?e+=1:e+=n.shape.reduce(((t,e)=>t*e));return e}class tl{constructor(t,e="float32",n="Variable",s=!0,i=null){this.dtype=null==e?"float32":e,this.shape=t.shape,this.id=qo(),n=null==n?"Variable":n,this.originalName=Za(n),this.name=Xa(this.originalName),this.trainable_=s,this.constraint=i,this.val=O(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function el(t){return t.map((t=>t.read()))}function nl(t){t.forEach((t=>{t[0].write(t[1])}))}class sl{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class il{constructor(t,e,n,s,i,r,a){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=i,this.outputTensorIndex=a,this.id=qo(),null!=r&&(this.originalName=Za(r),this.name=Xa(this.originalName)),this.rank=e.length}}let rl=0;class al{constructor(t,e){this.callArgs=e,this.id=rl++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let ol=0;class ll extends n.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=ol++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=fa(t)+"_"+Ho(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new ra(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new aa(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return pa(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return pa(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new ia(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new ia(`Layer ${this.name} is not connected, no input to return.`);return pa(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new ia(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new ia(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return pa(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=da(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=da(this.inputSpec);if(t.length!==e.length)throw new aa(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: ${t}`);for(let n=0;n<t.length;n++){const s=t[n],i=e[n];if(null==i)continue;const r=s.rank;if(null!=i.ndim&&r!==i.ndim)throw new aa(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${r}`);if(null!=i.maxNDim&&r>i.maxNDim)throw new aa(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${r}`);if(null!=i.minNDim&&r<i.minNDim)throw new aa(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${r}.`);if(null!=i.dtype&&s.dtype!==i.dtype)throw new aa(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${s.dtype}.`);if(i.axes){const t=s.shape;for(const e in i.axes){const s=Number(e),r=i.axes[e],a=s>=0?t[s]:t[t.length+s];if(null!=r&&-1===[r,null].indexOf(a))throw new aa(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${r} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],r=s.shape[t];if(null!=e&&null!=r&&e!==r)throw new aa(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${s.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=da(t);let s=!0;for(const t of n)if(!(t instanceof il)){s=!1;break}let i=!0;for(const t of n)if(t instanceof il){i=!1;break}if(s===i)throw new aa("Arguments to apply() must be all SymbolicTensors or all Tensors");return Ja(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of da(t))e.push(n.shape);this.build(pa(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);const i=da(s),r=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),r.push(t);if(s=pa(r),null!=this.activityRegularizer)throw new oa("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=da(t);const e=[];for(const n of t)e.push(n.shape);return pa(e)}(t),s=this.computeOutputShape(n);let i;const r="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map(((n,s)=>new il(r,n,this,da(t),e,this.name,s))):new il(r,s,this,da(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new oa("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn(`The rank of the input tensor provided (shape: ${JSON.stringify(t)}) does not match that of the batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer ${this.name}`);else{let e=!1;this.batchInputShape.forEach(((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)})),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: ${JSON.stringify(this.batchInputShape)}`)}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new ia(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new ia(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new ra(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return Qo(this.weights)}build(t){this.built=!0}getWeights(t=!1){return el(t?this.trainableWeights:this.weights)}setWeights(t){s((()=>{const n=this.weights;if(n.length!==t.length)throw new aa(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${n.length} weights. Provided weights: ${t}...`);if(0===n.length)return;const s=[],i=el(n);for(let r=0;r<i.length;++r){const a=i[r],o=n[r],l=t[r];if(!e.arraysEqual(a.shape,l.shape))throw new aa(`Layer weight shape ${a.shape} not compatible with provided weight shape ${l.shape}`);s.push([o,l])}nl(s)}))}addWeight(t,e,n,s,i,r,a){if(-1!==this._addedWeightNames.indexOf(t))throw new aa(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=Ko("zeros"));const o=s.apply(e,n),l=new tl(o,n,t,r,a);return o.dispose(),null!=i&&this.addLoss((()=>i.apply(l.read()))),null==r&&(r=!0),r?this._trainableWeights.push(l):this._nonTrainableWeights.push(l),l}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=da(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}addInboundNode(t,e,n,s,i,r,a=null){const o=da(t);e=da(e),n=da(n),s=da(s),i=Zo(i),r=Zo(r);const l=[],u=[],h=[];for(const t of o)l.push(t.sourceLayer),u.push(t.nodeIndex),h.push(t.tensorIndex);new al({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:h,inputTensors:o,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:r},a);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function ul(t,e,n){if((null==e||null!=n&&n>0)&&(e=t.sourceLayer,n=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[n];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let n=0;n<t.inboundLayers.length;n++){const s=ul(t.inputTensors[n],t.inboundLayers[n],t.nodeIndices[n]);for(const t of s)-1===e.indexOf(t)&&e.push(t)}return e}}}class hl extends ll{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:Ho("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new aa("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new aa("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new aa("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new il(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new al({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new aa(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function cl(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new aa("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new hl({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}async function pl(t){if(null==t)return;const e=[],n=[],s=[];for(const i in t){const r=t[i];if("number"!=typeof r){const t=r;e.push(t.data()),n.push(i),s.push(t)}}if(e.length>0){const i=await Promise.all(e);for(let e=0;e<i.length;++e)t[n[e]]=i[e][0];B(s)}}function dl(t){if(null!=t)for(const e in t){const n=t[e];"number"!=typeof n&&n.dispose()}}var fl;hl.className="InputLayer",n.registerClass(hl),function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(fl||(fl={}));class gl{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class ml{constructor(t,e=10){null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class yl extends gl{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,e){null==e&&(e={});const n=null==e.size?0:e.size;this.seen+=n;for(const t in e){const i=e[t];if("number"==typeof i)this.totals.hasOwnProperty(t)||(this.totals[t]=0),this.totals[t]=this.totals[t]+i*n;else{let e;t in this.totals?e=this.totals[t]:this.totals[t]=0;const r=s((()=>u(this.totals[t],a(i,n))));this.totals[t]=r,null!=e&&e.dispose()}}}async onEpochEnd(t,e){if(null!=e)for(const t of this.params.metrics)null!=this.totals[t]&&("number"==typeof this.totals[t]?e[t]=this.totals[t]/this.seen:s((()=>{const n=a(l(1,this.seen),this.totals[t]);e[t]=n,this.totals[t].dispose(),W(e[t])})))}}class bl extends gl{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const t in e)null==this.history[t]&&(this.history[t]=[]),this.history[t].push(e[t])}async syncData(){const t=[],e=[],n=[];for(const s in this.history){const i=this.history[s];for(let r=0;r<i.length;++r)if("number"!=typeof i[r]){const a=i[r];t.push(a.data()),e.push(s),n.push(r)}}const s=await Promise.all(t);for(let t=0;t<s.length;++t){this.history[e[t]][n[t]].dispose(),this.history[e[t]][n[t]]=s[t][0]}}}class wl extends gl{constructor(t,n){if(super(),this.currentEpoch=0,this.yieldEvery=n||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");e.isNumber(this.yieldEvery)&&(this.maybeWait=function(t,n){let s,i=e.now();return(...r)=>{const a=e.now();return a-i<n||(i=a,s=t(...r)),s}}(this.maybeWait.bind(this),this.yieldEvery)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,e,n){const s=[];null!=this.yield&&(await pl(n),s.push(this.yield(t,e,n))),s.push(P()),await Promise.all(s)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await pl(e),await this.epochBegin(t,e))}async onEpochEnd(t,e){const n=[];null!=this.epochEnd&&(await pl(e),n.push(this.epochEnd(t,e))),"epoch"===this.yieldEvery&&n.push(P()),await Promise.all(n)}async onBatchBegin(t,e){null!=this.batchBegin&&(await pl(e),await this.batchBegin(t,e))}async onBatchEnd(t,n){const s=[];null!=this.batchEnd&&(await pl(n),s.push(this.batchEnd(t,n))),"batch"===this.yieldEvery?s.push(P()):e.isNumber(this.yieldEvery)&&s.push(this.maybeWait(this.currentEpoch,t,n)),await Promise.all(s)}async onTrainBegin(t){null!=this.trainBegin&&(await pl(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await pl(t),await this.trainEnd(t))}}function kl(t,e){if(null==t&&(t={}),t instanceof gl)return[t];if(Array.isArray(t)&&t[0]instanceof gl)return t;return da(t).map((t=>new wl(t,e)))}class xl{constructor(){}static registerCallbackConstructor(t,n){e.assert(t>=0&&Number.isInteger(t),(()=>`Verbosity level is expected to be an integer >= 0, but got ${t}`)),xl.checkForDuplicate(n),null==xl.constructors[t]&&(xl.constructors[t]=[]),xl.constructors[t].push(n)}static checkForDuplicate(t){for(const e in xl.constructors){xl.constructors[+e].forEach((e=>{if(e===t)throw new aa("Duplicate callback constructor.")}))}}static clear(){xl.constructors={}}static createCallbacks(t){const e=[];for(const n in xl.constructors){const s=+n;t>=s&&e.push(...xl.constructors[s])}return e.map((t=>new t))}}function vl(t,e,n,s,i,r,a,o,l){const u=new bl,h=[new yl,...xl.createCallbacks(e)];null!=t&&h.push(...t),h.push(u);const c=new ml(h);return c.setParams({epochs:n,initialEpoch:s,samples:i,steps:r,batchSize:a,verbose:e,doValidation:o,metrics:l}),{callbackList:c,history:u}}function Sl(t,e={},s=!1){return wa(t,n.SerializationMap.getMap().classNameMap,e,"layer",s)}function Il(t,e){return s((()=>{"float32"!==t.dtype&&(t=t.asType("float32"));const n=r(yo(t),e,!0),s=G(n.shape,sa()),a=i(H(n,s));return l(t,a)}))}function Nl(t,e){return s((()=>j(yo(K(e,t)),-1)))}function Al(t,e){return s((()=>j(S(K(e,t)),-1)))}function Cl(t,e){return s((()=>{const n=K(t,e),s=o(S(t),sa(),Number.MAX_VALUE),i=S(l(n,s));return a(100,j(i,-1))}))}function zl(t,e,n=!1){return s((()=>{if(n)e=V(e);else{const t=r(e,e.shape.length-1,!0);e=l(e,t)}return e=o(e,sa(),1-sa()),q(r(a(t.toFloat(),U(e)),e.shape.length-1))}))}function Dl(t,e,n=!1){return s((()=>{const s=J(function(t){const e=[to(t.shape)];return t.reshape(e)}(t)).toInt(),i=(e=o(e,sa(),1-sa())).shape;return zl(Z(s,i[i.length-1]).reshape(i),e,n)}))}function $l(t,n){return s((()=>{let i;return i=o(n,sa(),1-sa()),i=U(l(i,K(1,i))),j(function(t,n){if(!e.arraysEqual(t.shape,n.shape))throw new aa(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(n.shape)}`);return s((()=>{const e=n.relu(),s=n.abs().neg();return e.sub(n.mul(t)).add(s.exp().log1p())}))}(t,i),-1)}))}function Tl(t,e){return s((()=>{const n=Il(t,-1),s=Il(e,-1),i=a(n,s);return q(r(i,-1))}))}xl.constructors={};const El={meanSquaredError:Nl,meanAbsoluteError:Al,meanAbsolutePercentageError:Cl,meanSquaredLogarithmicError:function(t,e){return s((()=>{const n=o(e,sa(),Number.MAX_VALUE),s=U(u(1,n)),i=o(t,sa(),Number.MAX_VALUE),r=U(u(1,i));return j(yo(K(s,r)),-1)}))},squaredHinge:function(t,e){return s((()=>{const n=H(0,K(1,a(t,e)));return j(yo(n),-1)}))},hinge:function(t,e){return s((()=>{const n=H(0,K(1,a(t,e)));return j(n,-1)}))},categoricalHinge:function(t,e){return s((()=>{const n=r(a(t,e),-1),s=c(a(K(1,t),e),-1);return H(0,u(1,K(s,n)))}))},logcosh:function(t,e){return s((()=>{const n=Math.log(2),s=K(e,t),i=K(u(s,X(a(-2,s))),n);return j(i,-1)}))},categoricalCrossentropy:zl,sparseCategoricalCrossentropy:Dl,binaryCrossentropy:$l,kullbackLeiblerDivergence:function(t,e){return s((()=>{const n=o(t,sa(),1),s=o(e,sa(),1);return r(a(t,U(l(n,s))),-1)}))},poisson:function(t,e){return s((()=>{const n=U(u(sa(),e));return j(K(e,a(t,n)),-1)}))},cosineProximity:Tl};function Fl(t){if("string"==typeof t){if(t in El)return El[t];let e=`Unknown loss ${t}`;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new aa(e)}return t}function Ll(t,e){return s((()=>{const n=a(.5,Y(e)),s=ro(Q(e,n),t.dtype);return j(tt(t,s),-1)}))}function _l(t,e){return s((()=>ro(tt(et(t,-1),et(e,-1)),"float32")))}function Rl(t,e){return s((()=>nt(t.equal(1),e.equal(1)).sum().cast("float32")))}function Ml(t,e){return s((()=>{const n=Rl(t,e),i=function(t,e){return s((()=>nt(t.equal(0),e.equal(1)).sum().cast("float32")))}(t,e),r=n.add(i);return st(Q(r,0),n.div(r),0).cast("float32")}))}function Ol(t,e){return s((()=>{const n=Rl(t,e),i=function(t,e){return s((()=>nt(t.equal(1),e.equal(0)).sum().cast("float32")))}(t,e),r=n.add(i);return st(Q(r,0),n.div(r),0).cast("float32")}))}function Bl(t,e){return $l(t,e)}function Pl(t,e){return t.rank===e.rank&&(t=t.squeeze([t.rank-1])),(e=e.argMax(-1)).dtype!==t.dtype&&(e=e.asType(t.dtype)),tt(t,e).asType("float32")}const Wl=zl,Ul=Dl,Kl={binaryAccuracy:Ll,categoricalAccuracy:_l,precision:Ml,categoricalCrossentropy:Wl,sparseCategoricalCrossentropy:Ul,mse:Nl,MSE:Nl,mae:Al,MAE:Al,mape:Cl,MAPE:Cl,cosine:Tl};function jl(t){if("string"==typeof t&&t in Kl)return Kl[t];if("string"!=typeof t&&null!=t)return t;throw new aa(`Unknown metric ${t}`)}function Vl(t){if(ha(null!==t,`Unknown LossOrMetricFn ${t}`),"string"==typeof t)return t;{let e;for(const n of Object.keys(El))if(El[n]===t){e=n;break}if(void 0!==e)return e;for(const n of Object.keys(Kl))if(Kl[n]===t){e=n;break}return void 0!==e?e:t.name}}function ql(t,e,n=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!Gl(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(t);n.length>1048576&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${n.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function Gl(t){if(null===t)return!0;if("object"==typeof t){if(Object.getPrototypeOf(t)===Object.prototype){const e=Object.keys(t);for(const n of e){if("string"!=typeof n)return!1;if(!Gl(t[n]))return!1}return!0}if(Array.isArray(t)){for(const e of t)if(!Gl(e))return!1;return!0}return!1}{const e=typeof t;return"string"===e||"number"===e||"boolean"===e}}function Hl(t,e,n,s=console.log){const i=function(t){let e=!0;const n=[],s=[];for(const e in t.nodesByDepth)n.push(t.nodesByDepth[e]);for(const t of n){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}s.push(...t)}if(e)for(const n of t.layers){let t=!1;for(const i of n.inboundNodes)if(-1!==s.indexOf(i)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),r=["Layer (type)","Output shape","Param #"];let a;if(i?(e=e||65,n=n||[.45,.85,1]):(e=e||98,n=n||[.33,.55,.67,1]),n[n.length-1]<=1&&(n=n.map((t=>Math.floor(e*t)))),!i){r.push("Receives inputs"),a=[];for(const e in t.nodesByDepth)a.push(...t.nodesByDepth[e])}s("_".repeat(e)),Jl(r,n,s),s("=".repeat(e));const o=t.layers;for(let t=0;t<o.length;++t)i?Zl(o[t],n,s):Xl(o[t],n,a,s),s((t===o.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const l=function(t){let e;e=null!=t.collectedTrainableWeights?Qo(t.collectedTrainableWeights):Qo(t.trainableWeights);return e}(t),u=Qo(t.nonTrainableWeights);s(`Total params: ${l+u}`),s(`Trainable params: ${l}`),s(`Non-trainable params: ${u}`),s("_".repeat(e))}function Jl(t,e,n=console.log){let s="";for(let n=0;n<t.length;++n)n>0&&(s=s.slice(0,s.length-1)+" "),s+=t[n],s=s.slice(0,e[n]),s+=" ".repeat(e[n]-s.length);n(s)}function Zl(t,e,n){let s;try{s=JSON.stringify(t.outputShape)}catch(t){s="multiple"}Jl([`${t.name} (${t.getClassName()})`,s,t.countParams().toString()],e,n)}function Xl(t,e,n,s){let i;try{i=JSON.stringify(t.outputShape)}catch(t){i="multiple"}const r=[];for(const e of t.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const n=e.inboundLayers[t].name,s=e.nodeIndices[t],i=e.tensorIndices[t];r.push(`${n}[${s}][${i}]`)}const a=t.name,o=t.getClassName(),l=0===r.length?"":r[0];Jl([`${a} (${o})`,i,t.countParams().toString(),l],e,s);for(let t=1;t<r.length;++t)Jl(["","","",r[t]],e,s)}function Yl(t,e,n){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof n}function Ql(t,e){if(null===t)return null;if("string"==typeof t)return ga(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Yl(e,i,s)?n.push(s):n.push(Ql(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n];if("name"===n&&"string"==typeof s)e[n]=s;else{const t=ga(n);e[t]=Ql(s,t)}}return e}}function tu(t,e){if(null==t)return null;if("string"==typeof t)return fa(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Yl(e,i,s)?n.push(s):n.push(tu(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n],i=fa(n);e[i]="name"!==n&&"className"!==n||"string"!=typeof s?tu(s,n):s}return e}}const eu="3.4.0";class nu{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof nu)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,e,n){if(null!=this.id2Value[t.id])throw new aa(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,e){if(null==t.dtype||t.dtype===e.dtype)return e;try{return at(e,t.dtype)}catch(n){throw new aa(`The dtype of the feed (${e.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,e),this.name2Id[t.name]=t.id,null!=n&&(this.id2Mask[t.id]=n),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof il){if(null==this.id2Value[t.id])throw new aa(`Nonexistent key: ${t.name}`);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new aa(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Value[e]}}getMask(t){if(t instanceof il){if(null==this.id2Value[t.id])throw new aa(`Nonexistent key: ${t.name}`);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new aa(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&B(this.id2Mask)}}const su={},iu={};function ru(t,n,s,i){const r=null!=s&&s.training,a=Array.isArray(t),o=a?t:[t],l=o.map((t=>t.name)),u=[],h=n.names();for(const t of l)-1!==h.indexOf(t)?u.push(n.getValue(t)):u.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const c=l.join(",")+"|"+n.names().join(",");let p,d;if(null==su[c]){const t=function(t,n){e.assert(null!=t&&t.length>0,(()=>"Expected at least one fetch, got none"));let s=[],i={};if(1===t.length){const e=ou(t[0],n);s=e.sorted,i=e.recipientMap}else{const e=new Set;for(const r of t){const{sorted:t,recipientMap:a}=ou(r,n);for(const n of t)e.has(n.name)||(s.push(n),e.add(n.name));for(const t in a)null==i[t]&&(i[t]=new Set),a[t].forEach((e=>i[t].add(e)))}}return{sorted:s,recipientCounts:au(i)}}(o,n);p=t.sorted,d=t.recipientCounts,su[c]=p,iu[c]=d}p=su[c],d={},r||Object.assign(d,iu[c]);const f=new nu(n);for(let t=0;t<p.length;++t){if(null!=i){const t=rt().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const e=p[t],a=e.sourceLayer;if(a instanceof hl)continue;const o=[],h=[],c=[];let g=!1;for(const t of e.inputs){const e=f.getValue(t),s=f.getMask(t);o.push(e),h.push(s),null!=s&&(g=!0),r||(d[t.name]--,0!==d[t.name]||n.hasKey(t)||-1!==l.indexOf(t.name)||e.isDisposed||!0===t.sourceLayer.stateful||c.push(e))}g&&((s=s||{}).mask=h[0]);const m=da(a.apply(o,s));let y=null;a.supportsMasking&&(y=a.computeMask(o,h));const b=lu(e),w=Array.isArray(b)?b:[b];for(let t=0;t<w.length;++t){f.hasKey(w[t])||f.add(w[t],m[t],Array.isArray(y)?y[0]:y);const e=l.indexOf(w[t].name);-1!==e&&(u[e]=m[t])}r||B(c)}return f.disposeMasks(),a?u:u[0]}function au(t){const e={};for(const n in t)e[n]=t[n].size;return e}function ou(t,e){const n=new Set,s=[],i={};for(const t of e.names())n.add(t);const r=[],a=[];for(r.push(t);r.length>0;){const t=r[r.length-1];if(n.has(t.name)){r.pop();continue}const e=a[a.length-1]===r.length-1;if(0===t.inputs.length||e)r.pop(),s.push(t),n.add(t.name),e&&a.pop();else{a.push(r.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||r.push(e)}}return{sorted:s,recipientMap:i}}function lu(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}class uu extends ll{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=Ho(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],xa(this.inputs).length!==this.inputs.length)throw new aa(`The list of inputs passed to the model is redundant. All inputs should only appear once. Found: ${this.inputs.map((t=>t.name))}`);xa(this.outputs).length!==this.outputs.length&&console.warn(`The list of outputs passed to the model is redundant. All outputs should only appear once. Found: ${this.outputs.map((t=>t.name))}`),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const t of this.outputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;this.outputLayers.push(e),this.outputLayersNodeIndices.push(n),this.outputLayersTensorIndices.push(s)}for(const t of this.inputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;ha(0===n,"input layer has >1 nodes"),ha(0===s,"input layer has >1 tensors"),this.inputLayers.push(e),this.inputLayersNodeIndices.push(n),this.inputLayersTensorIndices.push(s)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let e=0;e<this.inputLayers.length;e++){const n=this.inputLayers[e];if(!(n instanceof hl))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${t.inputs}. Input ${e} (0-based) originates from layer type ${n.getClassName()}.`);this.inputNames.push(n.name),this.feedInputShapes.push(n.batchInputShape),this.feedInputNames.push(n.name)}for(const t of this.outputLayers)this.outputNames.push(t.name);this.internalInputShapes=this.inputs.map((t=>t.shape)),this.internalOutputShapes=this.outputs.map((t=>t.shape));const e={},n={},s={},i={},r={},a=[],o=(t,e,n,s,i,l)=>{null!=s&&null!=i&&null!=l||(s=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=s.inboundNodes[i];if(-1!==n.indexOf(u))throw new ra(`The tensor ${t.name} at layer "${s.name}" is part of a cycle.`);if(-1!==e.indexOf(u))return;this.containerNodes.add(uu.nodeKey(s,i)),s.id in r||(r[s.id]=Object.keys(r).length),-1===n.indexOf(u)&&n.push(u);const h=u.inboundLayers.length;for(let t=0;t<h;t++){const s=u.inputTensors[t],i=u.inboundLayers[t],r=u.nodeIndices[t],a=u.tensorIndices[t];o(s,e,n,i,r,a)}for(e.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);a.push(u)},l=[],u=[];for(const t of this.outputs)o(t,l,u);const h=a.slice().reverse();for(const t of h){n[t.id]=t,t.id in e||(e[t.id]=0);let r=e[t.id];const a=null==s[t.outboundLayer.id]?0:s[t.outboundLayer.id];r=Math.max(r,a),s[t.outboundLayer.id]=r,i[t.outboundLayer.id]=t.outboundLayer,e[t.id]=r;for(let s=0;s<t.inboundLayers.length;s++){const i=t.inboundLayers[s],a=t.nodeIndices[s],o=i.inboundNodes[a],l=null==e[o.id]?0:e[o.id];e[o.id]=Math.max(r+1,l),n[o.id]=o}}const c={};for(const t in e){const s=e[t];s in c||(c[s]=[]),c[s].push(n[t])}const p={};for(const t in s){const e=s[t];e in p||(p[e]=[]),p[e].push(i[t])}let d=Object.keys(p).map((t=>parseInt(t,10))).sort(ka);this.layers=[];for(const t of d){const e=p[t];e.sort(((t,e)=>{const n=r[t.id],s=r[e.id];return n<s?-1:n>s?1:0}));for(const t of e)t instanceof uu&&this.internalContainerRefs.push(t),this.layers.push(t)}this.layersByDepth=p,d=Object.keys(c).map((t=>parseInt(t,10))).sort(ka);const f=this.inputs.slice(),g=[];for(const t of d)for(const e of c[t]){const t=e.outboundLayer;if(null!=t){for(const n of e.inputTensors)if(-1===f.indexOf(n))throw new ra(`Graph disconnected: cannot obtain value for tensor ${n} at layer "${t.name}". The following previous layers were accessed without issue: ${g}`);for(const t of e.outputTensors)f.push(t);g.push(t.name)}}this.nodesByDepth=c;const m=this.layers.map((t=>t.name));for(const t of m){const e=m.filter((e=>e===t)).length;if(1!==e)throw new ra(`The name "${t}" is used ${e} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(m))}this.outboundNodes=[],this.inboundNodes=[],new al({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map((t=>null)),outputMasks:this.outputs.map((t=>null)),inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs.map((t=>t.shape))}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach((e=>{e._trainableWeights.forEach((e=>e.trainable=t))})),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new aa("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t,e=!0){const n={};let s=0;for(const t of this.layers)for(const e of t.weights){if(null!=n[e.originalName])throw new aa(`Duplicate weight name: ${e.originalName}`);n[e.originalName]=e,s++}const i=[];for(const s in t){let r=s;if(null==n[s]){const t=s.split("/");r=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=n[r])i.push([n[r],t[s]]);else if(e)throw new aa(`Provided weight data has no target variable: ${s}`);delete n[r]}if(e){const t=[];for(const e in n)t.push(e);if(t.length>0)throw new aa(`${t.length} of ${s} weights are not set: ${t}`)}nl(i)}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion="tfjs-layers 3.4.0",e.backend="TensorFlow.js",e}toJSON(t,e=!0){const n=tu(this.updatedConfig());return e?JSON.stringify(n):n}call(t,e){return s((()=>{t=da(t);const n=new nu;for(let e=0;e<this.inputs.length;++e)n.add(this.inputs[e],t[e]);return ru(this.outputs,n,e)}))}computeMask(t,e){return s((()=>{let n;return t=da(t),n=null==e?ua(null,t.length):da(e),this.runInternalGraph(t,n)[1]}))}computeOutputShape(t){const e=Zo(t);if(e.length!==this.inputLayers.length)throw new aa(`Invalid inputShape argument ${t}: model has ${this.inputLayers.length} tensor inputs.`);const n={};for(let t=0;t<e.length;t++){const s=this.inputLayers[t],i=e[t];n[s.name+"_0_0"]=i}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(ka);if(s.length>1)for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer;if(-1!==this.inputLayers.map((t=>t.id)).indexOf(e.id))continue;const s=[];for(let e=0;e<t.inboundLayers.length;e++){const i=t.inboundLayers[e],r=t.nodeIndices[e],a=t.tensorIndices[e],o=n[`${i.name}_${r}_${a}`];s.push(o)}const i=Zo(e.computeOutputShape(pa(s))),r=e.inboundNodes.indexOf(t);for(let t=0;t<i.length;t++){n[`${e.name}_${r}_${t}`]=i[t]}}}const i=[],r=[];for(let t=0;t<this.outputLayers.length;t++){const e=this.outputLayers[t],n=this.outputLayersNodeIndices[t],s=this.outputLayersTensorIndices[t],i=`${e.name}_${n}_${s}`;r.push(i)}for(let t=0;t<r.length;t++){const e=r[t];ha(e in n),i.push(n[e])}return pa(i)}runInternalGraph(t,e){null==e&&(e=ua(null,t.length));const n={};for(let s=0;s<this.inputs.length;++s){const i=this.inputs[s],r=t[s],a=e[s];n[i.id]=[r,a]}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(ka);for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer,s=t.inputTensors,i=t.outputTensors,r=new Array;for(const t of s)t.id in n&&r.push(n[t.id]);if(r.length===s.length){let s,a,o,l,u={};if(null!=t.callArgs&&(u=t.callArgs),1===r.length){const[t,n]=r[0];null==u.mask&&(u.mask=n),o=da(e.call(t,u)),l=da(e.computeMask(t,n)),s=[t],a=[n]}else s=r.map((t=>t[0])),a=r.map((t=>t[1])),null==u.mask&&(u.mask=a),o=da(e.call(s,u)),l=da(e.computeMask(s,a));if(e.activityRegularizer)throw new oa("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],s=o[t],r=l[t];n[e.id]=[s,r]}}}}const i=[],r=[],a=[];for(const t of this.outputs){ha(t.id in n,`Could not compute output ${t.name} : ${t.id}`);const[e,s]=n[t.id];a.push(e.shape),i.push(e),r.push(s)}return[i,r,a]}buildNodeConversionMap(t){const e={};let n;for(const t of this.layers){n=t instanceof uu?1:0;for(let s=0;s<t.inboundNodes.length;s++){const i=uu.nodeKey(t,s);this.containerNodes.has(i)&&(e[i]=n,n+=1)}}return e}getLayer(t,e){if(null!=e){if(this.layers.length<=e)throw new aa(`Was asked to retrieve layer at index ${e}, but model only has ${this.layers.length} layer(s).`);return this.layers[e]}if(null==t)throw new aa("Provide either a layer name or layer index");for(const e of this.layers)if(e.name===t)return e;throw new aa(`No such layer: ${t}`)}calculateLosses(){return s((()=>{const t=[];for(const e of this.layers)for(let n=0;n<e.inboundNodes.length;++n){const s=uu.nodeKey(e,n);this.containerNodes.has(s)&&t.push(...e.calculateLosses())}return t}))}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),n=[];for(const t of this.layers){const s=t.getClassName(),i=t.getConfig(),r=[];for(let n=0;n<t.inboundNodes.length;n++){const s=t.inboundNodes[n],i=uu.nodeKey(t,n);let a={};if(this.containerNodes.has(i)){if(s.callArgs)try{JSON.stringify(s.callArgs),a=s.callArgs}catch(e){console.warn(`Layer ${t.name} was passed non-serializable keyword arguments: ${s.callArgs}. They will not be included in the serialized model (and thus will be missing at deserialization time).`),a={}}if(s.inboundLayers.length>0){const t=[];for(let n=0;n<s.inboundLayers.length;n++){const i=s.inboundLayers[n],r=s.nodeIndices[n],o=s.tensorIndices[n];let l=e[uu.nodeKey(i,r)];null==l&&(l=0),t.push([i.name,l,o,a])}r.push(t)}}}const a={};a.name=t.name,a.className=s,a.config=i,a.inboundNodes=r,n.push(a)}t.layers=n;const s=[];for(let t=0;t<this.inputLayers.length;t++){const n=this.inputLayers[t],i=this.inputLayersNodeIndices[t],r=uu.nodeKey(n,i);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.inputLayersTensorIndices[t];s.push([n.name,a,o])}t.inputLayers=s;const i=[];for(let t=0;t<this.outputLayers.length;t++){const n=this.outputLayers[t],s=this.outputLayersNodeIndices[t],r=uu.nodeKey(n,s);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.outputLayersTensorIndices[t];i.push([n.name,a,o])}return t.outputLayers=i,t}static fromConfig(t,e,n={},s=!1){const i={},r={};function a(t,e){t.name in r?r[t.name].push(e):r[t.name]=[e]}function o(t,e){const n=[];let s;for(const r of e){const o=r[0],l=r[1],u=r[2];if(s=null==r[3]?{}:r[3],!(o in i))return void a(t,e);const h=i[o];if(h.inboundNodes.length<=l)return void a(t,e);const c=h.inboundNodes[l];n.push(c.outputTensors[u])}n.length>0&&t.apply(pa(n),s)}function l(t){const n=t.name,r=Sl(t,null!=e.customObjects?e.customObjects:{});r.setFastWeightInitDuringBuild(s),i[n]=r;t.inboundNodes.forEach((t=>{if(!(t instanceof Array))throw new aa(`Corrupted configuration, expected array for nodeData: ${t}`);a(r,t)}))}const u=e.name,h=e.layers;for(const t of h)l(t);for(;!va(r);)for(const t of h){const e=i[t.name];if(e.name in r){const t=r[e.name];delete r[e.name];for(const n of t)o(e,n)}}const c=[],p=[],d=e.inputLayers;for(const t of d){const e=t[0],n=t[1],s=t[2];ha(e in i);const r=i[e].inboundNodes[n].outputTensors;c.push(r[s])}const f=e.outputLayers;for(const t of f){const e=t[0],n=t[1],s=t[2];ha(e in i);const r=i[e].inboundNodes[n].outputTensors;p.push(r[s])}return new t({inputs:c,outputs:p,name:u})}get stateful(){if(this._stateful)throw new aa("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){s((()=>{this.layers.forEach((t=>{t.stateful&&t.resetStates()}))}))}}function hu(t,e){return function(t,e,n){const s=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===s)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==s)throw new Error(`Provided ${n} is an array of ${t.length} element(s), but the model has ${s} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const n=[];return e.forEach((e=>{e in t?n.push(t[e]):n.push(null)})),n}throw new Error(`The model has multiple (${s}) outputs, so ${n} must be either an array with ${s} elements or an object with ${e} keys. Provided ${n} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function cu(t,e,n,i){if(null!=e||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=n){const e=s((()=>{if(1===t.shape.length)return t.clone();if(2===t.shape.length){if(t.shape[1]>1){const e=1;return t.argMax(e)}if(1===t.shape[1])return t.reshape([t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),i=Array.from(await e.data());B(e);const r=[];return i.forEach((t=>{if(null==n[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);r.push(n[t])})),p(r,"float32")}return null}function pu(t,e){return a(t,e)}function du(t,n){let s,i;const r=n;s=r.xs,i=r.ys,e.assert(null!=s&&null!=i,(()=>`A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${n}`));const a=fu("input",t.inputNames,s),o=fu("output",t.outputNames,i),l=a[0].shape[0];e.assert(a.length===t.inputs.length,(()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: ${JSON.stringify(t.inputNames)})`)),e.assert(o.length===t.outputs.length,(()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${o.length} outputs.  (Expected output keys: ${JSON.stringify(t.outputNames)})`));for(let n=0;n<a.length;n++)e.assert(a[n].shape[0]===l,(()=>`Batch size mismatch: input ${t.inputNames[n]} has ${a[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));for(let n=0;n<o.length;n++)e.assert(o[n].shape[0]===l,(()=>`Batch size mismatch: output ${t.outputNames[n]} has ${o[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));return{xs:a,ys:o}}function fu(t,n,s){if(s instanceof ot)return[s];if(Array.isArray(s))return e.assert(s.length===n.length,(()=>`Received an array of ${s.length} Tensors, but expected ${n.length} to match the ${t} keys ${n}.`)),s;{const e=[];for(const i of n){if(null==s[i])throw new aa(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);e.push(s[i])}return e}}async function gu(t,n,s){const i=null!=s.batchesPerEpoch;if(e.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),e.assert(null!=s,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),e.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),(()=>`For fitDataset(), config.epochs is expected to be a positive integer, but got ${s.epochs}`)),e.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),(()=>`For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${s.batchesPerEpoch}`)),e.assert(null==s.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const r=null!=s.validationData;let a,o;if(r)if(mu(s.validationData))e.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),(()=>`For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${s.validationBatches}`));else{const t=function(t){if(3===t.length)throw new oa("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);a=t.xs,o=t.ys}const l=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let h;h=r?u.slice().concat(u.map((t=>"val_"+t))):u.slice();const c=kl(s.callbacks,s.yieldEvery),p=null==s.verbose?1:s.verbose,{callbackList:d,history:f}=vl(c,p,s.epochs,null,null,function(t,e){let n=null;null!=e.batchesPerEpoch?n=e.batchesPerEpoch:Number.isFinite(t.size)&&(n=t.size);return n}(n,s),null,r,h);d.setModel(t),t.history=f,await d.onTrainBegin(),t.stopTraining_=!1;let g=null==s.initialEpoch?0:s.initialEpoch,m=await n.iterator();for(;g<s.epochs;){const e={};await d.onEpochBegin(g);let h=0,c=0;for(i||(m=await n.iterator());!i||h<s.batchesPerEpoch;){const n=await m.next();if(i&&n.done){console.warn(`You provided \`batchesPerEpoch\` as ${s.batchesPerEpoch}, but your dataset iterator ran out of data after ${h} batches; interrupting training. Make sure that your dataset can generate at least \`batchesPerEpoch * epochs\` batches (in this case, `+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=n.value){const{xs:e,ys:i}=du(t,n.value),r={};r.batch=c,r.size=e[0].shape[0],await d.onBatchBegin(c,r);const a=[];if(null!=s.classWeight){const e=hu(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)a.push(await cu(i[t],null,e[t]))}const o=e.concat(i).concat(a),p=l(o);B(o);for(let t=0;t<u.length;++t){const e=u[t],n=p[t];r[e]=n,W(n)}await d.onBatchEnd(c,r),dl(r),c++,h++}if(i?h>=s.batchesPerEpoch:n.done){if(r){let n;n=mu(s.validationData)?da(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):da(t.evaluate(a,o,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let s=0;s<t.metricsNames.length;++s)e[`val_${t.metricsNames[s]}`]=n[s]}break}if(t.stopTraining_)break}if(await d.onEpochEnd(g,e),g++,t.stopTraining_)break}return await d.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function mu(t){return"function"==typeof t.iterator}function yu(t){e.assert(t>0&&Number.isInteger(t),(()=>`batchSize is required to be a positive integer, but got ${t}`))}function bu(t,e,n){return null==t?[null]:Array.isArray(t)?t.map((t=>oo(t,e,n-e))):oo(t,e,n-e)}function wu(t,e){return s((()=>null==t?null:Array.isArray(t)?t.map((t=>wu(t,e))):mo(t,"int32"===e.dtype?e:e.toInt())))}function ku(t,e){const n=[];let s=0,i=null;for(;s<t;)i=s+e,i>=t&&(i=t),n.push([s,i]),s=i;return n}async function xu(t,n,i,r={}){if(t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let a,o,l,u,h,c,d;t.isTraining=!0;try{const f=null==r.batchSize?32:r.batchSize;yu(f);const g=!1,m=await t.standardizeUserData(n,i,r.sampleWeight,r.classWeight,g,f);a=m[0],o=m[1],d=m[2];let y,b=!1;if(null!=r.validationData&&r.validationData.length>0){if(b=!0,2!==r.validationData.length)throw 3===r.validationData.length?new oa("validationData including sample weights is not supported yet."):new aa(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${r.validationData} is invalid.`);l=r.validationData[0],u=r.validationData[1];const e=!0,n=await t.standardizeUserData(l,u,null,null,e,f);h=n[0],c=n[1],y=h.concat(c)}else if(null!=r.validationSplit&&r.validationSplit>0&&r.validationSplit<1){b=!0;const t=Math.floor(a[0].shape[0]*(1-r.validationSplit)),e=a[0].shape[0];h=bu(a,t,e),a=bu(a,0,t),c=bu(o,t,e),o=bu(o,0,t),y=h.concat(c)}else null!=r.validationSteps&&(b=!0);const w=a.concat(o).concat(d);t.checkTrainableWeightsConsistency();const k=t.makeTrainFunction(),x=t.getDedupedMetricsNames();let v,S;b?(t.makeTestFunction(),v=t.testFunction,S=x.slice().concat(x.map((t=>"val_"+t)))):(v=null,y=[],S=x.slice());const I=kl(r.callbacks,r.yieldEvery);return await async function(t,n,i,r,a,o,l,u,h,c,d,f,g,m,y){null==a&&(a=32),null==o&&(o=1),null==d&&(d=!0),null==g&&(g=0);let b=!1;if(null!=h&&null!=c&&(b=!0),null!=y&&(b=!0,null==m))throw new aa("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const w=t.checkNumSamples(i,a,m,"steps_per_epoch");let k;null!=w&&(k=io(0,w)),null==l&&(l=1);const{callbackList:x,history:v}=vl(u,l,o,g,w,m,a,b,f);x.setModel(t),t.history=v,await x.onTrainBegin(),t.stopTraining_=!1;for(let l=g;l<o;++l){await x.onEpochBegin(l);const o={};if(null!=m)throw new oa("stepsPerEpoch mode is not implemented yet.");{if("batch"===d)throw new oa("batch shuffling is not implemneted yet");d&&e.shuffle(k);const l=p(k),u=ku(w,a);for(let e=0;e<u.length;++e){const p={};if(await x.onBatchBegin(e,p),s((()=>{const s=u[e][0],d=u[e][1],f=oo(l,s,d-s);p.batch=e,p.size=d-s;const g=wu(i,f),m=n(g);for(let t=0;t<r.length;++t){const e=r[t],n=m[t];p[e]=n,W(n)}if(e===u.length-1&&b){const e=t.testLoop(h,c,a);for(let t=0;t<r.length;++t){const n=r[t],s=e[t];W(s),o["val_"+n]=s}}})),await x.onBatchEnd(e,p),dl(p),t.stopTraining_)break}l.dispose()}if(await x.onEpochEnd(l,o),t.stopTraining_)break}return await x.onTrainEnd(),await t.history.syncData(),t.history}(t,k,w,x,f,r.epochs,r.verbose,I,v,y,r.shuffle,S,r.initialEpoch,null,null)}finally{t.isTraining=!1,Su(a,n),Su(o,i),Su(h,l),Su(c,u),null!=d&&B(d)}}function vu(t){const e=[];t instanceof ot&&(t=[t]);for(let n=0;n<t.length;++n){const s=t[n];if(1===s.rank)e.push(ao(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");e.push(s)}}return e}function Su(t,e){if(null==t)return;const n=[];if(e instanceof ot)n.push(e.id);else if(Array.isArray(e))e.forEach((t=>n.push(t.id)));else if(null!=e)for(const t in e){const s=e[t];n.push(s.id)}const s=[];if(t instanceof ot)-1===n.indexOf(t.id)&&s.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===n.indexOf(t.id)&&s.push(t)}));else if(null!=t)for(const e in t){const i=t[e];-1===n.indexOf(i.id)&&s.push(i)}s.forEach((t=>{t.isDisposed||t.dispose()}))}function Iu(t){return Array.isArray(t)}function Nu(t){return!function(t){return t instanceof ot}(t)&&!Iu(t)}function Au(t,e,n,s=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(Iu(t)&&t.length>0)e=!0;else if(Nu(t)){for(const n in t)if(t.hasOwnProperty(n)){e=!0;break}}else e=!0;if(e)throw new aa(`Error when checking model ${i} expected no data, but got ${t}`)}return[]}if(null==t)return e.map((t=>null));let r;if(Nu(t)){t=t,r=[];for(const n of e){if(null==t[n])throw new aa(`No data provided for "${n}". Need data for each key in: ${e}`);r.push(t[n])}}else if(Iu(t)){if((t=t).length!==e.length)throw new aa(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): ${t}`);r=t}else{if(t=t,e.length>1)throw new aa(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${t.shape}`);r=[t]}if(r=vu(r),null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new aa(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s). but got array with shape ${a.shape}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l>=0&&o!==l)throw new aa(`Error when checking ${i}: expected ${e[t]} to have shape [${n[t]}], but got array with shape [${a.shape}].`)}}return r}function Cu(t,e,n,s=!0,i=""){let r;if(Array.isArray(t)){if(t.length!==e.length)throw new aa(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);r=t}else{if(e.length>1)throw new aa(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(t.shape)}.`);r=[t]}if(null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new aa(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s), but got array with shape ${JSON.stringify(a.shape)}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l!==o)throw new aa(`Error when checking ${i}: expected ${e[t]} to have shape ${JSON.stringify(n[t])} but got array with shape ${JSON.stringify(a.shape)}.`)}}}class zu extends uu{constructor(t){super(t),this.isTraining=!1}summary(t,e,n=console.log){if(!this.built)throw new aa("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");Hl(this,t,e,n)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=function(t){const e={Adagrad:()=>it.adagrad(.01),Adadelta:()=>it.adadelta(1,.95,sa()),Adam:()=>it.adam(.001,.9,.999,sa()),Adamax:()=>it.adamax(.002,.9,.999,sa(),0),RMSProp:()=>it.rmsprop(.001,.9,0,sa()),SGD:()=>it.sgd(.01)};if(e.adagrad=e.Adagrad,e.adadelta=e.Adadelta,e.adam=e.Adam,e.adamax=e.Adamax,e.rmsprop=e.RMSProp,e.sgd=e.SGD,t in e)return e[t]();throw new aa(`Unknown Optimizer ${t}`)}(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof lt))throw new aa("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let e=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new aa(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const n=t.loss;e=n.map((t=>Fl(t)))}else{const n=Fl(t.loss);this.outputs.forEach((t=>{e.push(n)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new aa(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: ${this.outputNames}`);for(const n of this.outputNames)null==t.loss[n]&&console.warn(`Output "${n}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${n} during training`),e.push(Fl(t.loss[n]))}this.lossFunctions=e,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let t=0;t<this.outputs.length;++t){const e=this.internalOutputShapes[t],n=this.outputNames[t];this.feedOutputNames.push(n),this.feedOutputShapes.push(e),this.feedLossFns.push(this.lossFunctions[t])}const n=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],Ja("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const s=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let n;if("string"==typeof t||"function"==typeof t)n=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${t}`);n=t}if(Array.isArray(n))return e.map((t=>n));{const t=[];for(const s of e){let e=n.hasOwnProperty(s)?n[s]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),i=(t,e,n)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([n,t])};Ja("metric",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;(e=>{let n,s,r;for(const a of e){if("string"==typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let i;1===e[e.length-1]||this.lossFunctions[t]===$l?-1!==["accuracy","acc"].indexOf(a)?s=Ll:-1!==["crossentropy","ce"].indexOf(a)&&(s=Bl):this.lossFunctions[t]===Dl?-1!==["accuracy","acc"].indexOf(a)?s=Pl:-1!==["crossentropy","ce"].indexOf(a)&&(s=Ul):-1!==["accuracy","acc"].indexOf(a)?s=_l:-1!==["crossentropy","ce"].indexOf(a)&&(s=Wl),-1!==["accuracy","acc"].indexOf(a)?i="acc":-1!==["crossentropy","ce"].indexOf(a)&&(i="ce"),r=s,n=""+i}else{const t=jl(a);r=t,n=""+Vl(a)}let e;Ja(n,(()=>{e=r})),i(t,n,e)}})(s[t])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,n={}){const s=null==n.batchSize?32:n.batchSize;yu(s);const i=this.standardizeUserDataXY(t,e,!0,s);try{const r=i[0].concat(i[1]);this.makeTestFunction();const a=this.testFunction;return pa(this.testLoop(a,r,s,n.verbose,n.steps))}finally{Su(i[0],t),Su(i[1],e)}}async evaluateDataset(t,n){return this.makeTestFunction(),async function(t,n,i){const r=null!=(i=i||{}).batches,o=t.testFunction;let h=[];if(i.verbose>0)throw new oa("Verbose mode is not implemented yet.");e.assert(!r||i.batches>0&&Number.isInteger(i.batches),(()=>`Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(i.batches)}`));const c="function"==typeof n.next?n:await n.iterator();let p=0,d=0;for(;!r||d<i.batches;){const e=await c.next();if(h=s((()=>{if(e.value){const{xs:n,ys:i}=du(t,e.value),r=n.concat(i),l=s((()=>o(r)));if(B(r),0===d)for(let t=0;t<l.length;++t)h.push(F(0));const c=r[0].shape[0];for(let t=0;t<l.length;++t){const e=l[t],n=h[t];h[t]=s((()=>u(h[t],a(c,e)))),d>0&&B(n)}B(l),p+=c,++d}return h})),e.done){r&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${i.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let t=0;t<h.length;++t){const e=h[t];h[t]=l(h[t],p),B(e)}return pa(h)}(this,t,n)}checkNumSamples(t,e,n,s="steps"){let i;if(null!=n){if(i=null,null!=e)throw new aa(`If ${s} is set, batchSize must be null or undefined.Got batchSize = ${e}`)}else{if(null==t)throw new aa(`Either the input data should have a defined shape, or ${s} shoud be specified.`);i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,e){if(Array.isArray(e)&&0===e.length)throw new aa("`outputs` is an empty Array, which is not allowed.");const n=Array.isArray(e),s=n?e:[e],i=this.retrieveSymbolicTensors(s),r=new nu;if(t instanceof ot&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new aa(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)r.add(this.inputs[e],t[e])}else for(const e of this.inputs){const n=t[e.name];if(null==n)throw new aa(`No value is provided for the model's input ${e.name}`);r.add(e,n)}const a=ru(i,r);return n?a:a[0]}retrieveSymbolicTensors(t){const e=ua(null,t.length);let n=t.length;for(const s of this.layers){const i=Array.isArray(s.output)?s.output:[s.output],r=i.map((t=>t.name));for(let s=0;s<t.length;++s){const a=r.indexOf(t[s]);if(-1!==a&&(e[s]=i[a],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw e.forEach(((e,s)=>{null==e&&n.push(t[s])})),new aa(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(n)}`)}return e}predictLoop(t,e=32,n=!1){return s((()=>{const i=this.checkNumSamples(t);if(n)throw new oa("Verbose predictLoop() is not implemented yet.");const r=ku(i,e),a=this.outputs.map((t=>[]));for(let e=0;e<r.length;++e){s((()=>{const n=r[e][0],s=r[e][1],i=bu(t,n,s),a=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)a.push({key:this.inputs[t],value:i[t]});else a.push({key:this.inputs[0],value:i});const o=new nu(a);return ru(this.outputs,o)})).forEach(((t,e)=>a[e].push(t)))}return pa(a.map((t=>N(t,0))))}))}predict(t,e={}){const n=vu(t);Cu(n,this.inputNames,this.feedInputShapes,!1);try{const s=null==e.batchSize?32:e.batchSize;return yu(s),this.predictLoop(n,s)}finally{Su(n,t)}}predictOnBatch(t){Cu(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,n,s=!0,i){if(null==this.optimizer_)throw new ra("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const r=[];for(let t=0;t<this.feedOutputShapes.length;++t){const e=this.feedOutputShapes[t];this.feedLossFns[t]===Dl?r.push(e.slice(0,e.length-1).concat([1])):r.push(e)}if(function(t,n,s){const i=xa(t.map((t=>t.shape[0])));i.sort();const r=xa(n.map((t=>t.shape[0])));if(r.sort(),i.length>1)throw new aa(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((t=>t.shape)))}`);if(r.length>1)throw new aa(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(n.map((t=>t.shape)))}`);if(i.length>0&&r.length>0&&!e.arraysEqual(i,r))throw new aa(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${r[0]} target sample(s).`)}(t=Au(t,this.feedInputNames,this.feedInputShapes,!1,"input"),n=Au(n,this.feedOutputNames,r,!1,"target")),function(t,e,n){const s=[Nl,$l,zl];for(let i=0;i<t.length;++i){const r=t[i],a=e[i],o=n[i];if(null!=a){if(a===zl&&1===r.shape[r.shape.length-1])throw new aa(`You are passing a target array of shape ${r.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==s.indexOf(a)){const t=r.shape.slice(1),e=o.slice(1);for(let n=0;n<t.length;++n){const s=t[n],i=e[n];if(null!=i&&s!==i)throw new aa(`A target Tensor with shape ${r.shape} was passed for an output of shape ${o}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(n,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new aa(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,n]}async standardizeUserData(t,e,n,s,i=!0,r){const[a,o]=this.standardizeUserDataXY(t,e,i,r);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const t=hu(s,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await cu(o[e],null,t[e]))}return[a,o,l]}testLoop(t,e,n,i=0,r){return s((()=>{const s=this.checkNumSamples(e,n,r,"steps"),o=[];if(i>0)throw new oa("Verbose mode is not implemented yet.");if(null!=r)throw new oa("steps mode in testLoop() is not implemented yet");{const i=ku(s,n),r=p(io(0,s));for(let n=0;n<i.length;++n){const s=i[n][0],l=i[n][1],h=oo(r,s,l-s),c=wu(e,h),p=t(c);if(0===n)for(let t=0;t<p.length;++t)o.push(F(0));for(let t=0;t<p.length;++t){const e=p[t];o[t]=u(o[t],a(l-s,e))}}for(let t=0;t<o.length;++t)o[t]=l(o[t],s)}return o}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let n=0;n<t.length;++n){const s=t[n];let i=s;if(ca(t,s)>1){i+=`_${ca(t.slice(0,n),s)}`}e.push(i)}return e}makeTrainFunction(){return t=>{const e=[],n=t.slice(0,this.inputs.length),s=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),i=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),r=[],a=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:n[e]});const a=new nu(t),o=ru(this.outputs,a,{training:!0});let l;for(let t=0;t<this.lossFunctions.length;++t){let n=(0,this.lossFunctions[t])(s[t],o[t]);null!=i[t]&&(n=pu(n,i[t]));const r=j(n);e.push(r),l=0===t?n:u(l,n)}for(let t=0;t<this.metricsTensors.length;++t){let n;if(this.outputs.length>1&&t<this.outputs.length)n=e[t];else{const e=this.metricsTensors[t][0],i=this.metricsTensors[t][1];n=j(e(s[i],o[i]))}W(n),r.push(n)}return l=j(l),this.calculateLosses().forEach((t=>{l=u(l,t)})),l}),!0,a)].concat(r)}}makeTestFunction(){this.testFunction=t=>s((()=>{const e=[];let n;const s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=[];for(let t=0;t<this.inputs.length;++t)r.push({key:this.inputs[t],value:s[t]});const a=new nu(r),o=ru(this.outputs,a);for(let t=0;t<this.lossFunctions.length;++t){const s=this.lossFunctions[t],r=j(s(i[t],o[t]));n=0===t?r:u(n,r),e.push(n)}for(let t=0;t<this.metricsTensors.length;++t){const n=this.metricsTensors[t][0],s=this.metricsTensors[t][1],r=j(n(i[s],o[s]));e.push(r)}return e}))}async fit(t,e,n={}){return xu(this,t,e,n)}async fitDataset(t,e){return gu(this,t,e)}async trainOnBatch(t,e){const n=await this.standardizeUserData(t,e),s=n[0],i=n[1],r=this.makeTrainFunction()(s.concat(i)),a=[];for(const t of r){const e=await t.data();a.push(e[0])}return B(r),pa(a)}getNamedWeights(t){const e=[],n=null!=t&&t.trainableOnly,s=n?this.trainableWeights:this.weights,i=this.getWeights(n);for(let t=0;t<s.length;++t)n&&!s[t].trainable||e.push({name:s[t].originalName,tensor:i[t]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const e=rt().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=e-rt().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=fa(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>fa(t)))}else{const e=Object.keys(this.loss);t={};const n=this.loss;for(const s of e){if("string"!=typeof n[s])throw new Error("Serialization of non-string loss is not supported.");t[s]=fa(n[s])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[fa(Vl(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>fa(Vl(t))));{const t={};for(const e in this.metrics)t[e]=fa(Vl(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=Sl(Ql(t.optimizer_config));let n,s;if("string"==typeof t.loss)n=ga(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>ga(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=ga(t.loss[e])}if(Array.isArray(t.metrics))s=t.metrics.map((t=>ga(t)));else if(null!=t.metrics){s={};for(const e in t.metrics)s[e]=ga(t.metrics[e])}this.compile({loss:n,metrics:s,optimizer:e})}async save(t,e){if("string"==typeof t){const e=ut.getSaveHandlers(t);if(0===e.length)throw new aa(`Cannot find any save handlers for URL '${t}'`);if(e.length>1)throw new aa(`Found more than one (${e.length}) save handlers for URL '${t}'`);t=e[0]}if(null==t.save)throw new aa("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const n=await ut.encodeWeights(this.getNamedWeights(e)),s={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v3.4.0",convertedBy:null};if(null!=e&&e.includeOptimizer&&null!=this.optimizer){s.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:e,specs:i}=await ut.encodeWeights(await this.optimizer.getWeights(),t);n.specs.push(...i),n.data=ut.concatenateArrayBuffers([n.data,e])}if(null!=this.userDefinedMetadata){const t=!0;ql(this.userDefinedMetadata,this.name,t),s.userDefinedMetadata=this.userDefinedMetadata}return s.weightData=n.data,s.weightSpecs=n.specs,t.save(s)}setUserDefinedMetadata(t){ql(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}zu.className="Model",n.registerClass(zu);class Du extends zu{}async function $u(t,e){if(null==e&&(e={}),"string"==typeof t){const n=ut.getLoadHandlers(t,e);if(0===n.length)n.push(ut.browserHTTPRequest(t,e));else if(n.length>1)throw new aa(`Found more than one (${n.length}) load handlers for URL '${t}'`);t=n[0]}return async function(t,e,n){null==n&&(n={});if(null==t.load)throw new aa("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const s=await t.load();let i=s.modelTopology;null!=i.model_config&&(i=i.model_config);const r=null==n.strict||n.strict,a=null!=s.weightData&&null!=s.weightSpecs&&r,o=Sl(Ql(i),e,a),l=s.trainingConfig;null!=l&&o.loadTrainingConfig(l);null!=s.userDefinedMetadata&&o.setUserDefinedMetadata(s.userDefinedMetadata);if(null!=s.weightData){if(null==s.weightSpecs)throw new aa("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:e}=function(t,e){const n=ut.decodeWeights(t,e),s={},i=[];return e.forEach((t=>{"optimizer"===t.group?i.push({name:t.name,tensor:n[t.name]}):s[t.name]=n[t.name]})),{modelWeights:s,optimizerWeights:i}}(s.weightData,s.weightSpecs);o.loadWeights(t,r),null!=o.optimizer&&e.length>0&&await o.optimizer.setWeights(e),B(t),B(e.map((t=>t.tensor)))}return o}(t,void 0,e)}Du.className="Functional",n.registerClass(Du);class Tu extends zu{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:Ho("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some((t=>t<0)))throw new aa(`Negative dimension size caused by adding layer ${t.name} with input shape [${t.inboundNodes[0].inputTensors[0].shape}]`)}add(t){const e=t instanceof Tu||t instanceof zu;let n;if(e){if(n=t,1!==n.outputs.length)throw new aa("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new aa("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new aa("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=cl({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==t.inboundNodes.length)throw new aa(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new aa("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=ul(this.outputs[0])}this.inboundNodes=[],new al({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:ua(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if(Yo(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new zu({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,n=console.log){this.built||this.build(),super.summary(t,e,n)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,n={}){if(!this.built)throw new ra("The model needs to be compiled before being used.");return this.model.evaluate(t,e,n)}async evaluateDataset(t,e){if(!this.built)throw new ra("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,n={}){if(!this.built)throw new ra("The model needs to be compiled before being used.");return this.model.fit(t,e,n)}async fitDataset(t,e){if(!this.built)throw new ra("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,n,s={},i=!1){let r,a={};if(n instanceof Array){if(null==n[0].className||"Merge"===n[0].className)throw new aa("Legacy serialization format not supported yet.");r=n}else e.assert(null!=n.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),r=n.layers,delete n.layers,a=n;const o=new t(a);if(!(o instanceof Tu))throw new oa(`Sequential.fromConfig called on non-Sequential input: ${o}`);for(const t of r){const e=Sl(t,void 0,i);i&&e.setFastWeightInitDuringBuild(!0),o.add(e)}return o}set stopTraining(t){if(null==this.model)throw new aa("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new aa("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const n={};n.className=e.getClassName(),n.config=e.getConfig(),t.push(n)}return{name:this.name,layers:t}}}function Eu(t){return new zu(t)}function Fu(t){return new Tu(t)}function Lu(t,e){return null==e&&(e={}),$u(t,e)}function _u(t){return cl(t)}function Ru(t,e){xl.registerCallbackConstructor(t,e)}Tu.className="Sequential",n.registerClass(Tu);class Mu extends n.Serializable{getConfig(){return{}}}class Ou extends Mu{apply(t,e=1){return function(t,e=1){if(1!==e)throw new oa(`Support for alpha values other than 1 (${e}) is not implemented yet.`);return v(t)}(t,e)}}Ou.className="elu",n.registerClass(Ou);class Bu extends Mu{apply(t){return ht(t)}}Bu.className="selu",n.registerClass(Bu);class Pu extends Mu{apply(t){return h(t)}}Pu.className="relu",n.registerClass(Pu);class Wu extends Mu{apply(t){return s((()=>ct(6,h(t))))}}Wu.className="relu6",n.registerClass(Wu);class Uu extends Mu{apply(t){return t}}Uu.className="linear",n.registerClass(Uu);class Ku extends Mu{apply(t){return pt(t)}}Ku.className="sigmoid",n.registerClass(Ku);class ju extends Mu{apply(t){return function(t){return s((()=>{const e=u(.5,a(.2,t));return o(e,0,1)}))}(t)}}ju.className="hardSigmoid",n.registerClass(ju);class Vu extends Mu{apply(t){return X(t)}}Vu.className="softplus",n.registerClass(Vu);class qu extends Mu{apply(t){return function(t){return s((()=>l(t,S(t).add(1))))}(t)}}qu.className="softsign",n.registerClass(qu);class Gu extends Mu{apply(t){return dt(t)}}Gu.className="tanh",n.registerClass(Gu);class Hu extends Mu{apply(t,e=-1){return V(t,e)}}Hu.className="softmax",n.registerClass(Hu);class Ju extends Mu{apply(t,e=-1){return ft(t,e)}}Ju.className="logSoftmax",n.registerClass(Ju);class Zu extends Mu{apply(t,e=1){return s((()=>pt(t.mul(e)).mul(t)))}}function Xu(t){return t.getClassName()}function Yu(t,e={}){return wa(t,n.SerializationMap.getMap().classNameMap,e,"activation")}function Qu(t){if(null==t){const t={className:"linear",config:{}};return Yu(t)}if("string"==typeof t){const e={};return e.className=t,e.config={},Yu(e)}return t instanceof Mu?t:Yu(t)}function th(t){if(null!=t&&"object"!=typeof t)throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${t}`)}Zu.className="swish",n.registerClass(Zu);class eh extends n.Serializable{}class nh extends eh{constructor(t){super(),th(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return s((()=>{let e=T([1]);return this.hasL1&&(e=u(e,r(a(this.l1,S(t))))),this.hasL2&&(e=u(e,r(a(this.l2,yo(t))))),e.asScalar()}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}nh.className="L1L2",n.registerClass(nh);const sh={l1l2:"L1L2"};function ih(t){return ya(t)}function rh(t,e={}){return wa(t,n.SerializationMap.getMap().classNameMap,e,"regularizer")}function ah(t){if(null==t)return null;if("string"==typeof t){return rh({className:t in sh?sh[t]:t,config:{}})}return t instanceof eh?t:rh(t)}class oh extends ll{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,e){t=Xo(t);let n=h(t);return null!=this.maxValue&&(n=o(n,0,this.maxValue)),n}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}oh.className="ReLU",n.registerClass(oh);class lh extends ll{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=Xo(t);return gt(n,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}lh.className="LeakyReLU",n.registerClass(lh);class uh extends ll{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=Ko(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=ah(t.alphaRegularizer),this.alphaConstraint=Ma(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new aa(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=Yo(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)n[e]=t[e];this.inputSpec=[new sl({ndim:t.length,axes:n})],this.built=!0}call(t,e){return t=Xo(t),mt(t,this.alpha.read())}getConfig(){const t={alphaInitializer:Uo(this.alphaInitializer),alphaRegularizer:ih(this.alphaRegularizer),alphaConstraint:_a(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}uh.className="PReLU",n.registerClass(uh);class hh extends ll{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new oa(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=Xo(t);return v(n)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}hh.className="ELU",n.registerClass(hh);class ch extends ll{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const n=Xo(t);return n.mul(ro(n.greater(this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}ch.className="ThresholdedReLU",n.registerClass(ch);class ph extends ll{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new Hu).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const n=Xo(t);return this.softmax(n,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function dh(t,e,n){if("number"==typeof t)return ua(t,e);if(t.length!==e)throw new aa(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let i=0;i<e;++i){const r=t[i];if((s=r)!==parseInt(s.toString(),10))throw new aa(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number ${r}`)}return t;var s}function fh(t,e,n,s,i=1){if(null==t)return t;let r;return r="same"===n?t:t-(e+(e-1)*(i-1))+1,Math.floor((r+s-1)/s)}function gh(t,e,n,s){if(null==t)return null;if("valid"===s)t=t*e+so([n-e,0]);else{if("same"!==s)throw new aa(`Unsupport padding mode: ${s}.`);t*=e}return t}function mh(t,e){return s((()=>(Va(e),"channelsFirst"===e?yt(t,[0,2,3,1]):t)))}function yh(t,e){return s((()=>(Va(e),"channelsFirst"===e?yt(t,[0,2,3,4,1]):t)))}function bh(t,e,n,i=[1,1],r="valid",a,o,l=null){return s((()=>{if(null==a&&(a="channelsLast"),Va(a),3!==t.rank&&4!==t.rank)throw new aa(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==e.rank&&4!==e.rank)throw new aa(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let s=mh(t,a);if("causal"===r)throw new oa("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return s=A.conv2d({x:s,filter:e,strides:i,pad:"same"===r?"same":"valid",dilations:o,dataFormat:"NHWC",bias:n,activation:l}),"channelsFirst"===a&&(s=yt(s,[0,3,1,2])),s}))}ph.className="Softmax",n.registerClass(ph);class wh extends ll{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",wh.verifyArgs(e),this.rank=t,Na(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new oa(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=dh(e.kernelSize,t,"kernelSize"),this.strides=dh(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,qa(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,Va(this.dataFormat),this.activation=Qu(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=Ko(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=Ma(e.biasConstraint),this.biasRegularizer=ah(e.biasRegularizer),this.activityRegularizer=ah(e.activityRegularizer),this.dilationRate=dh(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new aa(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new aa(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new aa(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(t){if(ha("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!Ia(t.kernelSize,"number",1,3))throw new aa(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:Xu(this.activation),useBias:this.useBias,biasInitializer:Uo(this.biasInitializer),biasRegularizer:ih(this.biasRegularizer),activityRegularizer:ih(this.activityRegularizer),biasConstraint:_a(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class kh extends wh{constructor(t,e){super(t,e),this.kernel=null,kh.verifyArgs(e),this.filters=e.filters,Na(this.filters,"filters"),this.kernelInitializer=Ko(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=Ma(e.kernelConstraint),this.kernelRegularizer=ah(e.kernelRegularizer)}build(t){t=Yo(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new aa(`The channel dimension of the input should be defined. Found ${t[e]}`);const n=t[e],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:n}}],this.built=!0}call(t,e){return s((()=>{let e;t=Xo(t);const n=null==this.bias?null:this.bias.read(),i=Ca(this.activation.getClassName());if(null!=i&&2===this.rank)e=bh(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate,i);else{if(1===this.rank)e=function(t,e,n,i=1,r="valid",a,o=1){return s((()=>{if(null==a&&(a="channelsLast"),Va(a),3!==t.shape.length)throw new aa(`The input of a conv1dWithBias operation should be 3, but is ${t.shape.length} instead.`);if(3!==e.shape.length)throw new aa(`The kernel for a conv1dWithBias operation should be 3, but is ${e.shape.length} instead`);if(null!=n&&1!==n.shape.length)throw new aa(`The bias for a conv1dWithBias operation should be 1, but is ${e.shape.length} instead`);if("channelsFirst"===a&&(t=yt(t,[0,2,1])),"causal"===r)throw new oa("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let s=kt(t,e,i,"same"===r?"same":"valid","NWC",o);return null!=n&&(s=wo(s,n)),s}))}(t,this.kernel.read(),n,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)e=bh(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new oa("convolutions greater than 3D are not implemented yet.");e=function(t,e,n,i=[1,1,1],r="valid",a,o){return s((()=>{if(null==a&&(a="channelsLast"),Va(a),4!==t.rank&&5!==t.rank)throw new aa(`conv3dWithBias expects input to be of rank 4 or 5, but received ${t.rank}.`);if(4!==e.rank&&5!==e.rank)throw new aa(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${t.rank}.`);let s=yh(t,a);if("causal"===r)throw new oa("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return s=xt(s,e,i,"same"===r?"same":"valid","NDHWC",o),null!=n&&(s=wo(s,n)),"channelsFirst"===a&&(s=yt(s,[0,4,1,2,3])),s}))}(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(e=this.activation.apply(e))}return e}))}computeOutputShape(t){t=Yo(t);const e=[],n="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<n.length;++t){const s=fh(n[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(s)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:Uo(this.kernelInitializer),kernelRegularizer:ih(this.kernelRegularizer),kernelConstraint:_a(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new aa(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(t.filters)}`)}}class xh extends kh{constructor(t){super(2,t),xh.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Ia(t.kernelSize,"number",1,2))throw new aa(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}xh.className="Conv2D",n.registerClass(xh);class vh extends kh{constructor(t){super(3,t),vh.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new aa(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}vh.className="Conv3D",n.registerClass(vh);class Sh extends xh{constructor(t){if(super(t),this.inputSpec=[new sl({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new aa(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=Yo(t)).length)throw new aa("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new aa("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new sl({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,e){return s((()=>{let e=Xo(t);if(4!==e.shape.length)throw new aa(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const n=e.shape,s=n[0];let i,r;"channelsFirst"===this.dataFormat?(i=2,r=3):(i=1,r=2);const a=n[i],o=n[r],l=this.kernelSize[0],u=this.kernelSize[1],h=this.strides[0],c=this.strides[1],p=[s,gh(a,h,l,this.padding),gh(o,c,u,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=yt(e,[0,2,3,1]));let d=bt(e,this.kernel.read(),p,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(d=yt(d,[0,3,1,2])),null!=this.bias&&(d=wo(d,this.bias.read(),this.dataFormat)),null!=this.activation&&(d=this.activation.apply(d)),d}))}computeOutputShape(t){const e=(t=Yo(t)).slice();let n,s,i;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3):(n=3,s=1,i=2);const r=this.kernelSize[0],a=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return e[n]=this.filters,e[s]=gh(e[s],o,r,this.padding),e[i]=gh(e[i],l,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}Sh.className="Conv2DTranspose",n.registerClass(Sh);class Ih extends vh{constructor(t){if(super(t),this.inputSpec=[new sl({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new aa(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=Yo(t)).length)throw new aa("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new aa("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new sl({ndim:5,axes:{[e]:n}})],this.built=!0}call(t,e){return s((()=>{let e=Xo(t);if(5!==e.shape.length)throw new aa(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const n=e.shape,s=n[0];let i,r,a;"channelsFirst"===this.dataFormat?(a=2,i=3,r=4):(a=1,i=2,r=3);const o=n[a],l=n[i],u=n[r],h=this.kernelSize[0],c=this.kernelSize[1],p=this.kernelSize[2],d=this.strides[0],f=this.strides[1],g=this.strides[2],m=[s,gh(o,d,h,this.padding),gh(l,f,c,this.padding),gh(u,g,p,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=yt(e,[0,2,3,4,1]));let y=wt(e,this.kernel.read(),m,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(y=yt(y,[0,4,1,2,3])),null!==this.bias&&(y=wo(y,this.bias.read(),this.dataFormat)),null!==this.activation&&(y=this.activation.apply(y)),y}))}computeOutputShape(t){const e=(t=Yo(t)).slice();let n,s,i,r;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3,r=4):(n=4,s=1,i=2,r=3);const a=this.kernelSize[0],o=this.kernelSize[1],l=this.kernelSize[2],u=this.strides[0],h=this.strides[1],c=this.strides[2];return e[n]=this.filters,e[s]=gh(e[s],u,a,this.padding),e[i]=gh(e[i],h,o,this.padding),e[r]=gh(e[r],c,l,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}Ih.className="Conv3DTranspose",n.registerClass(Ih);class Nh extends kh{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new aa("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new aa("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new aa(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=Ko(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=ah(e.depthwiseRegularizer),this.depthwiseConstraint=Ma(e.depthwiseConstraint),this.pointwiseInitializer=Ko(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=ah(e.pointwiseRegularizer),this.pointwiseConstraint=Ma(e.pointwiseConstraint)}build(t){if((t=Yo(t)).length<this.rank+2)throw new aa(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new aa(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const n=t[e],s=this.kernelSize.concat([n,this.depthMultiplier]),i=[];for(let t=0;t<this.rank;++t)i.push(1);i.push(n*this.depthMultiplier,this.filters);const r=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,r,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",i,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,r,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,r,this.biasConstraint):this.bias=null,this.inputSpec=[new sl({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,e){return s((()=>{let e;if(t=Xo(t),1===this.rank)throw new oa("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=yt(t,[0,2,3,1])),e=vt(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(e=wo(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),"channelsFirst"===this.dataFormat&&(e=yt(e,[0,3,1,2])),e}))}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=Uo(this.depthwiseInitializer),t.pointwiseInitializer=Uo(this.pointwiseInitializer),t.depthwiseRegularizer=ih(this.depthwiseRegularizer),t.pointwiseRegularizer=ih(this.pointwiseRegularizer),t.depthwiseConstraint=_a(this.depthwiseConstraint),t.pointwiseConstraint=_a(this.pointwiseConstraint),t}}Nh.className="SeparableConv";class Ah extends Nh{constructor(t){super(2,t)}}Ah.className="SeparableConv2D",n.registerClass(Ah);class Ch extends kh{constructor(t){super(1,t),Ch.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Ia(t.kernelSize,"number",1,1))throw new aa(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}Ch.className="Conv1D",n.registerClass(Ch);class zh extends ll{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,e){return s((()=>{if(t=Xo(t),"channelsLast"===this.dataFormat){const e=uo(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return uo(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=uo(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return uo(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}zh.className="Cropping2D",n.registerClass(zh);class Dh extends ll{constructor(t){var e;super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Va(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,e=this.interpolation,Sa(Pa,"InterpolationFormat",e)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],n=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,n]}{const e=null==t[1]?null:this.size[0]*t[1],n=null==t[2]?null:this.size[1]*t[2];return[t[0],e,n,t[3]]}}call(t,e){return s((()=>{let e=Xo(t);const n=e.shape;if("channelsFirst"===this.dataFormat){e=yt(e,[0,2,3,1]);const t=this.size[0]*n[2],s=this.size[1]*n[3],i="nearest"===this.interpolation?e.resizeNearestNeighbor([t,s]):e.resizeBilinear([t,s]);return yt(i,[0,3,1,2])}{const t=this.size[0]*n[1],s=this.size[1]*n[2];return"nearest"===this.interpolation?e.resizeNearestNeighbor([t,s]):e.resizeBilinear([t,s])}}))}getConfig(){const t={size:this.size,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Dh.className="UpSampling2D",n.registerClass(Dh);class $h extends wh{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=Ko(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=Ma(t.depthwiseConstraint),this.depthwiseRegularizer=ah(t.depthwiseRegularizer)}build(t){if((t=Yo(t)).length<4)throw new aa(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new aa(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const n=t[e],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s((()=>{let e=function(t,e,n=[1,1],i="valid",r,a){return s((()=>{null==r&&(r="channelsLast"),Va(r);let s=mh(t,r);if(4!==t.rank)throw new aa(`Input for depthwiseConv2d is required to be 4-D, but is instead ${t.rank}-D`);if(4!==e.rank)throw new aa(`depthwiseKernel is required to be 4-D, but is instead ${e.rank}-D`);return s=St(s,e,n,"same"===i?"same":"valid","NHWC",a),"channelsFirst"===r&&(s=yt(s,[0,3,1,2])),s}))}(t=Xo(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(e=wo(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),e}))}computeOutputShape(t){t=Yo(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,i=fh(e,this.kernelSize[0],this.padding,this.strides[0]),r=fh(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,i,r]:[t[0],i,r,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=Uo(this.depthwiseInitializer),t.depthwiseRegularizer=ih(this.depthwiseRegularizer),t.depthwiseConstraint=_a(this.depthwiseRegularizer),t}}function Th(t,e,n,s){if(Array.isArray(t)){if(null!=e||null!=n)throw new aa("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function i(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=i(e),constants:n=i(n)}}function Eh(t,e,n,i=!1,r,a,o=!1,l=!1){return s((()=>{const u=e.shape.length;if(u<3)throw new aa(`Input should be at least 3D, but is ${u}D.`);const h=[1,0].concat(io(2,u));if(e=yt(e,h),null!=a)throw new oa("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=r.asType("bool").asType("float32")).rank===u-1&&(r=It(r,-1)),r=yt(r,h)),i&&(e=Nt(e,0),null!=r&&(r=Nt(r,0)));const c=[];let p,d=n;const f=e.shape[0],g=At(e);let m,y;null!=r&&(m=At(r));for(let e=0;e<f;++e){const n=g[e],i=s((()=>t(n,d)));if(null==r)p=i[0],d=i[1];else{const t=s((()=>{const t=m[e],n=Y(t).sub(t);return{output:i[0].mul(t).add(d[0].mul(n)),newStates:d.map(((e,s)=>i[1][s].mul(t).add(e.mul(n))))}}));p=t.output,d=t.newStates}l&&c.push(p)}if(l){y=Ct(c,1)}return[p,y,d]}))}$h.className="DepthwiseConv2D",n.registerClass($h);class Fh extends ll{constructor(t){let e;if(super(t),null==t.cell)throw new aa("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new Wh({cells:t.cell}):t.cell,null==e.stateSize)throw new aa("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new sl({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return io(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map((t=>null))}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){Jo(t)&&(t=t[0]),t=t;let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let s;if(s=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const n=[];for(const s of e)n.push([t[0],s]);return[s].concat(n)}return s}computeMask(t,e){return s((()=>{Array.isArray(e)&&(e=e[0]);const t=this.returnSequences?e:null;if(this.returnState){const e=this.states.map((t=>null));return[t].concat(e)}return t}))}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new oa("Constants support is not implemented in RNN yet.");Jo(t)&&(t=t[0]),t=t;const n=this.stateful?t[0]:null,s=t.slice(2);this.inputSpec[0]=new sl({shape:[n,null,...s]});const i=[t[0]].concat(t.slice(2));let r;if(this.cell.build(i),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!e.arraysEqual(this.stateSpec.map((t=>t.shape[t.shape.length-1])),r))throw new aa(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=r.map((t=>new sl({shape:[null,t]})));this.stateful&&this.resetStates()}resetStates(t,n=!1){s((()=>{if(!this.stateful)throw new ia("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new aa("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>T([s,t]))):this.states_=[T([s,this.cell.stateSize])];else if(null==t)B(this.states_),null!=this.keptStates&&(B(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>T([s,t]))):this.states_[0]=T([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new aa(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===n?this.keptStates.push(this.states_.slice()):B(this.states_);for(let n=0;n<this.states_.length;++n){const i=t[n],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[n]:this.cell.stateSize,a=[s,r];if(!e.arraysEqual(i.shape,a))throw new aa(`State ${n} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${i.shape}`);this.states_[n]=i}}this.states_=this.states_.map((t=>W(t.clone())))}))}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Th(t,n,s,this.numConstants);t=i.inputs,n=i.initialState,s=i.constants;let r=[],a=[];if(null!=n){e.initialState=n,r=r.concat(n),this.stateSpec=[];for(const t of n)this.stateSpec.push(new sl({shape:t.shape}));a=a.concat(this.stateSpec)}null!=s&&(e.constants=s,r=r.concat(s),this.numConstants=s.length);if(r[0]instanceof il){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return s((()=>{const n=null==e?null:e.mask,s=null==e?null:e.training;let i=null==e?null:e.initialState;t=Xo(t),null==i&&(i=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(i.length!==r)throw new aa(`RNN Layer has ${r} state(s) but was passed ${i.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:s},o=Eh(((t,e)=>{const n=this.cell.call([t].concat(e),a);return[n[0],n.slice(1)]}),t,i,this.goBackwards,n,null,this.unroll,this.returnSequences),l=o[0],u=o[1],h=o[2];this.stateful&&this.resetStates(h,s);const c=this.returnSequences?u:l;return this.returnState?[c].concat(h):c}))}getInitialState(t){return s((()=>{let e=T(t.shape);return e=r(e,[1,2]),e=ao(e),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((t=>t>1?po(e,[1,t]):e)):this.cell.stateSize>1?[po(e,[1,this.cell.stateSize])]:[e]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===Fh.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign({},n,t,e)}static fromConfig(t,e,n={}){const s=Sl(e.cell,n);return new t(Object.assign(e,{cell:s}))}}Fh.className="RNN",n.registerClass(Fh);class Lh extends ll{}class _h extends Lh{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Na(this.units,"units"),this.activation=Qu(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Ko(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Ko(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Ko(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=ah(t.kernelRegularizer),this.recurrentRegularizer=ah(t.recurrentRegularizer),this.biasRegularizer=ah(t.biasRegularizer),this.kernelConstraint=Ma(t.kernelConstraint),this.recurrentConstraint=Ma(t.recurrentConstraint),this.biasConstraint=Ma(t.biasConstraint),this.dropout=no([1,so([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=no([1,so([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=Yo(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s((()=>{if(2!==(t=t).length)throw new aa(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let n=t[1];t=t[0];const s=null!=e.training&&e.training;let i;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Uh({ones:()=>Y(t),rate:this.dropout,training:s})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Uh({ones:()=>Y(n),rate:this.recurrentDropout,training:s}));const r=this.dropoutMask,o=this.recurrentDropoutMask;i=go(null!=r?a(t,r):t,this.kernel.read()),null!=this.bias&&(i=wo(i,this.bias.read())),null!=o&&(n=a(n,o));let l=u(i,go(n,this.recurrentKernel.read()));return null!=this.activation&&(l=this.activation.apply(l)),[l,l]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Xu(this.activation),useBias:this.useBias,kernelInitializer:Uo(this.kernelInitializer),recurrentInitializer:Uo(this.recurrentInitializer),biasInitializer:Uo(this.biasInitializer),kernelRegularizer:ih(this.kernelRegularizer),recurrentRegularizer:ih(this.recurrentRegularizer),biasRegularizer:ih(this.biasRegularizer),activityRegularizer:ih(this.activityRegularizer),kernelConstraint:_a(this.kernelConstraint),recurrentConstraint:_a(this.recurrentConstraint),biasConstraint:_a(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,e)}}_h.className="SimpleRNNCell",n.registerClass(_h);class Rh extends Fh{constructor(t){t.cell=new _h(t),super(t)}call(t,e){return s((()=>{null!=this.cell.dropoutMask&&(B(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(B(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}static fromConfig(t,e){return new t(e)}}Rh.className="SimpleRNN",n.registerClass(Rh);class Mh extends Lh{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new aa("GRUCell does not support reset_after parameter set to true.");this.units=t.units,Na(this.units,"units"),this.activation=Qu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Qu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Ko(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Ko(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Ko(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=ah(t.kernelRegularizer),this.recurrentRegularizer=ah(t.recurrentRegularizer),this.biasRegularizer=ah(t.biasRegularizer),this.kernelConstraint=Ma(t.kernelConstraint),this.recurrentConstraint=Ma(t.recurrentConstraint),this.biasConstraint=Ma(t.biasConstraint),this.dropout=no([1,so([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=no([1,so([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=Yo(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s((()=>{if(2!==(t=t).length)throw new aa(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const n=null!=e.training&&e.training;let s=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Uh({ones:()=>Y(t),rate:this.dropout,training:n,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Uh({ones:()=>Y(s),rate:this.recurrentDropout,training:n,count:3}));const i=this.dropoutMask,r=this.recurrentDropoutMask;let o,l,h;0<this.dropout&&this.dropout<1&&(t=a(t,i[0]));let c=go(t,this.kernel.read());this.useBias&&(c=wo(c,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(s=a(s,r[0]));const p=this.recurrentKernel.read(),[d,f]=zt(p,[2*this.units,this.units],p.rank-1),g=go(s,d),[m,y,b]=zt(c,3,c.rank-1),[w,k]=zt(g,2,g.rank-1);o=this.recurrentActivation.apply(u(m,w)),l=this.recurrentActivation.apply(u(y,k));const x=go(a(l,s),f);h=this.activation.apply(u(b,x));const v=u(a(o,s),a(u(1,q(o)),h));return[v,v]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Xu(this.activation),recurrentActivation:Xu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Uo(this.kernelInitializer),recurrentInitializer:Uo(this.recurrentInitializer),biasInitializer:Uo(this.biasInitializer),kernelRegularizer:ih(this.kernelRegularizer),recurrentRegularizer:ih(this.recurrentRegularizer),biasRegularizer:ih(this.biasRegularizer),activityRegularizer:ih(this.activityRegularizer),kernelConstraint:_a(this.kernelConstraint),recurrentConstraint:_a(this.recurrentConstraint),biasConstraint:_a(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,e)}}Mh.className="GRUCell",n.registerClass(Mh);class Oh extends Fh{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Mh(t),super(t)}call(t,e){return s((()=>{null!=this.cell.dropoutMask&&(B(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(B(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Oh.className="GRU",n.registerClass(Oh);class Bh extends Lh{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Na(this.units,"units"),this.activation=Qu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Qu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Ko(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Ko(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Ko(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=ah(t.kernelRegularizer),this.recurrentRegularizer=ah(t.recurrentRegularizer),this.biasRegularizer=ah(t.biasRegularizer),this.kernelConstraint=Ma(t.kernelConstraint),this.recurrentConstraint=Ma(t.recurrentConstraint),this.biasConstraint=Ma(t.biasConstraint),this.dropout=no([1,so([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=no([1,so([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const n=(t=Yo(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,n=this.units;s=new((e=class extends Io{apply(e,s){const i=t.apply([n]),r=(new Ao).apply([n]),a=t.apply([2*n]);return co(co(i,r),a)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return s((()=>{const n=null!=e.training&&e.training;if(3!==(t=t).length)throw new aa(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let s=t[1];const i=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Uh({ones:()=>Y(t),rate:this.dropout,training:n,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Uh({ones:()=>Y(s),rate:this.recurrentDropout,training:n,count:4}));const r=this.dropoutMask,o=this.recurrentDropoutMask;let l,h,c,p;0<this.dropout&&this.dropout<1&&(t=a(t,r[0]));let d=go(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(s=a(s,o[0])),d=u(d,go(s,this.recurrentKernel.read())),this.useBias&&(d=wo(d,this.bias.read()));const[f,g,m,y]=zt(d,4,d.rank-1);l=this.recurrentActivation.apply(f),h=this.recurrentActivation.apply(g),c=u(a(h,i),a(l,this.activation.apply(m))),p=this.recurrentActivation.apply(y);const b=a(p,this.activation.apply(c));return[b,b,c]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Xu(this.activation),recurrentActivation:Xu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Uo(this.kernelInitializer),recurrentInitializer:Uo(this.recurrentInitializer),biasInitializer:Uo(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:ih(this.kernelRegularizer),recurrentRegularizer:ih(this.recurrentRegularizer),biasRegularizer:ih(this.biasRegularizer),activityRegularizer:ih(this.activityRegularizer),kernelConstraint:_a(this.kernelConstraint),recurrentConstraint:_a(this.recurrentConstraint),biasConstraint:_a(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,e)}}Bh.className="LSTMCell",n.registerClass(Bh);class Ph extends Fh{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Bh(t),super(t)}call(t,e){return s((()=>{null!=this.cell.dropoutMask&&(B(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(B(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Ph.className="LSTM",n.registerClass(Ph);class Wh extends Lh{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return s((()=>{let n=(t=t).slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(n.splice(0,t.stateSize.length)):s.push(n.splice(0,1));s.reverse();const i=[];let r;for(let a=0;a<this.cells.length;++a){const o=this.cells[a];n=s[a],r=0===a?[t[0]].concat(n):[r[0]].concat(n),r=o.call(r,e),i.push(r.slice(1))}n=[];for(const t of i.slice().reverse())n.push(...t);return[r[0]].concat(n)}))}build(t){let e;Jo(t)&&(t=t[0]),t=t,this.cells.forEach(((n,s)=>{Ja(`RNNCell_${s}`,(()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]}))})),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map((t=>({className:t.getClassName(),config:t.getConfig()})))};return Object.assign({},t,e)}static fromConfig(t,e,n={}){const s=[];for(const t of e.cells)s.push(Sl(t,n));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return el(t)}setWeights(t){const e=[];for(const n of this.cells){const s=n.weights.length,i=t.splice(s);for(let t=0;t<n.weights.length;++t)e.push([n.weights[t],i[t]])}nl(e)}}function Uh(t){const{ones:e,rate:n,training:s=!1,count:i=1}=t,r=()=>ko(e(),n),a=()=>xo(r,e,s);if(!i||i<=1)return W(a().clone());return Array(i).fill(void 0).map(a).map((t=>W(t.clone())))}Wh.className="StackedRNNCells",n.registerClass(Wh);class Kh extends Fh{constructor(t){if(t.unroll)throw new oa("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new oa("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new sl({ndim:5})]}call(t,e){return s((()=>{if(null!=this.cell.dropoutMask&&(B(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(B(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),e&&e.constants)throw new aa("ConvRNN2D cell does not support constants");const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})}))}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return s((()=>{const{stateSize:e}=this.cell,n=t.shape,s=this.computeSingleOutputShape(n),i=[s[0],...s.slice(2)],r=T(i);return Array.isArray(e)?Array(e.length).fill(r):[r]}))}resetStates(t,n=!1){s((()=>{if(!this.stateful)throw new ia("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)];if(null==s[0])throw new aa("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>T(r))):this.states_=[T(r)];else if(null==t)B(this.states_),null!=this.keptStates&&(B(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>T(r))):this.states_[0]=T(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new aa(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);n?this.keptStates.push(this.states_.slice()):B(this.states_);for(let n=0;n<this.states_.length;++n){const s=t[n],i=r;if(!e.arraysEqual(s.shape,i))throw new aa(`State ${n} is incompatible with layer ${this.name}: expected shape=${i}, received shape=${s.shape}`);this.states_[n]=s}}this.states_=this.states_.map((t=>W(t.clone())))}))}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:s,padding:i,strides:r,dilationRate:a}=this.cell,o="channelsFirst"===e,l=t[o?3:2],u=t[o?4:3],h=fh(l,s[0],i,r[0],a[0]),c=fh(u,s[1],i,r[1],a[1]);return[...t.slice(0,2),...o?[n,h,c]:[h,c,n]]}}Kh.className="ConvRNN2D";class jh extends Bh{constructor(t){const{filters:e,kernelSize:n,strides:s,padding:i,dataFormat:r,dilationRate:a}=t;super(Object.assign({},t,{units:e})),this.filters=e,Na(this.filters,"filters"),this.kernelSize=dh(n,2,"kernelSize"),this.kernelSize.forEach((t=>Na(t,"kernelSize"))),this.strides=dh(s||1,2,"strides"),this.strides.forEach((t=>Na(t,"strides"))),this.padding=i||"valid",qa(this.padding),this.dataFormat=r||"channelsLast",Va(this.dataFormat),this.dilationRate=dh(a||1,2,"dilationRate"),this.dilationRate.forEach((t=>Na(t,"dilationRate")))}build(t){var e;t=Yo(t);const n="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[n])throw new aa(`The channel dimension of the input should be defined. Found ${t[n]}`);const s=t[n],i=this.kernelSize.concat([s,4*this.filters]);this.kernel=this.addWeight("kernel",i,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const r=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",r,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const n=this.biasInitializer,s=this.filters;t=new((e=class extends Io{apply(t,e){return ho([n.apply([s]),E([s]),n.apply([2*s])])}}).className="CustomInit",e)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,e){return s((()=>{if(3!==t.length)throw new aa(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const n=e.training||!1,s=t[0],i=t[1],r=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Uh({ones:()=>Y(s),rate:this.dropout,training:n,count:4}));const o=this.dropoutMask,l=(t,e,n)=>e&&e[n]?a(e[n],t):t;let h=l(s,o,0),c=l(s,o,1),p=l(s,o,2),d=l(s,o,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Uh({ones:()=>Y(i),rate:this.recurrentDropout,training:n,count:4}));const f=this.recurrentDropoutMask;let g=l(i,f,0),m=l(i,f,1),y=l(i,f,2),b=l(i,f,3);const[w,k,x,v]=zt(this.kernel.read(),4,3),[S,I,N,A]=this.useBias?zt(this.bias.read(),4):[null,null,null,null];h=this.inputConv(h,w,S,this.padding),c=this.inputConv(c,k,I,this.padding),p=this.inputConv(p,x,N,this.padding),d=this.inputConv(d,v,A,this.padding);const[C,z,D,$]=zt(this.recurrentKernel.read(),4,3);g=this.recurrentConv(g,C),m=this.recurrentConv(m,z),y=this.recurrentConv(y,D),b=this.recurrentConv(b,$);const T=this.recurrentActivation.apply(u(h,g)),E=this.recurrentActivation.apply(u(c,m)),F=u(a(E,r),a(T,this.activation.apply(u(p,y)))),L=a(this.recurrentActivation.apply(u(d,b)),this.activation.apply(F));return[L,L,F]}))}getConfig(){const t=function(t,e){var n={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(n[s]=t[s]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var i=0;for(s=Object.getOwnPropertySymbols(t);i<s.length;i++)e.indexOf(s[i])<0&&Object.prototype.propertyIsEnumerable.call(t,s[i])&&(n[s[i]]=t[s[i]])}return n}(super.getConfig(),["units"]),e={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign({},t,e)}inputConv(t,e,n,s){const i=Dt(t,e,this.strides,s||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return n?wo(i,n,this.dataFormat):i}recurrentConv(t,e){return Dt(t,e,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}jh.className="ConvLSTM2DCell",n.registerClass(jh);class Vh extends Kh{constructor(t){const e=new jh(t);super(Object.assign({},t,{cell:e}))}static fromConfig(t,e){return new t(e)}}Vh.className="ConvLSTM2D",n.registerClass(Vh);class qh extends ll{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let t=0;t<this.noiseShape.length;++t)n.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return n}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Xo(t);if(0<this.rate&&this.rate<1){const t=null!=e.training&&e.training,s=this.getNoiseShape(n);return xo((()=>ko(n,this.rate,s,this.seed)),(()=>n),t)}return t}))}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}qh.className="Dropout",n.registerClass(qh);class Gh extends qh{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}Gh.className="SpatialDropout1D",n.registerClass(Gh);class Hh extends ll{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,Na(this.units,"units"),this.activation=Qu(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=Ko(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=Ko(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=Ma(t.kernelConstraint),this.biasConstraint=Ma(t.biasConstraint),this.kernelRegularizer=ah(t.kernelRegularizer),this.biasRegularizer=ah(t.biasRegularizer),this.activityRegularizer=ah(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=Yo(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=Yo(t)).slice();return e[e.length-1]=this.units,e}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Xo(t),s=Ca(this.activation.getClassName());let i;return null!=s?i=go(n,this.kernel.read(),s,this.bias?this.bias.read():null):(i=go(n,this.kernel.read()),null!=this.bias&&(i=wo(i,this.bias.read())),null!=this.activation&&(i=this.activation.apply(i))),i}))}getConfig(){const t={units:this.units,activation:Xu(this.activation),useBias:this.useBias,kernelInitializer:Uo(this.kernelInitializer),biasInitializer:Uo(this.biasInitializer),kernelRegularizer:ih(this.kernelRegularizer),biasRegularizer:ih(this.biasRegularizer),activityRegularizer:ih(this.activityRegularizer),kernelConstraint:_a(this.kernelConstraint),biasConstraint:_a(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}Hh.className="Dense",n.registerClass(Hh);class Jh extends ll{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=Yo(t);for(const e of t.slice(1))if(null==e)throw new aa(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],to(t,1)]}call(t,e){return s((()=>{this.invokeCallHook(t,e);let n=Xo(t);if("channelsFirst"===this.dataFormat&&n.rank>1){const t=[0];for(let e=2;e<n.rank;++e)t.push(e);t.push(1),n=n.transpose(t)}return function(t){if(t.rank<=1)throw new aa(`batchFlatten requires a minimum rank of 2. Got rank: ${t.rank}.`);const e=[t.shape[0],to(t.shape,1)];return t.reshape(e)}(n)}))}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}Jh.className="Flatten",n.registerClass(Jh);class Zh extends ll{constructor(t){super(t),this.supportsMasking=!0,this.activation=Qu(t.activation)}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Xo(t);return this.activation.apply(n)}))}getConfig(){const t={activation:Xu(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}Zh.className="Activation",n.registerClass(Zh);class Xh extends ll{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,e){return s((()=>{return t=Xo(t),e=t,n=this.n,s((()=>{if(2!==e.shape.length)throw new aa(`repeat() expects a rank-2 tensor, but received a rank-${e.shape.length} tensor.`);return po(ao(e,1),[1,n,1])}));var e,n}))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}Xh.className="RepeatVector",n.registerClass(Xh);class Yh extends ll{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",s=e.slice();let i=1,r=null;for(let t=0;t<s.length;++t){const e=s[t];if(this.isUnknown(e)){if(null!==r)throw new aa("Can only specifiy one unknown dimension.");r=t}else i*=e}const a=to(t);if(null!==r){if(0===i||a%i!=0)throw new aa(n);s[r]=a/i}else if(a!==i)throw new aa(n);return s}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Xo(t),s=n.shape,i=s.slice(0,1).concat(this.fixUnknownDimension(s.slice(1),this.targetShape));return n.reshape(i)}))}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}Yh.className="Reshape",n.registerClass(Yh);class Qh extends ll{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const n=io(1,t.dims.length+1);if(!e.arraysEqual(t.dims.slice().sort(),n))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new sl({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=Yo(t)).slice();return this.dims.forEach(((n,s)=>{e[s+1]=t[n]})),e}call(t,e){return yt(Xo(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}Qh.className="Permute",n.registerClass(Qh);class tc extends ll{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,e){const n=Xo(t);return $t(Tt(n,this.maskValue),-1)}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Xo(t),s=$t(Tt(n,this.maskValue),-1,!0);return n.mul(s.asType(n.dtype))}))}}tc.className="Masking",n.registerClass(tc);class ec extends ll{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(da(t.inputLength))}this.inputDim=t.inputDim,Na(this.inputDim,"inputDim"),this.outputDim=t.outputDim,Na(this.outputDim,"outputDim"),this.embeddingsInitializer=Ko(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=ah(t.embeddingsRegularizer),this.activityRegularizer=ah(t.activityRegularizer),this.embeddingsConstraint=Ma(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,e){return s((()=>this.maskZero?(t=Xo(t),Tt(t,Et(t))):null))}computeOutputShape(t){if(t=Yo(t),null==this.inputLength)return[...t,this.outputDim];const e=da(this.inputLength);if(e.length!==t.length-1)throw new aa(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let n=0;for(let s=0;s<e.length;++s){const i=e[s],r=t[s+1];if(null!=i&&null!=r&&i!==r)throw new aa(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==i&&(e[n]=r),n++}}return[t[0],...e,this.outputDim]}call(t,e){return s((()=>{this.invokeCallHook(t,e);let n=Xo(t);"int32"!==n.dtype&&(n=ro(n,"int32"));return mo(this.embeddings.read(),n.as1D()).reshape(Yo(this.computeOutputShape(n.shape)))}))}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:Uo(this.embeddingsInitializer),embeddingsRegularizer:ih(this.embeddingsRegularizer),activityRegularizer:ih(this.activityRegularizer),embeddingsConstraint:_a(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}ec.className="Embedding",n.registerClass(ec);class nc extends ll{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new oa}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const n=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const i=t[t.length-e.length+s],r=e[s];if(null==i||null==r||i<0||r<0)n.push(null);else if(1===i)n.push(r);else if(1===r)n.push(i);else{if(i!==r)throw new aa("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));n.push(i)}}return n}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[Yo(t)]),(t=t).length<2)throw new aa(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const n of t)null!=n&&null!==n[0]&&e.push(n[0]);if(e=xa(e),e.length>1)throw new aa(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let n=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const s=null==t[e]?null:t[e].slice(1);n=this.computeElementwiseOpOutputShape(n,s)}const s=t.map((t=>t.length));-1===t.indexOf(null)&&1===xa(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,e){return s((()=>{if(t=t,this.reshapeRequired){const e=[],n=t.map((t=>t.rank));if(-1===n.indexOf(null)){const s=so(n);for(let n of t){const t=n.rank;for(let e=0;e<s-t;++e)n=ao(n,1);e.push(n)}return this.mergeFunction(e)}{let n=!1;for(const s of t){const t=s.rank;if(null==t){const t=s.shape,i=t[0],r=t.slice(1).concat([i]);let a=s.reshape([i].concat(to(t.slice(1))));a=yt(a,[1,0]),a=a.reshape(r),e.push(a),n=!0}else if(t>1){const i=io(1,t).concat([0]);e.push(yt(s,i)),n=!0}else e.push(s)}let s=this.mergeFunction(e);const i=s.rank;if(n)if(null==i){const t=s.shape,e=t[t.length-1],n=[e].concat(t.slice(0,t.length-1));s=yt(s.reshape([-1,e]),[1,0]).reshape(n)}else if(i>1){const t=[i-1].concat(io(0,i-1));s=yt(s,t)}return s}}return this.mergeFunction(t)}))}computeOutputShape(t){let e;e=null==(t=t)[0]?null:t[0].slice(1);for(let n=1;n<t.length;++n){const s=null==t[n]?null:t[n].slice(1);e=this.computeElementwiseOpOutputShape(e,s)}let n=[];for(const e of t)null!=e&&null!==e[0]&&n.push(e[0]);return n=xa(n),e=1===n.length?n.concat(e):[null].concat(e),e}computeMask(t,e){return s((()=>{if(null==e)return null;if(!Array.isArray(e))throw new aa("`mask` should be an Array");if(!Array.isArray(t))throw new aa("`inputs` should be an Array");if(e.length!==t.length)throw new aa(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${e.length})`);if(e.every((t=>null==t)))return null;let n=(e=e.map((t=>null==t?t:It(t,0))))[0];for(let t=1;t<e.length-1;++t)n=nt(n,e[t]);return n}))}}class sc extends nc{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=u(e,t[n]);return e}))}}sc.className="Add",n.registerClass(sc);class ic extends nc{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=a(e,t[n]);return e}))}}ic.className="Multiply",n.registerClass(ic);class rc extends nc{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=u(e,t[n]);return a(1/t.length,e)}))}}rc.className="Average",n.registerClass(rc);class ac extends nc{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0];for(let n=1;n<t.length;++n)e=H(e,t[n]);return e}))}}ac.className="Maximum",n.registerClass(ac);class oc extends nc{constructor(t){super(t)}mergeFunction(t){return s((()=>{let e=t[0];for(let n=1;n<t.length;++n)e=ct(e,t[n]);return e}))}}oc.className="Minimum",n.registerClass(oc);class lc extends nc{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new aa("A `Concatenate` layer should be called on a list of at least 2 inputs");t=t;let n=!0;for(const e of t)if(null!=e){n=!1;break}if(n)return;const s=[];for(let n=0;n<t.length;++n){const i=t[n].slice();i.splice(this.axis,1);let r=!1;for(const t of s)if(e.arraysEqual(t,i)){r=!0;break}r||s.push(i)}if(s.length>1)throw new aa("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return s((()=>ho(t,this.axis)))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new aa("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==n[s]||null==t[s]){n[s]=null;break}n[s]+=t[s]}return n}computeMask(t,e){if(null==e)return null;if(!Array.isArray(e))throw new aa("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new aa("`inputs` should be an array for Concatenate");if(e.length!==t.length)throw new aa(`Mismatch in the length of mask (${e.length}) and the legnth of inputs (${t.length})`);return s((()=>{let n=!0;if(e.forEach((t=>{null==t||(n=!1)})),n)return null;const s=[];for(let n=0;n<t.length;++n)null==e[n]?s.push(Y(t[n]).asType("bool")):e[n].rank<t[n].rank?s.push(It(e[n],-1)):s.push(e[n]);const i=N(s,this.axis);return Ft(i,-1,!1)}))}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function uc(t,e){for(;t<0;)t+=e;return t}lc.className="Concatenate",n.registerClass(lc);class hc extends nc{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){e.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0],s=t[1];if(n.length>3||s.length>3)throw new oa("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);if(n[i[0]]!==s[i[1]])throw new aa(`Dimension incompatibility: ${n[i[0]]} !== ${s[i[1]]}`)}mergeFunction(t){if(2!==t.length)throw new aa(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let n,i=t[0],r=t[1];return n=Array.isArray(this.axes)?this.axes.map(((e,n)=>uc(e,t[n].shape.length))):[uc(this.axes,i.shape.length),uc(this.axes,r.shape.length)],this.normalize&&(i=Il(i,n[0]),r=Il(r,n[1])),function(t,n,i){if(t.shape.length>3||n.shape.length>3)throw new oa("batchDot is not implemented for tensors of 4D or higher rank yet");if(e.assert(t.shape.length>=2,(()=>`batchDot requires the rank of x to be >= 2, but got ${t.shape.length}`)),e.assert(t.shape.length>=2,(()=>`batchDot requires the rank of y to be >= 2, but got ${n.shape.length}`)),"number"==typeof i&&(i=[i,i]),"complex64"===t.dtype||"complex64"===n.dtype)throw new oa("batchDot is not implemented for complex64-type Tensors yet.");const r=t.shape.length,a=n.shape.length;null==i&&(i=[r-1,a-2]);const o=i;return s((()=>{let e,s;if(r>a){e=r-a;const t=[];for(let n=0;n<e;++n)t.push(1);n=n.reshape(n.shape.concat(t))}else if(a>r){e=a-r;const n=[];for(let t=0;t<e;++t)n.push(1);t=t.reshape(t.shape.concat(n))}else e=0;if(2===t.shape.length&&2===n.shape.length)s=o[0]===o[1]?t.mul(n).sum(o[0]):t.transpose([1,0]).mul(n).sum(o[1]);else{const e=o[0]!==t.shape.length-1,i=o[1]===n.shape.length-1;s=t.matMul(n,e,i)}if(e>0){let t;t=r>a?r+a-3:r-1;const n=[];for(let s=t;s<t+e;++s)n.push(s);s=s.squeeze(n)}return 1===s.shape.length&&(s=s.expandDims(1)),s}))}(i,r,n)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[uc(this.axes,t.length),uc(this.axes,e.length)],n}computeOutputShape(t){e.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0].slice(),s=t[1].slice();if(n.length>3||s.length>3)throw new oa("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);n.splice(i[0],1),s.splice(i[1],1),s.splice(0,1);const r=n.concat(s);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}hc.className="Dot",n.registerClass(hc);class cc extends ll{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Xo(t);return xo((()=>fo(n.shape,0,this.stddev).add(n)),(()=>n),e.training||!1)}))}}cc.className="GaussianNoise",n.registerClass(cc);class pc extends ll{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return s((()=>{this.invokeCallHook(t,e);const n=Xo(t);if(this.rate>0&&this.rate<1){return xo((()=>{const t=Math.sqrt(this.rate/(1-this.rate));return n.mul(fo(n.shape,1,t))}),(()=>n),e.training||!1)}return n}))}}pc.className="GaussianDropout",n.registerClass(pc);class dc extends ll{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||Xo(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return s((()=>{if(this.rate<1&&this.rate>0){const n=this._getNoiseShape(t);return xo((()=>{const e=Xo(t),s=-1.7580993408473766;let i=Lt(L(n),this.rate);i=ro(i,"float32");const r=((1-this.rate)*(1+this.rate*s**2))**-.5,a=-r*s*this.rate;return e.mul(i).add(i.add(-1).mul(s)).mul(r).add(a)}),(()=>Xo(t)),e.training||!1)}return t}))}}function fc(t,e,n,s,i,r=.001){let a;if(2===t.rank)a=Rt(t,e,n,s,i,r);else if(3===t.rank)a=Mt(t,e,n,s,i,r);else{if(4!==t.rank)throw new oa(`batchNormalization is not implemented for array of rank ${t.rank} yet`);a=Ot(t,e,n,s,i,r)}return a}function gc(t,n,i,r,a=.001){return e.arraysEqual(r.slice().sort(),io(0,t.rank-1))?function(t,e,n,i,r=.001){return s((()=>{const s=_t(t,i),a=s.mean,o=s.variance;return[fc(t,a,o,n,e,r),a,o]}))}(t,n,i,r,a):function(t,e,n,i,r=.001){return s((()=>{const s=_t(t,i),a=s.mean,o=s.variance,l=[];for(const e of io(0,t.rank))-1!==i.indexOf(e)?l.push(1):l.push(t.shape[e]);const u=a.reshape(l),h=o.reshape(l),c=null==e?null:e.reshape(l),p=null==n?null:n.reshape(l);return[fc(t,u,h,p,c,r),a,o]}))}(t,n,i,r,a)}dc.className="AlphaDropout",n.registerClass(dc);class mc extends ll{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=Ko(t.betaInitializer||"zeros"),this.gammaInitializer=Ko(t.gammaInitializer||"ones"),this.movingMeanInitializer=Ko(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=Ko(t.movingVarianceInitializer||"ones"),this.betaConstraint=Ma(t.betaConstraint),this.gammaConstraint=Ma(t.gammaConstraint),this.betaRegularizer=ah(t.betaRegularizer),this.gammaRegularizer=ah(t.gammaRegularizer)}build(t){t=Yo(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new aa(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new sl({ndim:t.length,axes:{[e]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,n){return s((()=>{const i=null!=n.training&&n.training,r=Xo(t),a=r.shape,o=a.length,l=io(0,o),u=this.axis>=0?this.axis:this.axis+o;l.splice(u,1);const h=ua(1,o);h[u]=a[u];const c=l.slice();c.sort();const p=!e.arraysEqual(c,io(0,o).slice(0,o-1));if(!i)return(()=>{if(p){const t=this.movingMean.read().reshape(h),e=this.movingVariance.read().reshape(h),n=this.center?this.beta.read().reshape(h):null,s=this.scale?this.gamma.read().reshape(h):null;return fc(r,t,e,n,s,this.epsilon)}return fc(r,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[d,f,g]=gc(r,this.gamma.read(),this.beta.read(),l,this.epsilon),m=(t,e,n)=>{s((()=>{const s=1-n,i=t.read(),r=i.sub(e).mul(s);t.write(i.sub(r))}))};return(()=>{m(this.movingMean,f,this.momentum),m(this.movingVariance,g,this.momentum)})(),d}))}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Uo(this.betaInitializer),gammaInitializer:Uo(this.gammaInitializer),movingMeanInitializer:Uo(this.movingMeanInitializer),movingVarianceInitializer:Uo(this.movingVarianceInitializer),betaRegularizer:ih(this.betaRegularizer),gammaRegularizer:ih(this.gammaRegularizer),betaConstraint:_a(this.betaConstraint),gammaConstraint:_a(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}mc.className="BatchNormalization",n.registerClass(mc);class yc extends ll{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const t of this.axis)if(!Number.isInteger(t))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=Ko(t.betaInitializer||"zeros"),this.gammaInitializer=Ko(t.gammaInitializer||"ones"),this.betaRegularizer=ah(t.betaRegularizer),this.gammaRegularizer=ah(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=Yo(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error(`Invalid axis: ${t}`);if(this.axis.length!==xa(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const n=this.axis.map((e=>t[e]));this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,true):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,true):this.beta=null,this.built=!0}call(t,e){const n=Xo(t),i=n.shape,r=i.length;return s((()=>{let{mean:t,variance:e}=_t(n,this.axis,!0);const s=ua(1,r);for(const t of this.axis)s[t]=i[t];const a=t=>null!=t&&t.shape.length!==r&&this.axis!==[r-1]?t.reshape(s):t;let o=a(this.gamma.read()),l=a(this.beta.read());const u=[],h=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(u.push(i[t]),h.push(1)):(u.push(1),h.push(i[t]));return t=t.tile(u),e=e.tile(u),o=o.tile(h),l=l.tile(h),fc(n,t,e,l,o,this.epsilon)}))}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Uo(this.betaInitializer),gammaInitializer:Uo(this.gammaInitializer),betaRegularizer:ih(this.betaRegularizer),gammaRegularizer:ih(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}yc.className="LayerNormalization",n.registerClass(yc);class bc extends ll{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new aa(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,n;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new aa(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new aa(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new sl({ndim:4})]}computeOutputShape(t){let e,n;return t=Yo(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,e){return s((()=>{return e=Xo(t),n=this.padding,i=this.dataFormat,s((()=>{if(4!==e.rank)throw new aa(`temporalPadding expects input tensor to be 4-D, but received a ${e.rank}-D tensor.`);if(null==n&&(n=[[1,1],[1,1]]),2!==n.length||2!==n[0].length||2!==n[1].length)throw new aa("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==i&&(i="channelsLast"),"channelsLast"!==i&&"channelsFirst"!==i)throw new aa(`Unknown data format: ${i}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===i?[[0,0],[0,0],n[0],n[1]]:[[0,0],n[0],n[1],[0,0]],Bt(e,t)}));var e,n,i}))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function wc(t,e,n,i,r,a){return s((()=>{let s;Va(r),Ga(a),qa(i),null==n&&(n=[1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=mh(t,r);const o="same"===i?"same":"valid";return s="max"===a?Pt(t,e,n,o):Wt(t,e,n,o),"channelsFirst"===r&&(s=yt(s,[0,3,1,2])),s}))}function kc(t,e,n,i,r,a){return s((()=>{let s;Va(r),Ga(a),qa(i),null==n&&(n=[1,1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=yh(t,r);const o="same"===i?"same":"valid";return s="max"===a?Ut(t,e,n,o):Kt(t,e,n,o),"channelsFirst"===r&&(s=yt(s,[0,4,1,2,3])),s}))}bc.className="ZeroPadding2D",n.registerClass(bc);class xc extends ll{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new aa(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.poolSize)}`);this.poolSize=t.poolSize}if(Na(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new aa(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.strides)}`);this.strides=t.strides}Na(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,qa(this.padding),this.inputSpec=[new sl({ndim:3})]}computeOutputShape(t){const e=fh((t=Yo(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,e){return s((()=>{this.invokeCallHook(t,e),t=ao(Xo(t),2);const n=this.poolingFunction(Xo(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return jt(n,[2])}))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class vc extends xc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Va(i),qa(s),wc(t,e,n,s,i,"max")}}vc.className="MaxPooling1D",n.registerClass(vc);class Sc extends xc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Va(i),qa(s),wc(t,e,n,s,i,"avg")}}Sc.className="AveragePooling1D",n.registerClass(Sc);class Ic extends ll{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new aa(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides];Na(this.poolSize,"poolSize"),Na(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Va(this.dataFormat),qa(this.padding),this.inputSpec=[new sl({ndim:4})]}computeOutputShape(t){t=Yo(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2];return e=fh(e,this.poolSize[0],this.padding,this.strides[0]),n=fh(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n]:[t[0],e,n,t[3]]}call(t,e){return s((()=>(this.invokeCallHook(t,e),this.poolingFunction(Xo(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Nc extends Ic{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Va(i),qa(s),wc(t,e,n,s,i,"max")}}Nc.className="MaxPooling2D",n.registerClass(Nc);class Ac extends Ic{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Va(i),qa(s),wc(t,e,n,s,i,"avg")}}Ac.className="AveragePooling2D",n.registerClass(Ac);class Cc extends ll{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new aa(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];Na(this.poolSize,"poolSize"),Na(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Va(this.dataFormat),qa(this.padding),this.inputSpec=[new sl({ndim:5})]}computeOutputShape(t){t=Yo(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=fh(e,this.poolSize[0],this.padding,this.strides[0]),n=fh(n,this.poolSize[1],this.padding,this.strides[1]),s=fh(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n,s]:[t[0],e,n,s,t[4]]}call(t,e){return s((()=>(this.invokeCallHook(t,e),this.poolingFunction(Xo(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class zc extends Cc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Va(i),qa(s),kc(t,e,n,s,i,"max")}}zc.className="MaxPooling3D",n.registerClass(zc);class Dc extends Cc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Va(i),qa(s),kc(t,e,n,s,i,"avg")}}Dc.className="AveragePooling3D",n.registerClass(Dc);class $c extends ll{constructor(t){super(t),this.inputSpec=[new sl({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new oa}}class Tc extends $c{constructor(t){super(t||{})}call(t,e){return s((()=>{const e=Xo(t);return j(e,1)}))}}Tc.className="GlobalAveragePooling1D",n.registerClass(Tc);class Ec extends $c{constructor(t){super(t||{})}call(t,e){return s((()=>{const e=Xo(t);return c(e,1)}))}}Ec.className="GlobalMaxPooling1D",n.registerClass(Ec);class Fc extends ll{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Va(this.dataFormat),this.inputSpec=[new sl({ndim:4})]}computeOutputShape(t){return t=t,"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new oa}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Lc extends Fc{call(t,e){return s((()=>{const e=Xo(t);return"channelsLast"===this.dataFormat?j(e,[1,2]):j(e,[2,3])}))}}Lc.className="GlobalAveragePooling2D",n.registerClass(Lc);class _c extends Fc{call(t,e){return s((()=>{const e=Xo(t);return"channelsLast"===this.dataFormat?c(e,[1,2]):c(e,[2,3])}))}}_c.className="GlobalMaxPooling2D",n.registerClass(_c);class Rc extends ll{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,n={}){const s=Sl(e.layer,n);delete e.layer;const i={layer:s};return Object.assign(i,e),new t(i)}}class Mc extends Rc{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=Yo(t)).length<3)throw new aa(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=Yo(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e),s=t[1];return[n[0],s].concat(n.slice(1))}call(t,e){return s((()=>Eh(((t,n)=>[Xo(this.layer.call(t,e)),[]]),t=Xo(t),[],!1,null,null,!1,!0)[1]))}}Mc.className="TimeDistributed",n.registerClass(Mc);class Oc extends Rc{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=Sl(n),e.goBackwards=!0!==e.goBackwards;const s={};var i;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=Sl(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,i=this.mergeMode,Sa(Ka,"BidirectionalMergeMode",i),t.weights)throw new oa("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,n=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let e,n,s,i=this.forwardLayer.computeOutputShape(t);return Array.isArray(i)&&Array.isArray(i[0])||(i=[i]),i=i,this.returnState?(s=i.slice(1),e=i[0]):e=i[0],e=e,"concat"===this.mergeMode?(e[e.length-1]*=2,n=[e]):n=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):pa(n)}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Th(t,n,s,this.numConstants);if(t=i.inputs,n=i.initialState,s=i.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==s)return super.apply(t,e);const r=[],a=[];if(null!=n){const t=n.length;if(t%2>0)throw new aa("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,r.push(...n);const s=n.map((t=>new sl({shape:t.shape})));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),a.push(...s)}if(null!=s)throw new oa("Support for constants in Bidirectional layers is not implemented yet.");const o=r[0]instanceof il;for(const t of r)if(t instanceof il!==o)throw new aa("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return s((()=>{const n=e.initialState;let s,i,r,o;if(null==n)s=this.forwardLayer.call(t,e),i=this.backwardLayer.call(t,e);else{const r=n.slice(0,n.length/2),a=n.slice(n.length/2);s=this.forwardLayer.call(t,Object.assign(e,{initialState:r})),i=this.backwardLayer.call(t,Object.assign(e,{initialState:a}))}return this.returnState&&(Array.isArray(s)&&(r=s.slice(1).concat(i.slice(1))),s=s[0],i=i[0]),this.returnSequences&&(i=Nt(i,1)),"concat"===this.mergeMode?o=ho([s,i]):"sum"===this.mergeMode?o=u(s,i):"ave"===this.mergeMode?o=a(.5,u(s,i)):"mul"===this.mergeMode?o=a(s,i):null==this.mergeMode&&(o=[s,i]),this.returnState?null==this.mergeMode?o.concat(r):[o].concat(r):o}))}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){Ja(this.forwardLayer.name,(()=>{this.forwardLayer.build(t)})),Ja(this.backwardLayer.name,(()=>{this.backwardLayer.build(t)})),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map((t=>null));return Array.isArray(n)?n.concat(t).concat(t):[n].concat(t).concat(t)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=Sl(e.layer);if(delete e.layer,null!=e.numConstants)throw new oa("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=n,new t(s)}}function Bc(t){return new Sc(t)}function Pc(t){return new Ac(t)}function Wc(t){return new Dc(t)}function Uc(t){return new Ec(t)}function Kc(t){return new _c(t)}function jc(t){return new vc(t)}function Vc(t){return new Nc(t)}Oc.className="Bidirectional",n.registerClass(Oc);const qc=Uc,Gc=Kc,Hc=jc,Jc=Vc;var Zc=Object.freeze({__proto__:null,inputLayer:function(t){return new hl(t)},elu:function(t){return new hh(t)},reLU:function(t){return new oh(t)},leakyReLU:function(t){return new lh(t)},prelu:function(t){return new uh(t)},softmax:function(t){return new ph(t)},thresholdedReLU:function(t){return new ch(t)},conv1d:function(t){return new Ch(t)},conv2d:function(t){return new xh(t)},conv2dTranspose:function(t){return new Sh(t)},conv3d:function(t){return new vh(t)},conv3dTranspose:function(t){return new Ih(t)},separableConv2d:function(t){return new Ah(t)},cropping2D:function(t){return new zh(t)},upSampling2d:function(t){return new Dh(t)},depthwiseConv2d:function(t){return new $h(t)},activation:function(t){return new Zh(t)},dense:function(t){return new Hh(t)},dropout:function(t){return new qh(t)},spatialDropout1d:function(t){return new Gh(t)},flatten:function(t){return new Jh(t)},repeatVector:function(t){return new Xh(t)},reshape:function(t){return new Yh(t)},permute:function(t){return new Qh(t)},embedding:function(t){return new ec(t)},add:function(t){return new sc(t)},average:function(t){return new rc(t)},concatenate:function(t){return new lc(t)},maximum:function(t){return new ac(t)},minimum:function(t){return new oc(t)},multiply:function(t){return new ic(t)},dot:function(t){return new hc(t)},batchNormalization:function(t){return new mc(t)},layerNormalization:function(t){return new yc(t)},zeroPadding2d:function(t){return new bc(t)},averagePooling1d:Bc,avgPool1d:function(t){return Bc(t)},avgPooling1d:function(t){return Bc(t)},averagePooling2d:Pc,avgPool2d:function(t){return Pc(t)},avgPooling2d:function(t){return Pc(t)},averagePooling3d:Wc,avgPool3d:function(t){return Wc(t)},avgPooling3d:function(t){return Wc(t)},globalAveragePooling1d:function(t){return new Tc(t)},globalAveragePooling2d:function(t){return new Lc(t)},globalMaxPooling1d:Uc,globalMaxPooling2d:Kc,maxPooling1d:jc,maxPooling2d:Vc,maxPooling3d:function(t){return new zc(t)},gru:function(t){return new Oh(t)},gruCell:function(t){return new Mh(t)},lstm:function(t){return new Ph(t)},lstmCell:function(t){return new Bh(t)},simpleRNN:function(t){return new Rh(t)},simpleRNNCell:function(t){return new _h(t)},convLstm2d:function(t){return new Vh(t)},convLstm2dCell:function(t){return new jh(t)},rnn:function(t){return new Fh(t)},stackedRNNCells:function(t){return new Wh(t)},bidirectional:function(t){return new Oc(t)},timeDistributed:function(t){return new Mc(t)},globalMaxPool1d:qc,globalMaxPool2d:Gc,maxPool1d:Hc,maxPool2d:Jc,Layer:ll,RNN:Fh,RNNCell:Lh,input:_u,gaussianNoise:function(t){return new cc(t)},gaussianDropout:function(t){return new pc(t)},alphaDropout:function(t){return new dc(t)},masking:function(t){return new tc(t)}});var Xc=Object.freeze({__proto__:null,binaryAccuracy:function(t,e){return Ll(t,e)},binaryCrossentropy:function(t,e){return Bl(t,e)},sparseCategoricalAccuracy:function(t,e){return Pl(t,e)},categoricalAccuracy:function(t,e){return _l(t,e)},categoricalCrossentropy:function(t,e){return Wl(t,e)},precision:function(t,e){return Ml(t,e)},recall:function(t,e){return Ol(t,e)},cosineProximity:function(t,e){return Tl(t,e)},meanAbsoluteError:function(t,e){return Al(t,e)},meanAbsolutePercentageError:function(t,e){return Cl(t,e)},MAPE:function(t,e){return Cl(t,e)},mape:function(t,e){return Cl(t,e)},meanSquaredError:function(t,e){return Nl(t,e)},MSE:function(t,e){return Nl(t,e)},mse:function(t,e){return Nl(t,e)}}),Yc=Object.freeze({__proto__:null,modelFromJSON:async function(t,e){"modelTopology"in t||(t={modelTopology:t});let n=(t=t).modelTopology;null!=n.model_config&&(n=n.model_config);const s=Sl(Ql(n),e);if(null!=t.weightsManifest){const e=await ut.loadWeights(t.weightsManifest,t.pathPrefix,s.weights.map((t=>t.originalName))),n={};for(const t of s.weights)n[t.originalName]=e[t.originalName];s.loadWeights(n),B(e)}return s}});var Qc=Object.freeze({__proto__:null,l1l2:function(t){return new nh(t)},l1:function(t){return th(e=t),new nh({l1:null!=e?e.l1:null,l2:0});var e},l2:function(t){return th(e=t),new nh({l2:null!=e?e.l2:null,l1:0});var e}});class tp extends gl{constructor(){super(...arguments),this.model=null}setModel(t){if(!(t instanceof zu))throw new Error("model must be a LayersModel, not some other Container");this.model=t}}function ep(t,e){return t<e}function np(t,e){return t>e}class sp extends tp{constructor(t){if(super(),null==t&&(t={}),t.restoreBestWeights)throw new oa("restoreBestWeights = True is not implemented in EarlyStopping yet.");this.monitor=t.monitor||"val_loss",this.minDelta=Math.abs(t.minDelta||0),this.patience=t.patience||0,this.verbose=t.verbose||0,this.mode=t.mode||"auto",this.baseline=t.baseline,-1===["auto","min","max"].indexOf(this.mode)&&(console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`),this.mode="auto"),"min"===this.mode?this.monitorFunc=ep:"max"===this.mode||-1!==this.monitor.indexOf("acc")?this.monitorFunc=np:this.monitorFunc=ep,this.monitorFunc===ep&&(this.minDelta*=-1)}async onTrainBegin(t){this.wait=0,this.stoppedEpoch=0,null!=this.baseline?this.best=this.baseline:this.best=this.monitorFunc===ep?1/0:-1/0}async onEpochEnd(t,e){await pl(e);const n=this.getMonitorValue(e);null!=n&&(this.monitorFunc(n-this.minDelta,this.best)?(this.best=n,this.wait=0):(this.wait++,this.wait>=this.patience&&(this.stoppedEpoch=t,this.model.stopTraining=!0)))}async onTrainEnd(t){this.stoppedEpoch>0&&this.verbose&&console.log(`Epoch ${this.stoppedEpoch}: early stopping.`)}getMonitorValue(t){null==t&&(t={});const e=t[this.monitor];return null==e&&console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: ${Object.keys(t)}`),e}}const ip={earlyStopping:function(t){return new sp(t)}};export{tp as Callback,ml as CallbackList,wl as CustomCallback,sp as EarlyStopping,bl as History,sl as InputSpec,tl as LayerVariable,zu as LayersModel,Fh as RNN,Tu as Sequential,il as SymbolicTensor,ip as callbacks,Oa as constraints,jo as initializers,_u as input,Zc as layers,Lu as loadLayersModel,Xc as metrics,Eu as model,Yc as models,Ru as registerCallbackConstructor,Qc as regularizers,Fu as sequential,eu as version_layers};
//# sourceMappingURL=tf-layers.fesm.min.js.map
